{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2(v2) slovak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuCipgAa7Esc+7pTjEtz07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/gpt_2(v2)_slovak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npg83tYyKWcK",
        "colab_type": "code",
        "outputId": "fad64eae-b95b-40f2-8fc4-dfe6c12f017f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bbg6Q-TKYDR",
        "colab_type": "code",
        "outputId": "04fde696-ff8a-4eac-ce9e-72a3bfcb2187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar  3 10:01:19 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQglP74wKdx2",
        "colab_type": "code",
        "outputId": "f0956861-7de8-4e35-9cc2-501992d5465e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2t3L_2qwOpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def _gelu_python(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        This is now written in C in torch.nn.functional\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "if torch.__version__ < \"1.4.0\":\n",
        "    gelu = _gelu_python\n",
        "else:\n",
        "    gelu = F.gelu\n",
        "\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "ACT2FN = {\n",
        "    \"relu\": F.relu,\n",
        "    \"swish\": swish,\n",
        "    \"gelu\": gelu,\n",
        "    \"tanh\": F.tanh,\n",
        "    \"gelu_new\": gelu_new,\n",
        "}\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "    if activation_string in ACT2FN:\n",
        "        return ACT2FN[activation_string]\n",
        "    else:\n",
        "        raise KeyError(\n",
        "            \"function {} not found in ACT2FN mapping {} or torch.nn.functional\".format(\n",
        "                activation_string, list(ACT2FN.keys())\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoMovS4Dxapj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import typing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# renamed because it's not conv1d form pytorch, but a linear layer with transposition\n",
        "class NotConv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n",
        "            Basically works like a Linear layer but the weights are transposed\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SequenceSummary(nn.Module):\n",
        "    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n",
        "        Args of the config class:\n",
        "            summary_type:\n",
        "                - 'last' => [default] take the last token hidden state (like XLNet)\n",
        "                - 'first' => take the first token hidden state (like Bert)\n",
        "                - 'mean' => take the mean of all tokens hidden states\n",
        "                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)\n",
        "                - 'attn' => Not implemented now, use multi-head attention\n",
        "            summary_use_proj: Add a projection after the vector extraction\n",
        "            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n",
        "            summary_activation: 'tanh' or another string => add an activation to the output, Other => no activation. Default\n",
        "            summary_first_dropout: Add a dropout before the projection and activation\n",
        "            summary_last_dropout: Add a dropout after the projection and activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.summary_type = getattr(config, \"summary_type\", \"last\")\n",
        "        if self.summary_type == \"attn\":\n",
        "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
        "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
        "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.summary = Identity()\n",
        "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
        "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
        "                num_classes = config.num_labels\n",
        "            else:\n",
        "                num_classes = config.hidden_size\n",
        "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        activation_string = getattr(config, \"summary_activation\", None)\n",
        "        self.activation = (\n",
        "            get_activation(activation_string) if activation_string else Identity()\n",
        "        )  # type: typing.Callable\n",
        "\n",
        "        self.first_dropout = Identity()\n",
        "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
        "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
        "\n",
        "        self.last_dropout = Identity()\n",
        "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
        "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "    def forward(self, hidden_states, cls_index=None):\n",
        "        \"\"\" hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.\n",
        "            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n",
        "                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n",
        "                if summary_type == 'cls_index' and cls_index is None:\n",
        "                    we take the last token of the sequence as classification token\n",
        "        \"\"\"\n",
        "        if self.summary_type == \"last\":\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.summary_type == \"first\":\n",
        "            output = hidden_states[:, 0]\n",
        "        elif self.summary_type == \"mean\":\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.summary_type == \"cls_index\":\n",
        "            if cls_index is None:\n",
        "                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2] - 1, dtype=torch.long)\n",
        "            else:\n",
        "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
        "                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n",
        "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
        "            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n",
        "        elif self.summary_type == \"attn\":\n",
        "            raise NotImplementedError\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def prune_conv1d_layer(layer, index, dim=1):\n",
        "    \"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n",
        "        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if dim == 0:\n",
        "        b = layer.bias.clone().detach()\n",
        "    else:\n",
        "        b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = NotConv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    new_layer.bias.requires_grad = False\n",
        "    new_layer.bias.copy_(b.contiguous())\n",
        "    new_layer.bias.requires_grad = True\n",
        "    return new_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8hU8FD0wB40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# gpt: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# transformer block: https://arxiv.org/pdf/1801.10198.pdf\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# memory compressed att? https://arxiv.org/pdf/1801.10198.pdf\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, config, scale=False):\n",
        "        super(Attention,self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
        "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
        "        # print(n_state,'|', config.n_head)\n",
        "        assert n_state % config.n_head == 0\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = config.n_head\n",
        "        self.split_size = n_state\n",
        "        self.scale = scale\n",
        "\n",
        "        self.c_attn = NotConv1D(n_state * 3, nx)\n",
        "        self.c_proj = NotConv1D(n_state, nx)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.n_head, self.split_size // self.n_head)\n",
        "        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n",
        "        for head in heads:\n",
        "            # Compute how many pruned heads are before the head and move the index accordingly\n",
        "            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
        "\n",
        "        # Prune conv1d layers\n",
        "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
        "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
        "\n",
        "        # Update hyper params\n",
        "        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n",
        "        self.n_head = self.n_head - len(heads)\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def _attn(self, q, k, v, attention_mask=None, head_mask=None):\n",
        "        w = torch.matmul(q, k)\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        b = self.bias[:, :, ns - nd : ns, :ns]\n",
        "        w = w * b - 1e4 * (1 - b)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask\n",
        "            w = w + attention_mask\n",
        "\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "        w = self.attn_dropout(w)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            w = w * head_mask\n",
        "\n",
        "        outputs = [torch.matmul(w, v)]\n",
        "        if self.output_attentions:\n",
        "            outputs.append(w)\n",
        "        return outputs\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
        "\n",
        "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value = x.split(self.split_size, dim=2)\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
        "\n",
        "        attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n",
        "        a = attn_outputs[0]\n",
        "\n",
        "        a = self.merge_heads(a)\n",
        "        a = self.c_proj(a)\n",
        "        a = self.resid_dropout(a)\n",
        "\n",
        "        outputs = [a, present] + attn_outputs[1:]\n",
        "        return outputs  # a, present, (attentions)\n",
        "\n",
        "# not pytorch MLP ...\n",
        "class NotMLP(nn.Module):\n",
        "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(NotMLP,self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.c_fc = NotConv1D(n_state, nx)\n",
        "        self.c_proj = NotConv1D(nx, n_state)\n",
        "        self.act = gelu_new\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return self.dropout(h2)\n",
        "\n",
        "# fig 1 in https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# simplified: masked-multi -> layer norm -> mlp-> layer norm \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(Block,self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.attn = Attention(nx, n_ctx, config, scale)\n",
        "        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = NotMLP(4 * nx, config)\n",
        "\n",
        "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n",
        "        output_attn = self.attn(\n",
        "            self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask\n",
        "        )\n",
        "        a = output_attn[0]  # output_attn: a, present, (attentions)\n",
        "\n",
        "        x = x + a\n",
        "        m = self.mlp(self.ln_2(x))\n",
        "        x = x + m\n",
        "\n",
        "        outputs = [x] + output_attn[1:]\n",
        "        return outputs  # x, present, (attentions)\n",
        "\n",
        "\n",
        "#The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):   \n",
        "        super(GPT2Model,self).__init__()\n",
        "        self.config = config     \n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_past = config.output_past\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.wte\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.wte = new_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.h[layer].attn.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "    ):\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "        if position_ids is not None:\n",
        "            position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "        if position_ids is None:\n",
        "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "        # Attention mask.\n",
        "        if attention_mask is not None:\n",
        "            batch_size = input_ids.shape[0]\n",
        "            attention_mask = attention_mask.view(batch_size, -1)\n",
        "            # We create a 3D attention mask from a 2D tensor mask.\n",
        "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "            # this attention mask is more simple than the triangular masking of causal attention\n",
        "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "            # masked positions, this operation will create a tensor which is 0.0 for\n",
        "            # positions we want to attend and -10000.0 for masked positions.\n",
        "            # Since we are adding it to the raw scores before the softmax, this is\n",
        "            # effectively the same as removing these entirely.\n",
        "            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                head_mask = (\n",
        "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "                )  # We can specify head_mask for each layer\n",
        "            head_mask = head_mask.to(\n",
        "                dtype=next(self.parameters()).dtype\n",
        "            )  # switch to fload if need + fp16 compatibility\n",
        "        else:\n",
        "            head_mask = [None] * self.config.n_layer\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        if token_type_ids is not None:\n",
        "            token_type_embeds = self.wte(token_type_ids)\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
        "        hidden_states = self.drop(hidden_states)\n",
        "\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "        presents = ()\n",
        "        all_attentions = []\n",
        "        all_hidden_states = ()\n",
        "        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n",
        "\n",
        "            outputs = block(\n",
        "                hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]\n",
        "            )\n",
        "\n",
        "            hidden_states, present = outputs[:2]\n",
        "            if self.output_past:\n",
        "                presents = presents + (present,)\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(outputs[2])\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(*output_shape)\n",
        "        # Add last hidden state\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_past:\n",
        "            outputs = outputs + (presents,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            # let the number of heads free (-1) so we can extract attention even after head pruning\n",
        "            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]\n",
        "            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # last hidden state, (presents), (all hidden_states), (attentions)\n",
        "\n",
        "\n",
        "class GPT2LMHeadModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2LMHeadModel,self).__init__()\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
        "        # only last token for inputs_ids if past is defined in kwargs\n",
        "        if \"past\" in kwargs and kwargs[\"past\"]:\n",
        "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
        "\n",
        "        inputs = {\"input_ids\": input_ids}\n",
        "        inputs.update(kwargs)\n",
        "        return inputs\n",
        "\n",
        "    def forward(self,input_ids, labels = None):        \n",
        "        transformer_outputs = self.transformer(input_ids)\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        outputs = (lm_logits,) + transformer_outputs[1:]\n",
        "        if labels is not None:\n",
        "          # Shift so that tokens < n predict n\n",
        "          shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "          shift_labels = input_ids[..., 1:].contiguous()\n",
        "          # Flatten the tokens\n",
        "          loss_fct = CrossEntropyLoss()\n",
        "          loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "          outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n",
        "\n",
        "\n",
        "class GPT2DoubleHeadsModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2DoubleHeadsModel,self).__init__()\n",
        "        config.num_labels = 1\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.multiple_choice_head = SequenceSummary(config)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        mc_token_ids=None,\n",
        "        lm_labels=None,\n",
        "        mc_labels=None,\n",
        "    ):\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past=past,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n",
        "\n",
        "        outputs = (lm_logits, mc_logits) + transformer_outputs[1:]\n",
        "        if mc_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "        if lm_labels is not None:\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = lm_labels[..., 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKdAadsE4pOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" OpenAI GPT-2 configuration \"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "#Number of parameters: 45171200\n",
        "class GPT2Config:\n",
        "    model_type = \"gpt2\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50257,\n",
        "        n_positions=512,\n",
        "        n_ctx=512,\n",
        "        # n_positions=512,\n",
        "        # n_ctx=512,\n",
        "        # n_embd=768,\n",
        "        # n_layer=12,\n",
        "        # n_head=12,\n",
        "        n_embd=256,\n",
        "        n_layer=6,\n",
        "        n_head=8,\n",
        "        resid_pdrop=0.1,\n",
        "        embd_pdrop=0.1,\n",
        "        attn_pdrop=0.1,\n",
        "        layer_norm_epsilon=1e-5,\n",
        "        initializer_range=0.02,\n",
        "        summary_type=\"cls_index\",\n",
        "        summary_use_proj=True,\n",
        "        summary_activation=None,\n",
        "        summary_proj_to_labels=True,\n",
        "        summary_first_dropout=0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n",
        "        self.summary_type = summary_type\n",
        "        self.summary_use_proj = summary_use_proj\n",
        "        self.summary_activation = summary_activation\n",
        "        self.summary_first_dropout = summary_first_dropout\n",
        "        self.summary_proj_to_labels = summary_proj_to_labels\n",
        "\n",
        "    @property\n",
        "    def max_position_embeddings(self):\n",
        "        return self.n_positions\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self):\n",
        "        return self.n_embd\n",
        "\n",
        "    @property\n",
        "    def num_attention_heads(self):\n",
        "        return self.n_head\n",
        "\n",
        "    @property\n",
        "    def num_hidden_layers(self):\n",
        "        return self.n_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmPrpVQOKRmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "# from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# constants\n",
        "\n",
        "NUM_BATCHES = int(1e5)\n",
        "BATCH_SIZE = 8\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "VALIDATE_EVERY  = 100\n",
        "GENERATE_EVERY  = 500\n",
        "GENERATE_LENGTH = 512\n",
        "# SEQ_LEN = 4096\n",
        "SEQ_LEN = 512\n",
        "\n",
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def get_top_p(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "def sample_next_token(logits, top_p=0.9, temperature = 1.0):\n",
        "    logits = logits[0, -1, :] / temperature\n",
        "    filtered_logits = get_top_p(logits, top_p=top_p)\n",
        "\n",
        "    probs = F.softmax(filtered_logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(token))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "# instantiate model\n",
        "config = GPT2Config()\n",
        "config.output_hidden_states = True\n",
        "config.output_attentions = True\n",
        "config.output_past = True\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.cuda()\n",
        "# print(model)\n",
        "print('Number of parameters:',get_n_params(model))\n",
        "\n",
        "with gzip.open('/content/drive/My Drive/model_data/merged.gz') as file:\n",
        "    X = np.array([int(c) for c in file.read()])\n",
        "    si = int(len(X)-len(X)*0.2)\n",
        "    trX, vaX = np.split(X, [si])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        return full_seq[0:-1].cuda(), full_seq[1:].cuda()\n",
        "        # return full_seq[0:-1], full_seq[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader    = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "# optimizer\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,amsgrad=True)\n",
        "\n",
        "# scheduler = get_linear_schedule_with_warmup(\n",
        "#             optim,\n",
        "#             num_warmup_steps=VALIDATE_EVERY,\n",
        "#             num_training_steps=len(train_dataset) // GRADIENT_ACCUMULATE_EVERY * NUM_BATCHES\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "# def get_batch_loss(model, data):\n",
        "#     x, y = data\n",
        "#     pred = model(x)\n",
        "#     return F.cross_entropy(pred[0].transpose(1, 2), y, reduction='mean')\n",
        "\n",
        "\n",
        "# for i in tqdm.tqdm(range(0, NUM_BATCHES), mininterval=10., desc='training'):\n",
        "#     model.train()\n",
        "\n",
        "#     for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
        "#         loss = get_batch_loss(model, next(train_loader))\n",
        "#         loss.backward()\n",
        "\n",
        "#     print(f'training loss: {loss.item()}')\n",
        "#     torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # scheduler.step()\n",
        "\n",
        "#     if i % VALIDATE_EVERY == 0:\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             loss = get_batch_loss(model, next(val_loader))\n",
        "#             print(f'validation loss: {loss.item()}')\n",
        "\n",
        "#     if i % GENERATE_EVERY == 0:\n",
        "#         torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/gpt2v2', 'epoch-{}.pt'.format(i)))\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             inp, _ = random.choice(val_dataset)\n",
        "#             output_str = ''\n",
        "#             prime = decode_tokens(inp)\n",
        "\n",
        "#             # print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "#             print(prime)\n",
        "#             print('*'*100)\n",
        "\n",
        "#             for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "#                 logits = model(inp[None, :])[0]\n",
        "#                 next_token = sample_next_token(logits)\n",
        "#                 output_str += decode_token(next_token)\n",
        "#                 inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "#             print(output_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZkIPLqNkCUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f0454939-78fb-4b70-b30d-2b91362104de"
      },
      "source": [
        "import os\n",
        "import re\n",
        "files = [f for f in os.listdir('/content/drive/My Drive/gpt2v2') if f.startswith('gpt2v2-slovak-')]\n",
        "last_model_file = None\n",
        "epochs_run = 0\n",
        "if len(files) > 0:\n",
        "  files.sort(reverse=True)\n",
        "  last_model_file = os.path.join('/content/drive/My Drive/gpt2v2',files[0])\n",
        "  print(last_model_file)\n",
        "  epochs = re.findall(r'\\d+',files[0])\n",
        "  print(epochs)\n",
        "  epochs_run = 0\n",
        "  if len(epochs) == 3:\n",
        "    epochs_run = int(epochs[2])\n",
        "  print('number of epochs run:',epochs_run)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/gpt2v2/gpt2v2-slovak-1.pt\n",
            "['2', '2', '1']\n",
            "number of epochs run: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rElgAsWEj9Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8533fbf2-c96c-4ae0-ef80-7427188545a1"
      },
      "source": [
        "# training\n",
        "if last_model_file is not None:\n",
        "  model.load_state_dict(torch.load(last_model_file ))\n",
        "\n",
        "for epoch in range(epochs_run,10):\n",
        "  print('Training....')\n",
        "  model.train()\n",
        "  i = 0\n",
        "  for x,y in train_loader:\n",
        "    # (loss), lm_logits, presents, (all hidden_states), (attentions)\n",
        "    out = model(x,y)\n",
        "    loss = out[0]    \n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    if i % 1000 == 0:\n",
        "      print(f'{i}/{len(train_loader)}: Loss: {loss.item()}, Perplexity {torch.exp(out[0])}')\n",
        "    i = i + 1\n",
        "    \n",
        "  torch.save(model.state_dict(), f'/content/drive/My Drive/gpt2v2/gpt2v2-slovak-{epoch+1}.pt')\n",
        "\n",
        "  print('Validation...')\n",
        "  model.eval()\n",
        "  i = 0\n",
        "  with torch.no_grad():\n",
        "    loss = None\n",
        "    for x,y in val_loader:\n",
        "      out = model(x,y)\n",
        "      \n",
        "      if i % 1000 == 0:\n",
        "        print(f'{i}/{len(val_loader)}:Validation loss {out[0].item()}, Perplexity {torch.exp(out[0])}')\n",
        "\n",
        "      if loss is None:\n",
        "        loss=out[0]\n",
        "      else:\n",
        "        loss = loss + out[0]\n",
        "      i = i + 1\n",
        "\n",
        "\n",
        "    loss = loss/len(val_loader)\n",
        "\n",
        "    perplexity  = torch.exp(loss)\n",
        "    print(f'Validation loss {loss}')\n",
        "    print(f'Perplexity {perplexity}')\n",
        "\n",
        "  # generate\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      inp, _ = random.choice(val_dataset)\n",
        "      output_str = ''\n",
        "      prime = decode_tokens(inp)\n",
        "\n",
        "      print(prime)\n",
        "      print('*'*100)\n",
        "\n",
        "      for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "          logits = model(inp[None, :])[0]\n",
        "          next_token = sample_next_token(logits)\n",
        "          output_str += decode_token(next_token)\n",
        "          inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "      print(output_str)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training....\n",
            "0/55240: Loss: 0.7158677577972412, Perplexity 2.045961380004883\n",
            "1000/55240: Loss: 0.44681376218795776, Perplexity 1.5633231401443481\n",
            "2000/55240: Loss: 1.0188452005386353, Perplexity 2.769994020462036\n",
            "3000/55240: Loss: 0.5490291714668274, Perplexity 1.7315711975097656\n",
            "4000/55240: Loss: 0.8339993953704834, Perplexity 2.302509069442749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AlX2gzZkifs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "6abb6f08-8dbe-4fb6-e7a8-c58db4636007"
      },
      "source": [
        "# generate\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inp, _ = random.choice(val_dataset)\n",
        "    output_str = ''\n",
        "    prime = decode_tokens(inp)\n",
        "\n",
        "    # print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "    print(prime)\n",
        "    print('*'*100)\n",
        "\n",
        "    for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "        logits = model(inp[None, :])[0]\n",
        "        next_token = sample_next_token(logits)\n",
        "        output_str += decode_token(next_token)\n",
        "        inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "    print(output_str)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating:   1%|          | 4/512 [00:00<00:12, 39.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rajin, konkretne zo Syrie, Iranu, Libye, Somalska,\r\n",
            "Sudanu a Jemenu. Irak z novej verzie cestovneho zakazu vypadol. Nove\r\n",
            "nariadenie ma vstupit do platnosti 16. marca.\r\n",
            "Stat Havaj bol jednym z mnohych zalujucich v celej retazi sudnych\r\n",
            "zalob podanych proti povodnemu zakazu, ktory sposobil chaoticke\r\n",
            "situacie na letiskach a vyvolal aj protesty rozhnevanych ludi.Utok bol vykonany v odlahlej oblasti afganskej provincie Kunar. Americki\r\n",
            "armadni cinitelia uviedli, ze islo o najvyznamnejsiu podobnu operaciu\r\n",
            "za p\n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:06<00:00, 78.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "racu v nej okolite na euz a stele velmi case staty na dva a najma den\r\n",
            "sebestacny sinfekcia energie ako najmenej dve pecenta. Zatial nikto reagoval\r\n",
            "seria treba udrzba. Ja zadlzena kladatela plyn askor, aby k pride kvalitami\r\n",
            "zaplatit do uvahy z svojho premiera. Samotna namoho trojnasobok diviakovne\r\n",
            "presadzovala vojensku kalendarnou banku Praha potravin a zakladane\r\n",
            "regionalny obetam a jeho najnovsi v pripade ziadost jeho sebesta podari\r\n",
            "o pracovnych kandidatoch sa bude prevzatia, odpoveda postup casu ziad\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}