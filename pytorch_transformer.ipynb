{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6vZo2OqgJWQQMGh51rjRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colan_notebooks/blob/master/pytorch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV_t32wUin9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "f5f4bc8a-5f43-4fe7-86ee-b414e80b8b5b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 21.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=487a341f282d3b8d5e9c0cfbda6515a8985649d33c4a1efc0fbc364f3e4f70d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeVup08MivQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnAGLm8mfL-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e6376ec7-53d6-4e54-f1b3-844d21b760ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S75c7nlmf2Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
        "        in the sequence. The positional encodings have the same dimension as\n",
        "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
        "        functions of different frequencies.\n",
        "    .. math::\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        try:\n",
        "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        except:\n",
        "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aKCoVZhfrS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28589120-e9a7-4e48-db9f-c14255357cd9"
      },
      "source": [
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "NUM_BATCHES = 40\n",
        "BATCH_SIZE = 8\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "VALIDATE_EVERY  = 4\n",
        "GENERATE_EVERY  = 4\n",
        "GENERATE_LENGTH = 512\n",
        "SEQ_LEN = 4096\n",
        "\n",
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def get_top_p(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "def sample_next_token(logits, top_p=0.9, temperature = 1.0):\n",
        "    logits = logits[0, -1, :] / temperature\n",
        "    filtered_logits = get_top_p(logits, top_p=top_p)\n",
        "\n",
        "    probs = F.softmax(filtered_logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(token))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "# model = ReformerLM(\n",
        "#     dim = 512,\n",
        "#     depth = 6,\n",
        "#     max_seq_len = SEQ_LEN,\n",
        "#     num_tokens = 256,\n",
        "#     heads = 8,\n",
        "#     bucket_size = 64,\n",
        "#     n_hashes = 8,\n",
        "#     ff_chunks = 10,\n",
        "#     lsh_dropout = 0.1,\n",
        "#     weight_tie = True,\n",
        "#     causal = True,\n",
        "#     use_full_attn = False # set this to true for comparison with full attention\n",
        "# )\n",
        "\n",
        "# instantiate model\n",
        "#ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout\n",
        "# ntoken, ninp, nhead, nhid, nlayers, dropout=0.5\n",
        "model = TransformerModel(256,512,8,64,6,0.1)\n",
        "\n",
        "# if last_model_file is not None:\n",
        "#   model.load_state_dict(torch.load(last_model_file ))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "\n",
        "# prepare enwik8 data\n",
        "\n",
        "with gzip.open('/content/drive/My Drive/model_data/merged.gz') as file:\n",
        "    X = np.array([int(c) for c in file.read()])\n",
        "    si = int(len(X)-len(X)*0.2)\n",
        "    trX, vaX = np.split(X, [si])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        if torch.cuda.is_available():\n",
        "          return full_seq[0:-1].cuda(), full_seq[1:].cuda()\n",
        "        return full_seq[0:-1], full_seq[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
        "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "# optimizer\n",
        "# optimizer.load_state_dict(torch.load('optimizer.pt'))\n",
        "# scheduler.load_state_dict(torch.load('scheduler.pt'))\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE,amsgrad=True)\n",
        "\n",
        "# if os.path.exists('/content/drive/My Drive/model_saves/optim.pt'):\n",
        "#   optim.load_state_dict(torch.load('/content/drive/My Drive/model_saves/optim.pt'))\n",
        "\n",
        "#scheduler\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=VALIDATE_EVERY, gamma=0.1)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "            optim,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataset) // GRADIENT_ACCUMULATE_EVERY * NUM_BATCHES\n",
        "        )\n",
        "\n",
        "# if os.path.exists('/content/drive/My Drive/model_saves/scheduler.pt'):\n",
        "#   scheduler.load_state_dict(torch.load('/content/drive/My Drive/model_saves/scheduler.pt'))\n",
        "\n",
        "# training\n",
        "\n",
        "def get_batch_loss(model, data):\n",
        "    x, y = data\n",
        "    pred = model(x)\n",
        "    return F.cross_entropy(pred.transpose(1, 2), y, reduction='mean')\n",
        "\n",
        "for i in tqdm.tqdm(range(0, NUM_BATCHES), mininterval=10., desc='training'):\n",
        "    model.train()\n",
        "\n",
        "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
        "        loss = get_batch_loss(model, next(train_loader))\n",
        "        loss.backward()\n",
        "\n",
        "    print(f'training loss: {loss.item()}')\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i % VALIDATE_EVERY == 0:\n",
        "        torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/model_saves', 'epoch-{}.pt'.format(i)))\n",
        "        torch.save(optim.state_dict(),'/content/drive/My Drive/model_saves/optim.pt')\n",
        "        torch.save(scheduler.state_dict(),'/content/drive/My Drive/model_saves/scheduler.pt')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss = get_batch_loss(model, next(val_loader))\n",
        "            print(f'validation loss: {loss.item()}')\n",
        "\n",
        "    if i % GENERATE_EVERY == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inp, _ = random.choice(val_dataset)\n",
        "            output_str = ''\n",
        "            prime = decode_tokens(inp)\n",
        "\n",
        "            # print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "            print(prime)\n",
        "            print('*'*100)\n",
        "\n",
        "            for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "                logits = model(inp[None, :])\n",
        "                next_token = sample_next_token(logits)\n",
        "                output_str += decode_token(next_token)\n",
        "                inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "            print(output_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 0/40 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18140\n",
            "4535\n",
            "training loss: 6.579105377197266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "generating:   0%|          | 0/512 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation loss: 4.936424255371094\n",
            "robenej\r\n",
            "z biokvapaliny, takzvanej tekutej biomasy.\r\n",
            "Pri slnecnych elektrarnach s vykonom do 30 kilowattov je na Slovensku\r\n",
            "vykupna cena na urovni 0,088 eura za kilowatthodinu. Za nami nasleduje\r\n",
            "Ceska republika s vykupnou cenou 0,095 eura za kilowatthodinu a Madarsko\r\n",
            "s vykupnou cenou 0,102 eura za kilowatthodinu. Najviac zaplatia za elektrinu\r\n",
            "vyrobenu v slnecnych elektrarnach Francuzi, a to v priemere 0,135 eura\r\n",
            "za kilowatthodinu.\r\n",
            "Najnizsiu vykupnu cenu ma Slovensko aj pri veternych elektrarnach, a\r\n",
            "to 0,070 eura za kilowatthodinu. Za veternu energiu si najviac priplatia\r\n",
            "Madari, ked ich vykupna cena elektriny pri tomto obnovitelnom zdroji sa\r\n",
            "pohybuje na urovni 0,114 eura za kilowatthodinu. Prvenstvo v najnizsej\r\n",
            "vykupnej cene ma Slovensko aj pri vodnych elektrarnach s vykonom do pat\r\n",
            "megawattov. Kym u nas je hodnota vykupnej ceny pri tomto zdroji na urovni\r\n",
            "0,070 eura za kilowatthodinu, v Madarsku to je 0,114 eura a v Ceskej\r\n",
            "republike 0,121 eura za kilowatthodinu.\r\n",
            "Celkom stedre vykupne ceny zelenej elektriny ma Slovensko nastavene pri\r\n",
            "geotermalnej energii. U nas by sme v pripade existencie nejakej\r\n",
            "geotermalnej elektrarne zaplatili za elektrinu vyrobenu v nej 0,155 eura za\r\n",
            "kilowatthodinu. Vyssiu vykupnu cenu uz ma len Francuzsko (0,240 eura) a\r\n",
            "Nemecko (0,252 eura). Najnizsiu vykupnu cenu pri tomto druhu\r\n",
            "obnovitelneho zdroja ma Rakusko, a to 0,074 eura za kilowatthodinu.\r\n",
            "Pri biomase je najnizsia vykupna cena v Nemecku (0,081 eura) a\r\n",
            "najvyssia v Rakusku (0,146 eura). Slovensko ma tretiu najnizsiu\r\n",
            "vykupnu cenu elektriny vyrobenej prostrednictvom biomasy, a to 0,096 eura za\r\n",
            "kilowatthodinu. Inak je to uz pri biokvapaline, takzvanej tekutej biomase.\r\n",
            "Najvyssiu vykupnu cenu tu ma Madarsko (0,114 eura) nasledovane\r\n",
            "Slovenskom (0,091 eura). Najnizsiu vykupnu cenu pri tomto obnovitelnom\r\n",
            "zdroji ma Rakusko, a to 0,057 eura za kilowatthodinu. Slovensko nie je\r\n",
            "najlacnejsie ani pri bioplyne. Najnizsiu vykupnu cenu elektriny vyrobenej\r\n",
            "z bioplynu ma Rakusko (0,049 eura), najvyssiu Madarsko (0,114 eura).\r\n",
            "Slovaci zaplatia za elektrinu vyrobenu prostrednictvom bioplynu 0,073 eura\r\n",
            "za kilowatthodinu, cize tretiu najvyssiu a tretiu najnizsiu.\r\n",
            "Slovensko ma podla Uradu pre regulaciu sietovych odvetvi vysoky\r\n",
            "potencial vyuzivania obnovitelnych zdrojov energie. \"Ak porovname ich\r\n",
            "vyuzivanie za poslednych viac ako desat rokov, tak zistime, ze kym v roku\r\n",
            "2002 bol podiel obnovitelnych zdrojov na urovni asi 1,6 % z celkovej\r\n",
            "spotreby primarnych energetickych zdrojov, tak v sucasnosti je to uz nad\r\n",
            "10 %, konstatoval vo svoje analyze regulator.\r\n",
            "Regulator porovnaval vykupne ceny elektriny vyrobenej v obnovitelnych\r\n",
            "zdrojoch v okolitych krajinach okrem Polska aj v Nemecku a vo Francuzsku.\r\n",
            "Teda v statoch, kde je system podpory obnovitelnych zdrojov\r\n",
            "porovnatelny. V Polsku nie je zavedeny system vykupnych cien,\r\n",
            "elektricka energia z obnovitelnych zdrojov je podporovana prostrednictvom\r\n",
            "systemu kvot a danovych ulav a do celkoveho porovnania nie je preto\r\n",
            "zaclenene.Na priblizenie, len nieco viac ako jedna desatina z tejto obrovskej sumy\r\n",
            "staci na vyrovnanie celeho dlhu Slovenska. Nam pritom vytvorenie dlhu\r\n",
            "41 miliard eur trvalo 23 rokov a peniaze boli pouzite na vybudovanie\r\n",
            "automobiloveho priemyslu, postavenie dialnic ci vodneho diela\r\n",
            "Gabcikovo.\r\n",
            "V trhovej ekonomike aj sukromne banky financuju hospodarsky rast\r\n",
            "poskytovanim uverov pre ludi, firmy a staty. Preto krach velkych\r\n",
            "financnych domov rovnako ako zbankrotovanie statu prinesie do zivota\r\n",
            "beznych ludi znizovanie miezd a prepustanie zo zamestnania. V roku\r\n",
            "2008 krach americkej banky Lehman Brothers aj na vzdialenom Slovensku sposobil\r\n",
            "zvysenie nezamestnanosti zo 7,36 percenta na 14,8 percenta. Tento razantny\r\n",
            "rast poctu ludi bez prace obyvatelia krajiny pod Tatrami na vlastnej kozi\r\n",
            "zazili od augusta 2008 do januara 2013. Rovnaky scenar sa moze\r\n",
            "zopakovat po krachu financnych domov nachadzajucich sa na Apeninskom\r\n",
            "polostrove. V sucasnosti nezamestnanost na Slovensku dosahuje uroven\r\n",
            "9,64 percenta.\r\n",
            "Taliansky bankovy system tazia okovy zlyhanych uverov v suhrnnej\r\n",
            "hodnote 400 \n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "generating:   0%|          | 1/512 [00:01<12:19,  1.45s/it]\u001b[A\n",
            "generating:   0%|          | 2/512 [00:02<12:09,  1.43s/it]\u001b[A\n",
            "generating:   1%|          | 3/512 [00:04<12:10,  1.44s/it]\u001b[A\n",
            "generating:   1%|          | 4/512 [00:05<12:15,  1.45s/it]\u001b[A\n",
            "generating:   1%|          | 5/512 [00:07<12:22,  1.46s/it]\u001b[A\n",
            "generating:   1%|          | 6/512 [00:08<12:23,  1.47s/it]\u001b[A\n",
            "generating:   1%|▏         | 7/512 [00:10<12:16,  1.46s/it]\u001b[A\n",
            "generating:   2%|▏         | 8/512 [00:11<12:17,  1.46s/it]\u001b[A\n",
            "generating:   2%|▏         | 9/512 [00:13<12:16,  1.46s/it]\u001b[A\n",
            "generating:   2%|▏         | 10/512 [00:14<12:15,  1.47s/it]\u001b[A\n",
            "generating:   2%|▏         | 11/512 [00:16<12:16,  1.47s/it]\u001b[A\n",
            "generating:   2%|▏         | 12/512 [00:17<12:16,  1.47s/it]\u001b[A\n",
            "generating:   3%|▎         | 13/512 [00:19<12:20,  1.48s/it]\u001b[A\n",
            "generating:   3%|▎         | 14/512 [00:20<12:14,  1.48s/it]\u001b[A\n",
            "generating:   3%|▎         | 15/512 [00:21<12:14,  1.48s/it]\u001b[A\n",
            "generating:   3%|▎         | 16/512 [00:23<12:13,  1.48s/it]\u001b[A\n",
            "generating:   3%|▎         | 17/512 [00:24<12:12,  1.48s/it]\u001b[A\n",
            "generating:   4%|▎         | 18/512 [00:26<12:02,  1.46s/it]\u001b[A\n",
            "generating:   4%|▎         | 19/512 [00:27<11:53,  1.45s/it]\u001b[A\n",
            "generating:   4%|▍         | 20/512 [00:29<11:59,  1.46s/it]\u001b[A\n",
            "generating:   4%|▍         | 21/512 [00:30<12:00,  1.47s/it]\u001b[A\n",
            "generating:   4%|▍         | 22/512 [00:32<11:59,  1.47s/it]\u001b[A\n",
            "generating:   4%|▍         | 23/512 [00:33<11:51,  1.46s/it]\u001b[A\n",
            "generating:   5%|▍         | 24/512 [00:35<11:48,  1.45s/it]\u001b[A\n",
            "generating:   5%|▍         | 25/512 [00:36<11:48,  1.45s/it]\u001b[A\n",
            "generating:   5%|▌         | 26/512 [00:38<11:46,  1.45s/it]\u001b[A\n",
            "generating:   5%|▌         | 27/512 [00:39<11:47,  1.46s/it]\u001b[A\n",
            "generating:   5%|▌         | 28/512 [00:40<11:50,  1.47s/it]\u001b[A\n",
            "generating:   6%|▌         | 29/512 [00:42<11:47,  1.47s/it]\u001b[A\n",
            "generating:   6%|▌         | 30/512 [00:43<11:47,  1.47s/it]\u001b[A\n",
            "generating:   6%|▌         | 31/512 [00:45<11:48,  1.47s/it]\u001b[A\n",
            "generating:   6%|▋         | 32/512 [00:46<11:51,  1.48s/it]\u001b[A\n",
            "generating:   6%|▋         | 33/512 [00:48<11:49,  1.48s/it]\u001b[A\n",
            "generating:   7%|▋         | 34/512 [00:49<11:44,  1.47s/it]\u001b[A\n",
            "generating:   7%|▋         | 35/512 [00:51<11:34,  1.46s/it]\u001b[A\n",
            "generating:   7%|▋         | 36/512 [00:52<11:26,  1.44s/it]\u001b[A\n",
            "generating:   7%|▋         | 37/512 [00:54<11:34,  1.46s/it]\u001b[A\n",
            "generating:   7%|▋         | 38/512 [00:55<11:35,  1.47s/it]\u001b[A\n",
            "generating:   8%|▊         | 39/512 [00:57<11:40,  1.48s/it]\u001b[A\n",
            "generating:   8%|▊         | 40/512 [00:58<11:38,  1.48s/it]\u001b[A\n",
            "generating:   8%|▊         | 41/512 [01:00<11:39,  1.48s/it]\u001b[A\n",
            "generating:   8%|▊         | 42/512 [01:01<11:32,  1.47s/it]\u001b[A\n",
            "generating:   8%|▊         | 43/512 [01:02<11:22,  1.46s/it]\u001b[A\n",
            "generating:   9%|▊         | 44/512 [01:04<11:20,  1.45s/it]\u001b[A\n",
            "generating:   9%|▉         | 45/512 [01:05<11:23,  1.46s/it]\u001b[A\n",
            "generating:   9%|▉         | 46/512 [01:07<11:27,  1.47s/it]\u001b[A\n",
            "generating:   9%|▉         | 47/512 [01:08<11:22,  1.47s/it]\u001b[A\n",
            "generating:   9%|▉         | 48/512 [01:10<11:16,  1.46s/it]\u001b[A\n",
            "generating:  10%|▉         | 49/512 [01:11<11:18,  1.47s/it]\u001b[A\n",
            "generating:  10%|▉         | 50/512 [01:13<11:16,  1.47s/it]\u001b[A\n",
            "generating:  10%|▉         | 51/512 [01:14<11:17,  1.47s/it]\u001b[A\n",
            "generating:  10%|█         | 52/512 [01:16<11:19,  1.48s/it]\u001b[A\n",
            "generating:  10%|█         | 53/512 [01:17<11:21,  1.48s/it]\u001b[A\n",
            "generating:  11%|█         | 54/512 [01:19<11:22,  1.49s/it]\u001b[A\n",
            "generating:  11%|█         | 55/512 [01:20<11:14,  1.48s/it]\u001b[A\n",
            "generating:  11%|█         | 56/512 [01:22<11:12,  1.48s/it]\u001b[A\n",
            "generating:  11%|█         | 57/512 [01:23<11:12,  1.48s/it]\u001b[A\n",
            "generating:  11%|█▏        | 58/512 [01:25<11:10,  1.48s/it]\u001b[A\n",
            "generating:  12%|█▏        | 59/512 [01:26<11:01,  1.46s/it]\u001b[A\n",
            "generating:  12%|█▏        | 60/512 [01:27<10:53,  1.45s/it]\u001b[A\n",
            "generating:  12%|█▏        | 61/512 [01:29<10:57,  1.46s/it]\u001b[A\n",
            "generating:  12%|█▏        | 62/512 [01:30<10:57,  1.46s/it]\u001b[A\n",
            "generating:  12%|█▏        | 63/512 [01:32<10:59,  1.47s/it]\u001b[A\n",
            "generating:  12%|█▎        | 64/512 [01:33<11:01,  1.48s/it]\u001b[A\n",
            "generating:  13%|█▎        | 65/512 [01:35<10:58,  1.47s/it]\u001b[A\n",
            "generating:  13%|█▎        | 66/512 [01:36<11:02,  1.48s/it]\u001b[A\n",
            "generating:  13%|█▎        | 67/512 [01:38<11:02,  1.49s/it]\u001b[A\n",
            "generating:  13%|█▎        | 68/512 [01:39<10:58,  1.48s/it]\u001b[A\n",
            "generating:  13%|█▎        | 69/512 [01:41<10:54,  1.48s/it]\u001b[A\n",
            "generating:  14%|█▎        | 70/512 [01:42<10:52,  1.48s/it]\u001b[A\n",
            "generating:  14%|█▍        | 71/512 [01:44<10:49,  1.47s/it]\u001b[A\n",
            "generating:  14%|█▍        | 72/512 [01:45<10:47,  1.47s/it]\u001b[A\n",
            "generating:  14%|█▍        | 73/512 [01:47<10:48,  1.48s/it]\u001b[A\n",
            "generating:  14%|█▍        | 74/512 [01:48<10:47,  1.48s/it]\u001b[A\n",
            "generating:  15%|█▍        | 75/512 [01:50<10:43,  1.47s/it]\u001b[A\n",
            "generating:  15%|█▍        | 76/512 [01:51<10:30,  1.45s/it]\u001b[A\n",
            "generating:  15%|█▌        | 77/512 [01:52<10:27,  1.44s/it]\u001b[A\n",
            "generating:  15%|█▌        | 78/512 [01:54<10:32,  1.46s/it]\u001b[A\n",
            "generating:  15%|█▌        | 79/512 [01:55<10:39,  1.48s/it]\u001b[A\n",
            "generating:  16%|█▌        | 80/512 [01:57<10:47,  1.50s/it]\u001b[A\n",
            "generating:  16%|█▌        | 81/512 [01:59<10:52,  1.51s/it]\u001b[A\n",
            "generating:  16%|█▌        | 82/512 [02:00<10:51,  1.52s/it]\u001b[A\n",
            "generating:  16%|█▌        | 83/512 [02:02<10:46,  1.51s/it]\u001b[A\n",
            "generating:  16%|█▋        | 84/512 [02:03<10:35,  1.48s/it]\u001b[A\n",
            "generating:  17%|█▋        | 85/512 [02:05<10:38,  1.49s/it]\u001b[A\n",
            "generating:  17%|█▋        | 86/512 [02:06<10:36,  1.49s/it]\u001b[A\n",
            "generating:  17%|█▋        | 87/512 [02:07<10:32,  1.49s/it]\u001b[A\n",
            "generating:  17%|█▋        | 88/512 [02:09<10:26,  1.48s/it]\u001b[A\n",
            "generating:  17%|█▋        | 89/512 [02:10<10:22,  1.47s/it]\u001b[A\n",
            "generating:  18%|█▊        | 90/512 [02:12<10:20,  1.47s/it]\u001b[A\n",
            "generating:  18%|█▊        | 91/512 [02:13<10:19,  1.47s/it]\u001b[A\n",
            "generating:  18%|█▊        | 92/512 [02:15<10:17,  1.47s/it]\u001b[A\n",
            "generating:  18%|█▊        | 93/512 [02:16<10:17,  1.47s/it]\u001b[A\n",
            "generating:  18%|█▊        | 94/512 [02:18<10:19,  1.48s/it]\u001b[A\n",
            "generating:  19%|█▊        | 95/512 [02:19<10:18,  1.48s/it]\u001b[A\n",
            "generating:  19%|█▉        | 96/512 [02:21<10:14,  1.48s/it]\u001b[A\n",
            "generating:  19%|█▉        | 97/512 [02:22<10:13,  1.48s/it]\u001b[A\n",
            "generating:  19%|█▉        | 98/512 [02:24<10:13,  1.48s/it]\u001b[A\n",
            "generating:  19%|█▉        | 99/512 [02:25<10:10,  1.48s/it]\u001b[A\n",
            "generating:  20%|█▉        | 100/512 [02:27<09:59,  1.45s/it]\u001b[A\n",
            "generating:  20%|█▉        | 101/512 [02:28<09:56,  1.45s/it]\u001b[A\n",
            "generating:  20%|█▉        | 102/512 [02:30<09:58,  1.46s/it]\u001b[A\n",
            "generating:  20%|██        | 103/512 [02:31<09:58,  1.46s/it]\u001b[A\n",
            "generating:  20%|██        | 104/512 [02:32<09:56,  1.46s/it]\u001b[A\n",
            "generating:  21%|██        | 105/512 [02:34<09:48,  1.44s/it]\u001b[A\n",
            "generating:  21%|██        | 106/512 [02:35<09:51,  1.46s/it]\u001b[A\n",
            "generating:  21%|██        | 107/512 [02:37<09:49,  1.46s/it]\u001b[A\n",
            "generating:  21%|██        | 108/512 [02:38<09:49,  1.46s/it]\u001b[A\n",
            "generating:  21%|██▏       | 109/512 [02:40<09:49,  1.46s/it]\u001b[A\n",
            "generating:  21%|██▏       | 110/512 [02:41<09:49,  1.47s/it]\u001b[A\n",
            "generating:  22%|██▏       | 111/512 [02:43<09:47,  1.47s/it]\u001b[A\n",
            "generating:  22%|██▏       | 112/512 [02:44<09:49,  1.47s/it]\u001b[A\n",
            "generating:  22%|██▏       | 113/512 [02:46<09:49,  1.48s/it]\u001b[A\n",
            "generating:  22%|██▏       | 114/512 [02:47<09:48,  1.48s/it]\u001b[A\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWHy-TfDiyhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}