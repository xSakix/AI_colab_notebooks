{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYlYZe7qkyZbIT76MKmhB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/imdb_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvPSqJkBpLXq"
      },
      "source": [
        "# IMDB DNN\n",
        "\n",
        "Lets do the IMDB dataset with a simple DNN. The first one is in numpy and second will be done in pytorch, but only using tensor for the GPU. Not using backwards or any NN functionality, as the goal is to implement it and learn how it works behind the scenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGuAsvRTqf31"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "torch.manual_seed(2019)\n",
        "np.random.seed(42)\n",
        "EPS = torch.finfo(torch.float32).eps"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qoG1prgrx0f"
      },
      "source": [
        "def convert_to_array(x):\n",
        "    x_temp = []\n",
        "\n",
        "    for x in x_train:\n",
        "        if len(x) < maxlen:\n",
        "            for i in range(maxlen - len(x)):\n",
        "                x.append(0.0)\n",
        "        elif len(x) > maxlen:\n",
        "            x = x[0:maxlen]\n",
        "\n",
        "        x_temp.append(x)\n",
        "\n",
        "    return np.array(x_temp)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    other = torch.zeros(z.size()).cuda()\n",
        "    return torch.maximum(other,z).cuda()\n",
        "\n",
        "\n",
        "def back_relu(Z, dA):\n",
        "    dZ = dA.detach().clone().cuda()  # just converting dz to a correct object.\n",
        "\n",
        "    # When z <= 0, you should set dz to 0 as well.\n",
        "    # normaly it would be:\n",
        "    # Z[Z <= 0] = 0.\n",
        "    # Z[Z > 0] = 1\n",
        "    # dZ = dA*Z\n",
        "    # so for short we have this\n",
        "    dZ[Z <= 0.] = 0.\n",
        "    # which says, that make dZ a copy od dA,then where Z <= 0 we have 0\n",
        "    # and where Z > 0 we have 1*dA = dA\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1. / (1. + torch.exp(-z).cuda()+EPS)\n",
        "\n",
        "\n",
        "def back_sigmoid(Z, dA):\n",
        "    s = 1 / (1 + torch.exp(-Z).cuda()+EPS)\n",
        "    dZ = dA * s * (1 - s)\n",
        "    return dZ"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1dK-00CNrMi",
        "outputId": "5953fb9d-823f-4459-f19f-c9ceabd89815"
      },
      "source": [
        "x=torch.randn(1,5).cuda()\n",
        "dx=torch.randn(1,5).cuda()\n",
        "print(x)\n",
        "print(relu(x))\n",
        "print(\"*\"*80)\n",
        "print(x)\n",
        "print(dx)\n",
        "#where x <= 0 dx will 0\n",
        "print(back_relu(x,dx))\n",
        "print(\"*\"*80)\n",
        "print(sigmoid(x))\n",
        "print(back_sigmoid(x,dx))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1187,  0.2110,  0.7463, -0.6136, -0.1186]], device='cuda:0')\n",
            "tensor([[0.0000, 0.2110, 0.7463, 0.0000, 0.0000]], device='cuda:0')\n",
            "********************************************************************************\n",
            "tensor([[-0.1187,  0.2110,  0.7463, -0.6136, -0.1186]], device='cuda:0')\n",
            "tensor([[1.5565, 1.3662, 1.0199, 2.4644, 1.1630]], device='cuda:0')\n",
            "tensor([[0.0000, 1.3662, 1.0199, 0.0000, 0.0000]], device='cuda:0')\n",
            "********************************************************************************\n",
            "tensor([[0.4704, 0.5526, 0.6784, 0.3512, 0.4704]], device='cuda:0')\n",
            "tensor([[0.3878, 0.3378, 0.2225, 0.5616, 0.2897]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxVzrG4frDWR",
        "outputId": "25f148d9-56be-4d3b-9421-9338b796d0a5"
      },
      "source": [
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQCmUfzhq348",
        "outputId": "ea9cffa7-564d-41aa-b74d-37bde65d439e"
      },
      "source": [
        "x_train = convert_to_array(x_train)\n",
        "x_val = convert_to_array(x_val)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1).T\n",
        "y_val = y_val.reshape(y_val.shape[0], -1).T\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_val = x_val.reshape(x_val.shape[0], -1).T\n",
        "\n",
        "print(\"*\" * 80)\n",
        "print(\"x_train:{}\".format(x_train.shape))\n",
        "print(\"x_val:{}\".format(x_val.shape))\n",
        "print(\"y_train:{}\".format(y_train.shape))\n",
        "print(\"y_val:{}\".format(y_val.shape))\n",
        "print(\"*\" * 80)\n",
        "\n",
        "assert (x_train.shape == (maxlen, 25000))\n",
        "assert (y_train.shape == (1, 25000))\n",
        "assert (x_val.shape == (maxlen, 25000))\n",
        "assert (y_val.shape == (1, 25000))\n",
        "\n",
        "print(\"*\" * 80)\n",
        "\n",
        "print(\"max x_train before:{}\".format(np.max(x_train)))\n",
        "print(\"max x_val before:{}\".format(np.max(x_val)))\n",
        "print(\"min before:{}, {}\".format(np.min(x_train), np.min(x_val)))\n",
        "\n",
        "# norm didn't work well\n",
        "# norm = np.linalg.norm(x_train, ord=2)\n",
        "# print(\"norm={}\".format(norm))\n",
        "\n",
        "# normalizing around max_features works well\n",
        "# x_train = x_train / max_features\n",
        "# x_val = x_val / max_features\n",
        "\n",
        "# centering around mean\n",
        "x_mean = np.mean(x_train)\n",
        "x_std = np.std(x_train)\n",
        "print(\"(mean,std)=({},{})\".format(x_mean, x_std))\n",
        "x_train = (x_train - x_mean) / x_std\n",
        "x_val = (x_val - x_mean) / x_std\n",
        "\n",
        "print(\"max x_train after norm:{}\".format(np.max(x_train)))\n",
        "print(\"max x_val after norm:{}\".format(np.max(x_val)))\n",
        "print(\"min after norm:{}, {}\".format(np.min(x_train), np.min(x_val)))\n",
        "\n",
        "# assert ((x_train >= 0.).all() and (x_train < 1.).all())\n",
        "\n",
        "print(\"*\" * 80)\n",
        "\n",
        "print(\"y_train unique vals:{}\".format(np.unique(y_train)))\n",
        "print(\"y_val unique vals:{}\".format(np.unique(y_train)))\n",
        "\n",
        "print(\"*\" * 80)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************************************************************************\n",
            "x_train:(200, 25000)\n",
            "x_val:(200, 25000)\n",
            "y_train:(1, 25000)\n",
            "y_val:(1, 25000)\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "max x_train before:19999.0\n",
            "max x_val before:19999.0\n",
            "min before:0.0, 0.0\n",
            "(mean,std)=(896.850569,2520.963839741494)\n",
            "max x_train after norm:7.5773198845084515\n",
            "max x_val after norm:7.5773198845084515\n",
            "min after norm:-0.35575701438540475, -0.35575701438540475\n",
            "********************************************************************************\n",
            "y_train unique vals:[0 1]\n",
            "y_val unique vals:[0 1]\n",
            "********************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dpCpO7EsATu"
      },
      "source": [
        "# 2 layer network\n",
        "m = x_train.shape[1]\n",
        "n_x = x_train.shape[0]\n",
        "n_h = 128\n",
        "n_y = 1\n",
        "# init params\n",
        "W1 = torch.randn(n_h, n_x).cuda() * 0.01\n",
        "b1 = torch.zeros((n_h, 1)).cuda()\n",
        "W2 = torch.randn(n_y, n_h).cuda() * 0.01\n",
        "b2 = torch.zeros((n_y, 1)).cuda()\n",
        "\n",
        "assert (W1.size() == (n_h, n_x))\n",
        "assert (b1.size() == (n_h, 1))\n",
        "assert (W2.size() == (n_y, n_h))\n",
        "assert (b2.size() == (n_y, 1))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua57cU7FsOaU",
        "outputId": "237f71b9-625b-43a6-dcde-c2099ceb494e"
      },
      "source": [
        "costs = []\n",
        "n_iter = 100000\n",
        "learning_rate = 0.01\n",
        "\n",
        "x_train = torch.tensor(x_train,dtype=torch.float32).cuda()\n",
        "y_train = torch.tensor(y_train,dtype=torch.float32).cuda()\n",
        "\n",
        "for i in range(0, n_iter):\n",
        "    # forward\n",
        "    # A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
        "    # do a forward pass over relu\n",
        "    # print(\"W1.shape:{}\".format(W1.shape))\n",
        "    # print(\"X.shape:{}\".format(x_train.shape))\n",
        "    # m x n * n x p = m x p\n",
        "    # (5, 200) * (200, 25000)\n",
        "    Z1 = torch.mm(W1, x_train).cuda() + b1\n",
        "    assert (Z1.size() == (n_h, m))\n",
        "    A1 = relu(Z1)\n",
        "    assert (A1.size() == (n_h, m))\n",
        "    # A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
        "    Z2 = torch.mm(W2, A1).cuda() + b2\n",
        "    assert (Z2.size() == (n_y, m))\n",
        "    A2 = sigmoid(Z2)\n",
        "    assert (A2.size() == (n_y, m))\n",
        "\n",
        "    # compute cost\n",
        "    cost = -(1 / m) * torch.sum(y_train * torch.log(A2).cuda() + (1 - y_train) * torch.log(1 - A2).cuda()).cuda()\n",
        "    cost = torch.squeeze(cost)\n",
        "    # backward compute loss\n",
        "    dA2 = -(torch.divide(y_train, A2).cuda() - torch.divide(1 - y_train, 1 - A2).cuda())\n",
        "    # print(\"dA2.shape={}\".format(dA2.shape))\n",
        "    assert (dA2.size() == A2.size())\n",
        "    # backward\n",
        "    dZ2 = back_sigmoid(Z2, dA2)\n",
        "    assert (dZ2.size() == dA2.size())\n",
        "\n",
        "    dW2 = (1 / m) * torch.mm(dZ2, A1.T).cuda()\n",
        "    db2 = (1 / m) * torch.sum(dZ2, dim=1, keepdims=True).cuda()\n",
        "    dA1 = torch.mm(W2.T, dZ2).cuda()\n",
        "\n",
        "    assert (dA1.size() == A1.size())\n",
        "    assert (dW2.size() == W2.size())\n",
        "    assert (db2.size() == b2.size())\n",
        "\n",
        "    dZ1 = back_relu(Z1, dA1)\n",
        "    assert (dZ1.size() == dA1.size())\n",
        "\n",
        "    dW1 = (1 / m) * torch.mm(dZ1, x_train.T).cuda()\n",
        "    db1 = (1 / m) * torch.sum(dZ1, dim=1, keepdims=True).cuda()\n",
        "    assert (dW1.size() == W1.size())\n",
        "    assert (db1.size() == b1.size())\n",
        "\n",
        "    # update params\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    # print stats\n",
        "    if i % 1000 == 0:\n",
        "        print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "    if i % 1000 == 0:\n",
        "        costs.append(cost)\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.6931062340736389\n",
            "Cost after iteration 1000: 0.6920350790023804\n",
            "Cost after iteration 2000: 0.6901419162750244\n",
            "Cost after iteration 3000: 0.6875673532485962\n",
            "Cost after iteration 4000: 0.6852700710296631\n",
            "Cost after iteration 5000: 0.6830765604972839\n",
            "Cost after iteration 6000: 0.6806917190551758\n",
            "Cost after iteration 7000: 0.6780092120170593\n",
            "Cost after iteration 8000: 0.6750380396842957\n",
            "Cost after iteration 9000: 0.6717337369918823\n",
            "Cost after iteration 10000: 0.6679622530937195\n",
            "Cost after iteration 11000: 0.6635579466819763\n",
            "Cost after iteration 12000: 0.6583427786827087\n",
            "Cost after iteration 13000: 0.6521320343017578\n",
            "Cost after iteration 14000: 0.6447542309761047\n",
            "Cost after iteration 15000: 0.6360668540000916\n",
            "Cost after iteration 16000: 0.6259809136390686\n",
            "Cost after iteration 17000: 0.6145573854446411\n",
            "Cost after iteration 18000: 0.601876437664032\n",
            "Cost after iteration 19000: 0.5880638957023621\n",
            "Cost after iteration 20000: 0.5733454823493958\n",
            "Cost after iteration 21000: 0.5580275058746338\n",
            "Cost after iteration 22000: 0.5423973202705383\n",
            "Cost after iteration 23000: 0.5266596078872681\n",
            "Cost after iteration 24000: 0.5110139846801758\n",
            "Cost after iteration 25000: 0.4956114590167999\n",
            "Cost after iteration 26000: 0.4805349111557007\n",
            "Cost after iteration 27000: 0.465840220451355\n",
            "Cost after iteration 28000: 0.4516195058822632\n",
            "Cost after iteration 29000: 0.43787917494773865\n",
            "Cost after iteration 30000: 0.42459163069725037\n",
            "Cost after iteration 31000: 0.41178271174430847\n",
            "Cost after iteration 32000: 0.399410218000412\n",
            "Cost after iteration 33000: 0.38745394349098206\n",
            "Cost after iteration 34000: 0.3758987486362457\n",
            "Cost after iteration 35000: 0.36474964022636414\n",
            "Cost after iteration 36000: 0.3539586365222931\n",
            "Cost after iteration 37000: 0.34361952543258667\n",
            "Cost after iteration 38000: 0.33368992805480957\n",
            "Cost after iteration 39000: 0.3241196274757385\n",
            "Cost after iteration 40000: 0.3148539066314697\n",
            "Cost after iteration 41000: 0.3058989644050598\n",
            "Cost after iteration 42000: 0.29729530215263367\n",
            "Cost after iteration 43000: 0.28903070092201233\n",
            "Cost after iteration 44000: 0.28105929493904114\n",
            "Cost after iteration 45000: 0.27335238456726074\n",
            "Cost after iteration 46000: 0.26593515276908875\n",
            "Cost after iteration 47000: 0.2588209807872772\n",
            "Cost after iteration 48000: 0.2519688904285431\n",
            "Cost after iteration 49000: 0.24539195001125336\n",
            "Cost after iteration 50000: 0.23903819918632507\n",
            "Cost after iteration 51000: 0.23292924463748932\n",
            "Cost after iteration 52000: 0.22702035307884216\n",
            "Cost after iteration 53000: 0.221299409866333\n",
            "Cost after iteration 54000: 0.21579036116600037\n",
            "Cost after iteration 55000: 0.21046783030033112\n",
            "Cost after iteration 56000: 0.205328568816185\n",
            "Cost after iteration 57000: 0.20036581158638\n",
            "Cost after iteration 58000: 0.1955682635307312\n",
            "Cost after iteration 59000: 0.19094659388065338\n",
            "Cost after iteration 60000: 0.18647587299346924\n",
            "Cost after iteration 61000: 0.18215782940387726\n",
            "Cost after iteration 62000: 0.17795944213867188\n",
            "Cost after iteration 63000: 0.17389515042304993\n",
            "Cost after iteration 64000: 0.16995511949062347\n",
            "Cost after iteration 65000: 0.16614869236946106\n",
            "Cost after iteration 66000: 0.16247251629829407\n",
            "Cost after iteration 67000: 0.15890851616859436\n",
            "Cost after iteration 68000: 0.15547026693820953\n",
            "Cost after iteration 69000: 0.15213122963905334\n",
            "Cost after iteration 70000: 0.14887908101081848\n",
            "Cost after iteration 71000: 0.1457148641347885\n",
            "Cost after iteration 72000: 0.14265519380569458\n",
            "Cost after iteration 73000: 0.1396883726119995\n",
            "Cost after iteration 74000: 0.13680286705493927\n",
            "Cost after iteration 75000: 0.13399046659469604\n",
            "Cost after iteration 76000: 0.13125790655612946\n",
            "Cost after iteration 77000: 0.12859980762004852\n",
            "Cost after iteration 78000: 0.12601132690906525\n",
            "Cost after iteration 79000: 0.12351272255182266\n",
            "Cost after iteration 80000: 0.12109942734241486\n",
            "Cost after iteration 81000: 0.1187547892332077\n",
            "Cost after iteration 82000: 0.11648039519786835\n",
            "Cost after iteration 83000: 0.11427416652441025\n",
            "Cost after iteration 84000: 0.11212347447872162\n",
            "Cost after iteration 85000: 0.11001993715763092\n",
            "Cost after iteration 86000: 0.10797743499279022\n",
            "Cost after iteration 87000: 0.10599715262651443\n",
            "Cost after iteration 88000: 0.10407041758298874\n",
            "Cost after iteration 89000: 0.10220226645469666\n",
            "Cost after iteration 90000: 0.10038700699806213\n",
            "Cost after iteration 91000: 0.09862534701824188\n",
            "Cost after iteration 92000: 0.09691459685564041\n",
            "Cost after iteration 93000: 0.09525036811828613\n",
            "Cost after iteration 94000: 0.09362275153398514\n",
            "Cost after iteration 95000: 0.09203498065471649\n",
            "Cost after iteration 96000: 0.09049011766910553\n",
            "Cost after iteration 97000: 0.08898509293794632\n",
            "Cost after iteration 98000: 0.0875144675374031\n",
            "Cost after iteration 99000: 0.08607985079288483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bw7tgVZhCeM",
        "outputId": "18ab8de3-aa24-490c-a1f4-bfeca6a11a80"
      },
      "source": [
        "#predict\n",
        "p = torch.zeros((1, x_train.shape[1])).cuda()\n",
        "Z1 = torch.mm(W1, x_train).cuda() + b1\n",
        "A1 = relu(Z1)\n",
        "Z2 = torch.mm(W2, A1).cuda() + b2\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "# convert probas to 0/1 predictions\n",
        "for i in range(0, A2.shape[1]):\n",
        "    if A2[0, i] > 0.5:\n",
        "        p[0, i] = 1\n",
        "    else:\n",
        "        p[0, i] = 0\n",
        "\n",
        "print(\"Accuracy on training set: \" + str(torch.sum((p == y_train)/x_train.shape[1]).cuda()))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: tensor(0.9840, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5VbprF3sczX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "bc8b5766-5801-4239-bdde-0854cd68a14f"
      },
      "source": [
        "x_val = torch.tensor(x_val,dtype=torch.float32).cuda()\n",
        "y_val = torch.tensor(y_val,dtype=torch.float32).cuda()\n",
        "\n",
        "\n",
        "#predict\n",
        "p = torch.zeros((1, x_val.shape[1])).cuda()\n",
        "Z1 = torch.mm(W1, x_val).cuda() + b1\n",
        "A1 = relu(Z1)\n",
        "Z2 = torch.mm(W2, A1).cuda() + b2\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "# convert probas to 0/1 predictions\n",
        "for i in range(0, A2.shape[1]):\n",
        "    if A2[0, i] > 0.5:\n",
        "        p[0, i] = 1\n",
        "    else:\n",
        "        p[0, i] = 0\n",
        "\n",
        "print(\"Accuracy on validation/test set: \" + str(torch.sum((p == y_val)/x_val.shape[1]).cuda()))\n",
        "\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on validation/test set: tensor(0.4967, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV9fn/8deVzQgEQlhhhI3sERFxUUULOHDV4qr229Zqa7XaoR3f2tran62tVq3ji7Ou4lbce6IiAdkz7E3YM5CE6/fHubHHNIEgOblzznk/H4/z8Nz3/bnvc9258bzPvT63uTsiIpK8UsIuQEREwqUgEBFJcgoCEZEkpyAQEUlyCgIRkSSnIBARSXIKAkl4Znacmc0Puw6R+kpBIDFlZkvNbESYNbj7R+7eI8wa9jOz4Wa2so4+6yQzm2dmu8zsPTPreIC2BUGbXcE8I6Km9TGzN8xsg5npxqMEpCCQuGdmqWHXAGAR9eL/KTNrATwH/C/QHCgCnjzALP8GvgBygd8Az5hZXjCtDHgK+F7MCpZQ1Yt/tJJ8zCzFzK43s0VmttHMnjKz5lHTnzaztWa21cw+NLPeUdMeNrN7zOxVM9sJfCPY8/i5mc0I5nnSzLKC9l/5FX6gtsH0X5rZGjNbbWbfNzM3s67VrMf7ZnaTmU0EdgGdzey7ZjbXzLab2WIz+2HQthHwGtDWzHYEr7YH+1t8TWcDs939aXcvBX4P9DeznlWsQ3dgEHCDu+9292eBmcA5AO4+390fAGYfZk1STykIJCw/Ac4ETgDaApuBu6KmvwZ0A1oCU4HHK81/AXATkA18HIw7DxgJdAL6AZce4POrbGtmI4FrgRFAV2B4DdblYuCyoJZlwHrgNKAJ8F3gNjMb5O47gVHAandvHLxW1+Bv8SUz62BmWw7wuiBo2huYvn++4LMXBeMr6w0sdvftUeOmV9NWElBa2AVI0rocuNLdVwKY2e+B5WZ2sbuXu/uD+xsG0zabWVN33xqMftHdJwbvS80M4I7gixUzewkYcIDPr67tecBD7j476rMvPMi6PLy/feCVqPcfmNmbwHFEAq0qB/xbRDd09+VAzkHqAWgMlFQat5VIWFXVdmsVbfNr8DmSALRHIGHpCDy//5csMBeoAFqZWaqZ3RwcKtkGLA3maRE1/4oqlrk26v0uIl9w1amubdtKy67qcyr7ShszG2Vmn5nZpmDdRvPV2iur9m9Rg8+uzg4ieyTRmgDbD7OtJCAFgYRlBTDK3XOiXlnuvorIYZ8xRA7PNAUKgnksav5YXb2yBmgXNdy+BvN8WYuZZQLPAn8DWrl7DvAq/6m9qroP9Lf4iuDQ0I4DvPbvvcwG+kfN1wjoQtXH+WcTObcRvbfQv5q2koAUBFIX0s0sK+qVBtwL3LT/kkYzyzOzMUH7bGAPsBFoCPy5Dmt9CviumR1hZg2JXHVzKDKATCKHZcrNbBRwStT0dUCumTWNGnegv8VXuPvyqPMLVb32n0t5HuhjZucEJ8J/B8xw93lVLHMBMA24Idg+ZxE5b/JsUI8Fy8gIhrOCwJMEoSCQuvAqsDvq9XvgdmAC8KaZbQc+A44K2j9C5KTrKmBOMK1OuPtrwB3Ae0Bx1GfvqeH824GriATKZiJ7NxOips8jcqnm4uBQUFsO/Lf4uutRQuSqn5uCOo4Cxu6fbmb3mtm9UbOMBQqDtjcD5wbLgMihq938Zw9hN6Ab9BKI6cE0ItUzsyOAWUBm5RO3IolCewQilZjZWWaWaWbNgL8ALykEJJEpCET+2w+J3AuwiMjVO1eEW45IbOnQkIhIktMegYhIkou7O4tbtGjhBQUFYZchIhJXpkyZssHd86qaFndBUFBQQFFRUdhliIjEFTNbVt00HRoSEUlyCgIRkSSnIBARSXIxDQIzG2lm882s2Myur2L6bWY2LXgtCHpeFBGROhSzk8UWeXzgXcDJwEpgsplNcPc5+9u4+zVR7X8CDIxVPSIiUrVY7hEMAYrdfbG77wXGE+lauDrnE+mMS0RE6lAsgyCfrz6wYyXVPPEo6H63E/BuNdMvM7MiMysqKan80CURETkc9eU+grHAM+5eUdVEdx8HjAMoLCz8Wn1iFC3dxMTijbTJyaJt0wa0b96A9s0akpJiB59ZRCSBxTIIVvHVpzu1C8ZVZSzw4xjWwpRlm7nt7QVfGZedmUbv/Cb0b5fD0V1yOapTLg0yUmNZhohIvROzTueCp1AtAE4iEgCTgQsqPeQbM+sJvA508hoUU1hY6F/3zuLSsgrWbStl9ZZSlm3cyazVW5m5ahtz12xjb/k+MlJTKCxoxtDOuQzp1JwB7XPISlcwiEj8M7Mp7l5Y1bSY7RG4e7mZXQm8AaQCD7r7bDO7EShy9/1PbRoLjK9JCByurPRUOuY2omNuI47ukvvl+NKyCj5fsomPFpbwcfFGbnt7Ae6QkZpCv3ZNGVzQjCM7NufIguY0bZge6zJFROpU3HVDfTh7BDW1ZddeipZu5vOlmyhauomZq7ZSVuGYQa82TTi6cy7HdmvB0M652mMQkbhwoD0CBUENlJZVMG3FFiYt3sSnizcwdfkW9pbvIys9haGdczmpZ0tO7tWa1k2z6rQuEZGaUhDUstKyCj5bvJH355fw/vz1LN24C4D+7XMY3ac1p/dvS9ucBqHWKCISTUEQQ+5O8fodvDlnHa/PWsvMVVsBGFLQnLMH5XN6/7Y0yqwvV+mKSLJSENShpRt28tL01bwwbRWLSnbSKCOVMQPzueiojvRq2yTs8kQkSSkIQuDuTF2+mScmreDlGavZU76P47vnccUJXRjauTlmupFNROqOgiBkW3eV8dikZTw0cQkbduxlUIccfj36CAoLmoddmogkCQVBPVFaVsHTRSu4891i1m/fw8jerbluVE86tWgUdmkikuAOFAR6ME0dykpP5eKjC3j/F8O5ZkR3PlxYwjdv+5C73iumvGJf2OWJSJJSEISgYUYaV4/oxvs/H86IXi255Y35nHX3J8xdsy3s0kQkCSkIQtSySRZ3XziYuy8cxJqtuxnzz4k8MWk58Xa4TkTim4KgHhjdtw1vXnMCQ7vk8uvnZ3LdszMoLauyR24RkVqnIKgnmjfK4KFLj+SqE7vyVNFKzr33E9ZvKw27LBFJAgqCeiQ1xbj2lB7c/51CFpfs5Nx7P2XZxp1hlyUiCU5BUA+N6NWKx79/FNtKyzjnnk+Zs1onkUUkdhQE9dTADs14+odHk55qfHvcp8xcuTXskkQkQSkI6rFurbJ5+vKjaZKVziUPfU7x+u1hlyQiCUhBUM+1a9aQx75/FClmXHT/56zYtCvskkQkwSgI4kCnFo149HtD2LW3nIsfmMSGHXvCLklEEoiCIE4c0aYJD313CGu2lnLFY1PYW64uKUSkdigI4sjgjs3427f6M3npZn77wkzdgSwitUKPzoozp/dvy4J127nz3WJ6tG7C947tFHZJIhLntEcQh64Z0Z1v9m7FTa/MYWLxhrDLEZE4pyCIQykpxq3nDaBLXmOuHj+N9dvVFYWIfH0xDQIzG2lm882s2Myur6bNeWY2x8xmm9kTsawnkTTKTOOuCwexY08Z1zw5jYp9Ol8gIl9PzILAzFKBu4BRQC/gfDPrValNN+BXwDHu3hv4aazqSUTdW2XzhzN6M7F4I3e/Vxx2OSISp2K5RzAEKHb3xe6+FxgPjKnU5gfAXe6+GcDd18ewnoR0XmF7xgxoy21vL+DzJZvCLkdE4lAsgyAfWBE1vDIYF6070N3MJprZZ2Y2Mob1JCQz46az+tKuWUN+9vQ0duwpD7skEYkzYZ8sTgO6AcOB84H7zCynciMzu8zMisysqKSkpI5LrP8aZ6bx9/P6s3Lzbm56ZW7Y5YhInIllEKwC2kcNtwvGRVsJTHD3MndfAiwgEgxf4e7j3L3Q3Qvz8vJiVnA8O7KgOZcd15l/f76c9+brCJuI1Fwsg2Ay0M3MOplZBjAWmFCpzQtE9gYwsxZEDhUtjmFNCe2ak7vTvVVjrntmBlt27Q27HBGJEzELAncvB64E3gDmAk+5+2wzu9HMzgiavQFsNLM5wHvAL9x9Y6xqSnRZ6ancet4ANu3cy40vzQm7HBGJExZv/dUUFhZ6UVFR2GXUa39/cz53vlvMI/8zhOO761CaiICZTXH3wqqmhX2yWGLgx9/oSue8Rvz6+Zns2quriETkwBQECSgrPZWbz+7Hys27ufXNBWGXIyL1nIIgQQ3p1JwLj+rAgxOXMH3FlrDLEZF6TEGQwK4b1ZO87Ex+/fxMyiv0IBsRqZqCIIE1yUrnhtN7M3v1Nh75dFnY5YhIPaUgSHCj+rRmeI88/v7mfNZuVXfVIvLfFAQJzsy48Yw+lO9zbnx5dtjliEg9pCBIAh1yG/KTE7vy6sy1vDdP3U+IyFcpCJLED47vTJe8RtwwYTalZRVhlyMi9YiCIElkpqXyxzF9WL5pF/d+sCjsckSkHlEQJJFhXVtwev+23P3+IpZt3Bl2OSJSTygIksxvRh9Beorx+wmzibd+pkQkNhQESaZ10yyuObk7780v4a0568IuR0TqAQVBErpkWAE9WmXzh5fmsHuvThyLJDsFQRJKT03hD2N6s2rLbu55vzjsckQkZAqCJDW0cy5nDmjLvR8sZukGnTgWSWYKgiT269FHkJGWwu9f0oljkWSmIEhiLZtk8dMR3Xh/fglv6sSxSNJSECS5/SeOb9SJY5GkpSBIctEnju96TyeORZKRgkAY2jmXswbmM+7DxSwu2RF2OSJSxxQEAsCvRvckMy2FG3THsUjSURAIAC2zs7j2lO58tHADr89aG3Y5IlKHFATypYuHduSINk248eU57NxTHnY5IlJHYhoEZjbSzOabWbGZXV/F9EvNrMTMpgWv78eyHjmwtNQU/nRmH9ZsLeX2dxaGXY6I1JGYBYGZpQJ3AaOAXsD5ZtariqZPuvuA4HV/rOqRmhncsRnnD2nPAx8vYd7abWGXIyJ1IJZ7BEOAYndf7O57gfHAmBh+ntSSX36zJ00bpPPb52exb59OHIskulgGQT6wImp4ZTCusnPMbIaZPWNm7atakJldZmZFZlZUUlISi1olSrNGGVw/qidFyzbzzJSVYZcjIjEW9snil4ACd+8HvAX8q6pG7j7O3QvdvTAvL69OC0xW5w5qx5CC5vz5tbls3LEn7HJEJIZiGQSrgOhf+O2CcV9y943uvv9b5n5gcAzrkUOQkmLcdFYfdu4p58+vzgu7HBGJoVgGwWSgm5l1MrMMYCwwIbqBmbWJGjwDmBvDeuQQdWuVzQ+P78KzU1fySfGGsMsRkRiJWRC4ezlwJfAGkS/4p9x9tpndaGZnBM2uMrPZZjYduAq4NFb1yNdz5Yld6ZjbkN+8MIvSMnVKJ5KILN66EygsLPSioqKwy0gqHy/cwEUPTOKqk7px7cndwy5HRL4GM5vi7oVVTQv7ZLHEgWO7teCsgfnc834xC9ZtD7scEallCgKpkd+eegSNM9O47tkZVOjeApGEoiCQGsltnMnvTu/FF8u38OinS8MuR0RqkYJAauzMAfkc3z2Pv74xn1VbdoddjojUEgWB1JiZ8eez+gDwm+dn6rkFIglCQSCHpF2zhvz8lB68P7+E56auOvgMIlLvKQjkkF0yrIDCjs34w0uzWbetNOxyROQwKQjkkKWmGH89tx97yvfpEJFIAlAQyNfSOa8xv/hmD96eu54Xp60OuxwROQwKAvnavntMJwZ1yOGGCbNZr0NEInFLQSBfW2qK8bdv9WdPeQXXPTtDh4hE4pSCQA5L57zGXD+yJ+/NL2H85BUHn0FE6h0FgRy27xxdwDFdc/njy3NYtnFn2OWIyCFSEMhhS0kxbjm3P6kpxs+emq6+iETijIJAakXbnAb84YzeFC3bzL0fLAq7HBE5BAoCqTVnDczntH5tuO2tBUxfsSXsckSkhhQEUmvMjJvO7EvL7Ex++uQ0du4pD7skEakBBYHUqqYN07n12wNYunEnf3x5TtjliEgNKAik1g3tnMvlJ3Rh/OQVvDZzTdjliMhBKAgkJq4Z0Z3+7XP45bMzWLFpV9jliMgBKAgkJjLSUrhz7EBwuGr8F5RV7Au7JBGphoJAYqZDbkNuPqcfXyzfwt/enB92OSJSjRoFgZl9qybjRCo7tV8bzh/Sgf/7YDHvzV8fdjkiUoWa7hH8qobjRP7LDaf3omfrbK59cpqedSxSDx0wCMxslJndCeSb2R1Rr4eBg14kbmYjzWy+mRWb2fUHaHeOmbmZFR7yGki9l5Weyt0XDqKswvnx41PZW67zBSL1ycH2CFYDRUApMCXqNQH45oFmNLNU4C5gFNALON/MelXRLhu4Gph0qMVL/Oic15i/ntuPaSu28P9emxt2OSISJe1AE919OjDdzJ5w9zIAM2sGtHf3zQdZ9hCg2N0XB/ONB8YAle8y+iPwF+AXX6N+iSOj+7bh0mEFPDRxKYUdm3NqvzZhlyQi1PwcwVtm1sTMmgNTgfvM7LaDzJMPRHdQvzIY9yUzG0QkVF450ILM7DIzKzKzopKSkhqWLPXRr0cfwaAOOfzimeksXLc97HJEhJoHQVN33wacDTzi7kcBJx3OB5tZCnAr8LODtXX3ce5e6O6FeXl5h/OxErKMtBTuvnAwDTPS+OGjU9hWWhZ2SSJJr6ZBkGZmbYDzgJdrOM8qoH3UcLtg3H7ZQB/gfTNbCgwFJuiEceJr3TSLuy8cxPJNu/jZU9PZp+cXiISqpkFwI/AGsMjdJ5tZZ2DhQeaZDHQzs05mlgGMJXKSGQB33+ruLdy9wN0LgM+AM9y96JDXQuLOkE7N+e2pR/DWnHXc+W5x2OWIJLUDnizez92fBp6OGl4MnHOQecrN7EoiAZIKPOjus83sRqDI3SccaH5JfJcMK2DGqq3c9vYCerTOZmSf1mGXJJKUzP3gu+Vm1g64EzgmGPURcLW7r4xhbVUqLCz0oiLtNCSK0rIKvj3uMxau285zPxpGz9ZNwi5JJCGZ2RR3r/LQe00PDT1E5LBO2+D1UjBO5LBkpacy7uLBNM5M4wePFLFp596wSxJJOjUNgjx3f8jdy4PXw4Au35Fa0apJFv938WDWbdvD5Y9N0Z3HInWspkGw0cwuMrPU4HURsDGWhUlyGdihGbec24/Pl2ziN8/PpCaHLEWkdtQ0CP6HyKWja4E1wLnApTGqSZLUmAH5XHVSN56espJ7P1gcdjkiSaNGVw0RuXz0kv3dSgR3GP+NSECI1JprRnRjyYad/OX1eRTkNmRUX3VDIRJrNd0j6Bfdt5C7bwIGxqYkSWZmxi3n9mNghxx++uQ0pi4/WJdWInK4ahoEKUFnc8CXewQ13ZsQOSRZ6anc/51CWjfN4vv/KmLphp1hlySS0GoaBH8HPjWzP5rZH4FPgL/GrixJdrmNM3n4u0Nwdy596HNdVioSQzUKAnd/hEiHc+uC19nu/mgsCxPp1KIR919SyOqtpfzPw5PZtfegz0ISka+hxg+vd/c57v7P4FX5mQIiMTG4Y3PuGDuQGSu3cMVjUymr0D0GIrWtxkEgEpaRfVpz01l9+WBBCb94Wr2VitQ2nfCVuHD+kA5s2rmXW96YT7NGGfzutF6YWdhliSQEBYHEjR8N78LGHXt5cOISsjPTuPaUHmGXJJIQFAQSN8yM3556BDv3lHPHu8U0zEzj8hO6hF2WSNxTEEhcSUkx/nx2X3aVVXDza/NomJHKd44uCLsskbimIJC4k5pi3Hpef3bvreB3L84mPTWF84d0CLsskbilq4YkLqWnpnDXhQP5Ro88fvXcTJ6avCLskkTiloJA4lZmWir3XDSY47q14LrnZvDslDp/YJ5IQlAQSFzLSk/lvu8UckyXFvz8mek8XaQ9A5FDpSCQuLc/DI7t2oJfPjuD8Z8vD7skkbiiIJCE0CAjEgYndM/j+udm8uhny8IuSSRuKAgkYWSlp/J/Fw9mxBEt+d8XZnHfh3rKmUhNKAgkoWSmpXL3hYM5tW8bbnp1Lre9tUDPPxY5iJgGgZmNNLP5ZlZsZtdXMf1yM5tpZtPM7GMz6xXLeiQ5ZKSlcMf5A/nW4Hbc/s5CbnplrsJA5ABidkOZmaUCdwEnAyuByWY2oVIX1k+4+71B+zOAW4GRsapJkkdqivGXc/rRKDON+z9ewpbdZdx8dl/SUrUTLFJZLO8sHgIUu/tiADMbD4wBvgwCd98W1b4RoJ9tUmtSUowbTu9FTsN0/vH2Qjbv3Ms/LxhEg4zUsEsTqVdi+fMoH4i+qHtlMO4rzOzHZraIyKMvr6pqQWZ2mZkVmVlRSUlJTIqVxGRm/HREd/50Zh/enb+eix6YxJZdeuylSLTQ95Pd/S537wJcB/y2mjbj3L3Q3Qvz8vLqtkBJCBcN7chdFwxi5sqtnH3PJyzfuCvskkTqjVgGwSqgfdRwu2BcdcYDZ8awHklyo/u24bHvH8XGHXs5+56JTF+xJeySROqFWAbBZKCbmXUyswxgLDAhuoGZdYsaPBVYGMN6RBjSqTnPXjGMBhmpjB33GW/MXht2SSKhi1kQuHs5cCXwBjAXeMrdZ5vZjcEVQgBXmtlsM5sGXAtcEqt6RPbr2rIxz11xDD1aZ3P5Y1O494NFurxUkprF2/8AhYWFXlRUFHYZkgBKyyr4+dPTeXnGGr41uB03ndWXjLTQT5uJxISZTXH3wqqm6cE0krSy0lO5Y+xAOuc15o53FrJ0407uuWgwLRpnhl2aSJ3Szx9JaikpxrUnd+eO8wcyc9VWzrjzY2at2hp2WSJ1SkEgApzRvy3PXD4MB8699xNenHagC9xEEouCQCTQJ78pE648lr75Tbl6/DT+8NJsyir2hV2WSMwpCESi5GVn8sQPhvLdYwp4aOJSLrxvEuu3lYZdlkhMKQhEKklPTeGG03tz+9gBzFy1ldF3fMwnxRvCLkskZhQEItUYMyCfF688hpyG6Vz4wCRuf3shFfvi63JrkZpQEIgcQPdW2bz442M4a0A+t729gIsfmMQ6HSqSBKMgEDmIRplp/P28/vz1nH58sXwLI//xIe/MXRd2WSK1RkEgUgNmxnlHtuelnxxLm6YN+N6/ivjdi7PYvbci7NJEDpuCQOQQdG3ZmOd/PIzvHduJRz5dxql3fsSMlerFVOKbgkDkEGWmpfK/p/Xi8e8fxe69FZx99yf84+0FuudA4paCQORrOqZrC16/+nhO7deGf7y9kLPunsj8tdvDLkvkkCkIRA5D04bp3D52IPdcOIg1W0o5/c6Pueu9Yu0dSFxREIjUglF92/DmNcczoldLbnljPmP+OVGd10ncUBCI1JLcxpncfeFg7r1oECU79jDmron8v9fm6soiqfcUBCK1bGSfNrx9zQmcMyif//tgMaf84wPen78+7LJEqqUgEImBpg3T+eu5/Rl/2VDSU1O49KHJXPnEVN2VLPWSgkAkhoZ2zuW1q4/jmhHdeXPOOk76+wfc/9FiynUyWeoRBYFIjGWmpXL1iG68dc3xFBY040+vzOW0Oz/m00Ubwy5NBFAQiNSZjrmNeOjSI7n3osFsLy3n/Ps+40ePT2Hl5l1hlyZJTg+vF6lDZsbIPq0Z3iOPcR8u5u73i3ln7np+cFxnrhjehUaZ+l9S6p72CERCkJWeylUndeOdnw1nZJ/W/PO9Yob/7X2enLxczzyQOqcgEAlRfk4Dbh87kOd+NIz2zRpw3bMzGXX7h7w7bx3uCgSpGzENAjMbaWbzzazYzK6vYvq1ZjbHzGaY2Ttm1jGW9YjUV4M6NOPZK4Zx94WD2Fu+j/95uIix4z5j6vLNYZcmSSBmQWBmqcBdwCigF3C+mfWq1OwLoNDd+wHPAH+NVT0i9Z2ZMbpvG9669gRuHNObRSU7OPvuT/j+v4qYt3Zb2OVJAovlHsEQoNjdF7v7XmA8MCa6gbu/5+77L5n4DGgXw3pE4kJ6agrfObqAD37xDX5+SncmLdnIqNs/4if//oLi9TvCLk8SUCyDIB9YETW8MhhXne8Br1U1wcwuM7MiMysqKSmpxRJF6q9GmWlceWI3PvrlN7j8hC68M3cdp9z2Adc8OY1FJQoEqT314mSxmV0EFAK3VDXd3ce5e6G7F+bl5dVtcSIhy2mYwXUje/LRL7/BD47rzGuz1nDyrR9w1b+/YME6Pf9ADl8sL1peBbSPGm4XjPsKMxsB/AY4wd33xLAekbiW2ziTX40+gh8c35n7PlrMo58uY8L01Xyzdyt+NLwr/dvnhF2ixCmL1SVqZpYGLABOIhIAk4EL3H12VJuBRE4Sj3T3hTVZbmFhoRcVFcWgYpH4snnnXh6cuIR/fbKUbaXlHNu1BZef0IVjuuZiZmGXJ/WMmU1x98Iqp8XyWmUzGw38A0gFHnT3m8zsRqDI3SeY2dtAX2BNMMtydz/jQMtUEIh81fbSMp6YtJz7P15CyfY99G7bhMuO78zovm1IT60XR3+lHggtCGJBQSBStT3lFbzwxSrGfbiYRSU7yc9pwCXDOvLtIzvQtEF62OVJyBQEIklk3z7n3Xnruf/jxXy2eBONMlL5VmF7LhlWQKcWjcIuT0KiIBBJUrNWbeWBj5fw8ozVlFU43+iRxyXDCji+Wx4pKTqPkEwUBCJJbv22Uh6ftJzHJy1nw449FOQ25KKhHfnW4PY0bajDRslAQSAiQOQ8wuuz1vLIp8uYsmwzmWkpnNavLRcc1YFBHXJ0tVECUxCIyH+ZvXorT0xazgtfrGLn3gp6ts7m20e256yB+eQ0zAi7PKllCgIRqdaOPeVMmLaaJycvZ/rKrWSkpfDN3q05d3A7ju3aglSdS0gICgIRqZE5q7fxVNEKXpi2ii27ymjdJIuzB+Vz9qB2dG3ZOOzy5DAoCETkkOwpr+Cduet5umgFHywoYZ9Dv3ZNOWtgPqf1a0tedmbYJcohUhCIyNe2flspE6av5rmpq5izZhupKcYxXVtw5oC2nNyrFdlZuuooHigIRKRWzF+7nRenreLFaatZtWU3mWkpnNizJaf3b8s3erSkQUZq2CVKNRQEIlKr3J0pyzbz8ow1vDxjDRt27KFhRion9mzJaf3acEJ3hQ/vTeQAAA4JSURBVEJ9oyAQkZip2OdMWryRV2au4fVZa9m4cy8N0lMZ3iOPkX1ac2LPljp8VA8oCESkTpRX7GPSkk28NmsNb8xeR8n2PaSnGsO6tOCbvVsz4oiWtGySFXaZSUlBICJ1bt8+Z+ryzbwxey1vzF7H8k2Rx5P3b5/DyUe05KQjWtGzdbbuZq4jCgIRCZW7M3/ddt6es4635q5n+ootAOTnNODEni05sWdLju6SS1a6zivEioJAROqV9dtKeXfeet6Zt56PF25gd1kFmWkpDOuSy/AeLRneI4+OueoyuzYpCESk3iotq2DSkk28N289781fz7KNkUNIHXMbckL3PI7rlsfRXXJpnBnLR6wnPgWBiMSNpRt28uHCEt6fX8Knizayu6yCtBRjUIdmDOuay7FdW9C/fY4ew3mIFAQiEpf2lFcwddkWPlxYwsTiDcxctRV3aJSRypGdmjOsSy5Hd25Br7ZN1DneQRwoCLSvJSL1VmZaKkd3yeXoLrkAbNm1l08WbeSTRRv4dNFG/jy/BIDszDSO7NScozo1Z0in5vTJb6o9hkOgIBCRuJHTMIPRfdswum8bANZtK+WzxRuZtGQTny3eyLvz1gPQID2VQR1zKOzYnCMLmjOwQw6NdI6hWjo0JCIJY/32UiYv2czkpZuYtGQT89Zuwx1SU4wj2mQzuEMzBnVsxqAOzWjXrEFS3cOgcwQikpS2lZbxxfItFC3dxJRlm5m2Ygu79lYAkJedyaAOOQzs0IyB7XPo264pDTMSd68htHMEZjYSuB1IBe5395srTT8e+AfQDxjr7s/Esh4RSS5NstI5oXseJ3TPAyJdYMxbu52pyzczddlmpi7fwhuz1wGRvYZuLRszoH0O/dvn0K9dU7q3yk6Kcw0x2yMws1RgAXAysBKYDJzv7nOi2hQATYCfAxNqEgTaIxCR2rRxxx6mr9zCF8u3MH3lVqav2MLW3WUAZKal0KttE/rmN6VPflP6tG1Kt1aN4zIcwtojGAIUu/vioIjxwBjgyyBw96XBtH0xrENEpFq5jTM5sWcrTuzZCoh0h7Fs4y5mrNrKzJWRcHh2ykoe+XQZABlpKfRolU3vtk3o3bYJvdo2oWfrJnF9MjqWlecDK6KGVwJHfZ0FmdllwGUAHTp0OPzKRESqYWYUtGhEQYtGnNG/LRDpQG/Jxp3MWrWV2au3MXv1Vl6btZbxk1cE80DH5g05ok0kFHq2yaZn62zaN2tIShzc3xAXEebu44BxEDk0FHI5IpJkUlKMLnmN6ZLXmDED8oHInsPqraXMWb2NOau3MW/tNuat3c7rs9ey/4h7w4xUurXKpmerbLq3zqZHq2y6t2pMXnZmvbpiKZZBsApoHzXcLhgnIhL3zIz8nAbk5zTg5F6tvhy/a285C9btYP7abcxds535a7fzzrx1PFn0nwMkTRuk071VY7q2zKZry8Z0a9mYri0b06ZpVigBEcsgmAx0M7NORAJgLHBBDD9PRCR0DTPSGNA+hwHtc74yfsOOPSxYu50F67azYP0OFq7bzmuz1rBlV9mXbRplpNKlZeNg76MRnYO9kI65DWPaRXdM7yMws9FELg9NBR5095vM7EagyN0nmNmRwPNAM6AUWOvuvQ+0TF01JCKJwt3ZuHMvC9ftoLhkB4vW76B4/Q4Wl+xg9dbSL9uZQbtmDfj5KT2+PDR1qEK7j8DdXwVerTTud1HvJxM5ZCQiknTMjBaNM2nROPPL/pT227mnnCUbdrKoZAeLS3ayZMNOWjTOjEkdcXGyWEQk2TTKTIvcu5DfNOafFX93RYiISK1SEIiIJDkFgYhIklMQiIgkOQWBiEiSUxCIiCQ5BYGISJJTEIiIJLm4e1SlmZUAy77m7C2ADbVYTrxIxvVOxnWG5FzvZFxnOPT17ujueVVNiLsgOBxmVlRdXxuJLBnXOxnXGZJzvZNxnaF211uHhkREkpyCQEQkySVbEIwLu4CQJON6J+M6Q3KudzKuM9TieifVOQIREflvybZHICIilSgIRESSXNIEgZmNNLP5ZlZsZteHXU8smFl7M3vPzOaY2WwzuzoY39zM3jKzhcF/m4Vda20zs1Qz+8LMXg6GO5nZpGB7P2lmGWHXWNvMLMfMnjGzeWY218yOTpJtfU3w73uWmf3bzLISbXub2YNmtt7MZkWNq3LbWsQdwbrPMLNBh/p5SREEZpYK3AWMAnoB55tZr3Crioly4Gfu3gsYCvw4WM/rgXfcvRvwTjCcaK4G5kYN/wW4zd27ApuB74VSVWzdDrzu7j2B/kTWP6G3tZnlA1cBhe7eh8jz0MeSeNv7YWBkpXHVbdtRQLfgdRlwz6F+WFIEATAEKHb3xe6+FxgPjAm5plrn7mvcfWrwfjuRL4Z8Iuv6r6DZv4Azw6kwNsysHXAqcH8wbMCJwDNBk0Rc56bA8cADAO6+1923kODbOpAGNDCzNKAhsIYE297u/iGwqdLo6rbtGOARj/gMyDGzNofyeckSBPnAiqjhlcG4hGVmBcBAYBLQyt3XBJPWAq1CKitW/gH8EtgXDOcCW9y9PBhOxO3dCSgBHgoOid1vZo1I8G3t7quAvwHLiQTAVmAKib+9ofpte9jfb8kSBEnFzBoDzwI/dfdt0dM8cr1wwlwzbGanAevdfUrYtdSxNGAQcI+7DwR2UukwUKJta4DguPgYIkHYFmjEfx9CSXi1vW2TJQhWAe2jhtsF4xKOmaUTCYHH3f25YPS6/buKwX/Xh1VfDBwDnGFmS4kc8juRyLHznODQASTm9l4JrHT3ScHwM0SCIZG3NcAIYIm7l7h7GfAckX8Dib69ofpte9jfb8kSBJOBbsGVBRlETi5NCLmmWhccG38AmOvut0ZNmgBcEry/BHixrmuLFXf/lbu3c/cCItv1XXe/EHgPODdollDrDODua4EVZtYjGHUSMIcE3taB5cBQM2sY/Hvfv94Jvb0D1W3bCcB3gquHhgJbow4h1Yy7J8ULGA0sABYBvwm7nhit47FEdhdnANOC12gix8zfARYCbwPNw641Rus/HHg5eN8Z+BwoBp4GMsOuLwbrOwAoCrb3C0CzZNjWwB+AecAs4FEgM9G2N/BvIudAyojs/X2vum0LGJGrIhcBM4lcUXVIn6cuJkREklyyHBoSEZFqKAhERJKcgkBEJMkpCEREkpyCQEQkySkIJCbM7JPgvwVmdkEtL/vXVX1WrJjZmWb2uxgte0eMljt8f0+sh7GMpWbW4gDTx5tZt8P5DKkfFAQSE+4+LHhbABxSEETdIVqdrwRB1GfFyi+Buw93ITVYr5ir5RruIfK3kTinIJCYiPqlezNwnJlNC/qRTzWzW8xsctB3+g+D9sPN7CMzm0DkTlHM7AUzmxL0PX9ZMO5mIj1PTjOzx6M/K7iz8pagn/qZZvbtqGW/H9V3/+PBXamY2c0WeX7DDDP7WxXr0R3Y4+4bguGHzexeMysyswVBX0f7n4dQo/Wq4jNuMrPpZvaZmbWK+pxzo9rsiFpedesyMhg3FTg7at7fm9mjZjYReNTM8szs2aDWyWZ2TNAu18zeDP7e9xO5UQkza2RmrwQ1ztr/dwU+AkbUh4CTwxT2HXR6JeYL2BH8dzjB3b7B8GXAb4P3mUTujO0UtNsJdIpqu//OyQZE7iLNjV52FZ91DvAWkT7qWxHpjqBNsOytRPpgSQE+JXIXdi4wn/88uzunivX4LvD3qOGHgdeD5XQjctdn1qGsV6XlO3B68P6vUct4GDi3mr9nVeuSRaQHym5EvsCf4j93Wf+eSA+dDYLhJ4Bjg/cdiHRJAnAH8Lvg/alBbS2Cv+t9UbU0jXr/FjA47H9veh3eS3sEUtdOIdIvyjQiXWTnEvnyAvjc3ZdEtb3KzKYDnxHpVOtgx6OPBf7t7hXuvg74ADgyatkr3X0fka43Coh8oZYCD5jZ2cCuKpbZhkh3z9Gecvd97r4QWAz0PMT1irYX2H8sf0pQ18FUtS49iXTGttAj39CPVZpngrvvDt6PAP4Z1DoBaGKRHmuP3z+fu79C5AEvEOm24GQz+4uZHefuW6OWu55IL6ASx7RLJ3XNgJ+4+xtfGWk2nMgv5+jhEcDR7r7LzN4n8qv369oT9b4CSHP3cjMbQqTjsnOBK4n0XhptN9C00rjK/bI4NVyvKpQFX9xf1hW8Lyc4dGtmKUD0oxf/a10OsPz9omtIAYa6e2mlWquc0d0XWOTxh6OBP5nZO+5+YzA5i8jfSOKY9ggk1rYD2VHDbwBXWKS7bMysu0UeqFJZU2BzEAI9iTx6c7+y/fNX8hHw7eB4fR6RX7ifV1dY8Cu4qbu/ClxD5HGPlc0FulYa9y0zSzGzLkQ6O5t/COtVU0uBwcH7M4Cq1jfaPKAgqAng/AO0fRP4yf4BMxsQvP2Q4MS+mY0i0okdZtYW2OXujwG3EOnuer/uRA7bSRzTHoHE2gygIjjE8zCRZwUUAFODk5wlVP1YwdeBy81sLpEv2s+ipo0DZpjZVI90Ob3f88DRwHQiv9J/6e5rgyCpSjbwopllEflFf20VbT4E/m5mFvXLfTmRgGkCXO7upcHJ1ZqsV03dF9Q2ncjf4kB7FQQ1XAa8Yma7iIRidjXNrwLuMrMZRL4DPgQuJ9Kr57/NbDbwSbCeAH2BW8xsH5HeMK8ACE5s7/ZIl9gSx9T7qMhBmNntwEvu/raZPUzkJOwzB5kt4ZnZNcA2d38g7Frk8OjQkMjB/ZnIQ9Llq7bwn4epSxzTHoGISJLTHoGISJJTEIiIJDkFgYhIklMQiIgkOQWBiEiS+/957pxte79y5gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM2kUNFAg25W"
      },
      "source": [
        "# summary\n",
        "\n",
        "So even with enough power(GPU) and kinda low loss/cost we actually don't get better accuracy on validation set. \n",
        "\n",
        "That looks like **overfitting**.\n"
      ]
    }
  ]
}