{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seneca.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNbDU/t5sMMt/kC5KY2qvUZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/seneca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSlANv1PhZ3R",
        "colab_type": "text"
      },
      "source": [
        "# Seneca\n",
        "\n",
        "Text downloaded from gutenberg.\n",
        "\n",
        "Model used is GPT-2 and finetuned. After 30 epochs, ~ 5 perplexity.\n",
        "\n",
        "## love (200 length)\n",
        "\n",
        "   Wherewith the bright red thread lashes the lyre, and where at the bath\n",
        "    The frenzied groan falls all the birds, for her name                  435\n",
        "    Is Ulysses; who will at her bidding tremble\n",
        "    For her ashes? I fall first from heaven\n",
        "    And there come to ruin the girl!\n",
        "\t\n",
        "## Love (400 length)\n",
        "Love, to whom it belongs;\n",
        "    Say, What shall I gain by doing this?\n",
        "    A prize which does not please men?                      10\n",
        "    Can I cause an outrage at returning a deposit?\n",
        "    Why must I expend so much treasure?\n",
        "    Whatever happiness I gain,                                 15\n",
        "    Must come at once from these anxious hands.\n",
        "    Retreat boldly, supplicate those who boast of your\n",
        "    Ancien servitude; bestow a greater boon than this,\n",
        "    Make the wrong you did bear an opportunity of proving\n",
        "    To be right; nor may any disgrace ever reach you\n",
        "    Unlearn this most shameful of deeds:                      20\n",
        "    Reveal your intentions openly; you are my debtor;\n",
        "    Wait patiently and show all your deceit;                     25\n",
        "    Though no shame comes to owe any debt. To gain\n",
        "    This very gain, which comes at great cost to me,                    30\n",
        "    Pay back my gratitude by persistent prayer.\n",
        "    So may I repay these anxious hands,\n",
        "    But make a second effort; by so doing,\n",
        "    I have shed the blood which my body deserved.               35!\n",
        "\n",
        "\t\n",
        "## What is the meaning of life?\n",
        "\n",
        "There is nothing beyond the commonest kind of pleasure; but it lies beyond that which we can enjoy without it, which cannot atone for itself.\n",
        "Our life consists in its various parts, which have to do with what we are about to do, with the fortunes which we expect to bestow upon our successors, and\n",
        "with what else we may earn. One part of life does not afford us pleasure; another, although it\n",
        "can assist us in some important, economical or economical duty, does not live\n",
        "by the regular rights and privileges which are the property of the state, of us. We are freed from any\n",
        "grandeur which checks us against our will; we know that fortune has placed us in our\n",
        "need of money, and can raise us by any means whatever to which she will not give\n",
        "money, though no one can raise a man from poverty without taking into consideration the\n",
        "interests which he will borrow against it. Yet even when we are tormented by these\n",
        "unnatural temptations, even after they have been removed from us, yet still\n",
        "feel them.\n",
        "\n",
        "What then? is it that prevents us from gratifying these passions by ceasing to\n",
        "expect them? Nothing. They cannot obtain any happiness by ceasing to be\n",
        "satisfied with their past affairs. It is folly to expect a future which will give and endure\n",
        "in order to obtain anything from it, since this life does not yield out anything to\n",
        "our hopes, and our desires don't change our intentions. These passions\n",
        "are as present in men as in a flower; but these from their beginnings\n",
        "can only arise when and where opportunity exists. Thus what they bestow will not last\n",
        "for long; whereas what they take away will become new; thus, in like manner,\n",
        "what they bestow destroys the old, though it may spur new ones to enter it.\n",
        "What then? is it that!\n",
        "\n",
        "\n",
        "## What does it mean to love?\n",
        "\n",
        "What does it matter, if you turn aside the necessity of procreation to make\n",
        "yourself miserable? you have pushed aside all reason, reason has no power, it cannot be\n",
        "fictitious. It belongs to human nature to make rules for itself; to forbid men to\n",
        "be bad is just as unnatural as to let them pass themselves in the\n",
        "door of their own dwellings.\n",
        "\n",
        "XXIII. Since I deny the power of love, why do you, Chrysippus, maintain that\n",
        "it is the part of nature to define one man for another, and to\n",
        "ideally make him separate from all others? Does this human weakness\n",
        "mean that we ought to cease to cherish and respect one man only\n",
        "according to his own character? What, if this be so, does Chrysippus admit of\n",
        "it? We need not explain to him why we ought to consider what he\n",
        "may have accomplished, and what may have come before him. He will answer,\n",
        "that he who gives is deserving, who gives best; the person who bestows benefits ought\n",
        "not to be the last to receive them, but is essential to them: for what has\n",
        "come before him to be of service to himself? He who thinks of himself to\n",
        "be favoured by his own interest, would make no distinction between his\n",
        "benefit and that of others; what would he deem useful, if the best man\n",
        "without his aid turned against him and retired to preserve his own\n",
        "interest? nay, if he thought that only one man was useful, and that the best man\n",
        "in every case was despised, and his own interests were supreme over all the others,\n",
        "why should we let him continue to prize his own? unless nature give him\n",
        "more pleasure than she gives him, and she gives more to him, and this ought\n",
        "to be so in all societies.\n",
        "\n",
        "  \"Because what is honourable is that which one!\n",
        "\n",
        "\n",
        "## Does a multiverse exists?\n",
        "\n",
        "AN INTRODUCTION. \"You speak of no single being.\" Descartes makes the following claims. I shall quote some of them in order to give a sense of their importance,\n",
        "but first I wish to discuss one thing which he never used to do; namely, to make\n",
        "a general definition of a thing, and in doing so give it a formal\n",
        "name. He first defines a thing by its being divided by a line. The formula I\n",
        "cannot repeat sufficiently far. For my purposes the line\n",
        "remains the same; but, as I have said, the division of the\n",
        "matter follows from this;--this being the base form of all that follows, it becomes\n",
        "indeed that the dividing line between the two parts of the\n",
        "thing is never exactly the same as the dividing line between the parts. The whole thing\n",
        "remains the same, but is split by a line; you will find it by the following\n",
        "drawings, which the reader will see are not the same, but one after the\n",
        "same. [Footnote: Though the dividing line between the parts is never exactly\n",
        "the same as the dividing line between the parts, it still follows\n",
        "that the dividing line separates the parts.] Next, there is a third dividing\n",
        "point, or point, which divides the whole thing: here this one is always the same\n",
        "but divides it: this is our dividing point. A division is in itself no division, but\n",
        "it is the same as an equivalent division.\n",
        "\n",
        "  \"'Tis our doing,\n",
        "  \"Tell me, wretch; if our dividing point be the same, does it follow\n",
        "that it divides us?\" No. For we declare our intention with the words\n",
        "which we are speaking; they follow the rules of our language.\n",
        "\n",
        "  \"'Tis our wont,\n",
        "  'Tis done; we lie down; lie down again;\n",
        "   Let '!\n",
        " \n",
        " \n",
        "## What should i do with life?\n",
        "\n",
        "If my goodness gives you hope, if my hope wins your heart, if my hope leads you to love,\n",
        "if my goodness gives you joy, if my hope wins your heart, and, even better, if, by so doing,\n",
        "by doing it in righteousness I can save you from the greatest dangers of all.\n",
        "\n",
        "XXVI. However you may hope to live a happy and prosperous age, keep well\n",
        "those things which you wish to acquire, and, besides these, repay what you owe with good will,\n",
        "pray with good feeling, have good company, and, most of all, cease your covetousness of what\n",
        "is good. Let us therefore act boldly, by our good faith, our skill in\n",
        "intended policy, and our hope that life may repeat itself to our memory.\n",
        "\n",
        "XXVII. After this, even if fortune don't give you any security, let us\n",
        "do everything in our power to restore it, not to win any, but to give everything\n",
        "which we might win, not even in the meanwhile unforeseen, provided that we\n",
        "do everything in our power to give well to those whom we ought to bestow it\n",
        "in order that they may repay what they have lent. What is a benefit repaid\n",
        "in the first place is life itself: for to live rightly there is\n",
        "nothing wrong in dying under compulsion. I should be permitted to live if\n",
        "life were without beginning; if to live rightly is an end in itself. Yet I\n",
        "would answer that it is in my power to live under compulsion to keep\n",
        "the peace, or to do what is honourable and pleasing; because human life itself is\n",
        "degraded by force and violence, and requires courage and moderation, which may never be\n",
        "rendered good. It is not to us to make war upon a foreign people for\n",
        "security reasons: for even if this don't succeed in giving you the means!\n",
        "\n",
        "\n",
        "## investment, \n",
        "\n",
        "a strong middle class is a necessary condition for prosperity. However, I must remind you that this is not an exact term, though one may use it as it pleases; for in like manner, there are some instances, when the standard of living--the standard of prosperity--is raised not by the social equality of all men, but by the material equality of all men. The shades of a virtuous mind reach beyond themselves, taking the first step towards the prosperity of all, whilst those nearest to it fall short of it, in order that they may move further to the side of the good, while the others who wish for great things retain their shadow.\n",
        "\n",
        "The case in point is, therefore, one of want of good fortune. On the one hand, the hard hand is eager to bestow it, yet only when the resources of the country can be turned into means can it pay for what it has received; on the other hand, the greedy hand takes no time at all to bestow it, but prefers the fruit of its labour, for it does not dare but to withhold it.\n",
        "\n",
        "If you look at the whole subject in its fullest light, you will find that what I have just mentioned is hardly any different from what is generally believed, for wealth is of no use to the ignorant, because it is lost instantly upon disentangling from them what they covet, or to whom they bestow it; but it is everywhere wanting, everywhere wanting in its proper place, as though it were the chief factor in any part of the thing which it touches.\n",
        "I shall give you some examples, but I shall not argue against them; let us proceed from them in their proper order. First, let us define our purpose:\n",
        "--for my object is to make a living.\n",
        "\n",
        "This is a pleasure enjoyed at the time when the light is failing, when life is stretched thin and weary, and the heat is mounting. All that!\n",
        "\n",
        "\n",
        "## What is work?\n",
        "\n",
        "As we saw with\n",
        "that first question from the Stoics, work does not always serve the teacher. The\n",
        "subject becomes inadequate as the stage moves on:\n",
        "what could I learn by a rough education, unless I learned it by\n",
        "learning by chance? \"Benefits,\" they say, \"are useless unless they are\n",
        "used by bad men.\" Well, no, but bad men are bad even though their benefits\n",
        "should be of use to them; for to a bad man benefits best when he makes use\n",
        "of them. If I learned that what a bad man has put in\n",
        "my way of making money by prescription, then that man would be good\n",
        "not because he works, but because he has been trained to think of what\n",
        "he has done, and to think of what he intends to do with it.\n",
        "The less you use benefits, the better you become; for you do\n",
        "not become rich by their use; they are valuable merely by their\n",
        "use, and use is not all that distinguishes a good man from a bad one.\n",
        "\n",
        "XXXVII. Aristotle once said, \"There is no one who can obtain what he wants\n",
        "without labour. For him who can, it follows, procure what he wants.\" I say\n",
        "not that labour is not all that distinguishes a good man from a bad one; but\n",
        "it is all that distinguishes a bad man from a good man. For you, my friend,\n",
        "need not need labour to make a living. A better model of good\n",
        "care requires nothing more than the use of labour. Thus we have\n",
        "nothing but benefits to prove a man's poverty, just as he cannot obtain\n",
        "anything without labour. If you want anything from me, you must obtain from me\n",
        "nothing; what can I gain? It is possible to put into a house what no\n",
        "one in Britain can put into a box; but there is no force which can take\n",
        "life out of any!\n",
        "\n",
        "\n",
        "## Should one prefer family or work?\n",
        "\n",
        "ECONSE OF\n",
        "\n",
        "PREFECTING SCHOOLS\n",
        "\n",
        "Although all thought has been paid to the school of Liberal thought, and nothing has been proved, it will be argued that, although it is untrue, it shows a great deal of agreement among the eminent Liberalists and others in the liberal arts that this system of education ought to be reformed in order that it may afford to all men an adequate education at the expense of the few who are obliged to pay a subsidy for it. As we have seen, the man who bestows upon us the most excellent educations in the world is the man who receives most from us; but there is an inherent difference between those who receive for education the very best which they can obtain for themselves, and those who receive for it the condition in which they can bestow them: the latter man bestows those which he gives for free, and the former man gives them without any concern for their welfare. Every school therefore must be reformed, and each must do its duty under a different name. It is my belief that a strictly federated school ought to be established in England, in order that those schools which receive their fee from the state may receive a return for it in suitable forms. As soon as there is a feeling aroused in the country that the higher education which they receive is not deserving of the public subsidy, they will cut themselves off from that which they receive; at the same time, they will raise the hopes of the state, and the resources which they will use in order to maintain an equality with the mass of mankind; and, worst of all, they will begin to regard the advantages which they confer upon those whom they serve as greater than they themselves admit to be. Let these persons thus be regarded as exempt from the influence of the state, and as capable of making any repayment to the state at the expense of those who receive them: let them therefore accept a public benefit from!\n",
        "\n",
        "## How to build attention?\n",
        "\n",
        "If you wish to be tried, you ought to receive it in good faith. \"You are blind,\" say you, \"who think that if you can learn, you can turn yourself into a god; for although a god may not be discovered, but makes it intelligible, yet that which is discovered must nevertheless be found, and must necessarily be admitted into the highest possible order.\" If I am ungrateful for a receipt, I am also ungrateful for\n",
        "anything my creditor receives; but if he gives it, I am also indebted to him, not to the\n",
        "giver, but to me; therefore he cannot repay my debt to me. I am blind in myself, unless I use\n",
        "an unjust and cowardly trick to get my creditors to think I am under no obligation. Even\n",
        "in matters of faith, when we have built up a great bond of trust, we must also make a good\n",
        "faith effort to investigate its strength and weakness. \"The bond of trust,\" you say, \"must\n",
        "be strong enough to make an ungrateful creditor repay it.\" How can a man be ungrateful\n",
        "for anything he receives from his creditor? You need not therefore be a bad man; nor need you\n",
        "need you fear that a bad man will do you wrong, because you can tell when a bad man will\n",
        "do you wrong by what he intends to do.\n",
        "\n",
        "XXIV. I shall now return to those matters in which you will most certainly be\n",
        "incensed. It is common custom to dispute who has the right to receive what\n",
        "he has not. It has been the custom of kings and princes alike to divide presents between\n",
        "their subjects, and only afterwards separate them. It has sometimes been a\n",
        "mistake for kings to divide a large sum between their subjects, and to leave them\n",
        "individually bound to what their subjects have agreed upon. Who can deny that\n",
        "a thing can belong to!\n",
        "\n",
        "## What is good?\n",
        "\n",
        "Whose virtue is it? who can deny that we receive it? Who can never give it away?\n",
        "Whithersoever thou seest it to be returned, unto whomsoever thou wilt send it, whence\n",
        "it may be found. So when thou wilt raise up the children of men to heaven and stand\n",
        "indeed before their parents and behold the offspring of all kings,\n",
        "when they shall gaze upon the celestial host, so many preserver and constellations\n",
        "shall arise in heaven and earth, and, seeing all things united, will\n",
        "compare and choose the one to the other, choosing each one after him as his own.\n",
        "\n",
        "  \"Who,\" asks he, \"who has not learned to anticipate the lot\n",
        "of kings?\"\n",
        "\n",
        "He who now wishes to comprehend the plan of his kingdom, must first learn to anticipate\n",
        "the glory of princes, and that of kings. There will be naught worse than to suffer these\n",
        "to be borne to their deaths, and that it should be done by force.\n",
        "  But who, wishing his own inheritance to be overstuffed with all\n",
        "which he inherits, assumes the reins over all of his estate, in order that by his\n",
        "influence no one may need to give anything to his own father?                         \n",
        "Those who govern all mankind seem to wish for no return for\n",
        "what they have received, and prove what they have not.\n",
        "\n",
        "How many princes have been in command of the seas?\n",
        "What have they done for thee, whose wrath has ceased to bear arms?\n",
        "What nation has given thee such supreme dominion? What has defiled thee\n",
        "and hast defiled thy self? How many have been able to follow in\n",
        "their footsteps the legions which they commanded? How many have remained faithful to\n",
        "their command even after death!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMVTaWXijAm",
        "colab_type": "code",
        "outputId": "f39d99d8-4bb7-4db5-b1a4-1f36e2d297e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers\n",
        "!pip install -r transformers/examples/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 19737, done.\u001b[K\n",
            "remote: Total 19737 (delta 0), reused 0 (delta 0), pack-reused 19737\n",
            "Receiving objects: 100% (19737/19737), 11.60 MiB | 6.86 MiB/s, done.\n",
            "Resolving deltas: 100% (14447/14447), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/58/3d789b98923da6485f376be1e04d59ad7003a63bdb2b04b5eea7e02857e5/transformers-2.5.0-py3-none-any.whl (481kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting tokenizers==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/1d/ea7e2c628942e686595736f73678348272120d026b7acd54fe43e5211bb1/tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=326b38cb524c692b55f1f0d7aef06470beca75a1e0cabb5434f994ecdf2c6b07\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.0 transformers-2.5.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 3)) (0.22.1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (45.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r transformers/examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=124b6b26051f47c5e94fdcadd66bf49f7df905717dc6b9abb0fdd974fdb0caa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorboardX, seqeval\n",
            "Successfully installed seqeval-0.0.12 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac8MN2ZliuLa",
        "colab_type": "code",
        "outputId": "b5d594dc-f941-41a4-e2cd-eb04f8832194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtfDk87Vix7m",
        "colab_type": "code",
        "outputId": "77a0bf04-93da-41ee-8f1a-fb3e49ce9bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python transformers/examples/run_language_modeling.py --output_dir='/content/drive/My Drive/seneca/output' --model_type=gpt2 --model_name_or_path='/content/drive/My Drive/seneca/output' --do_train --train_data_file='/content/drive/My Drive/seneca/seneca.txt' --do_eval --eval_data_file='/content/drive/My Drive/seneca/seneca.txt' --no_cuda --num_train_epochs 10 --overwrite_output_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/19/2020 19:20:07 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "02/19/2020 19:20:07 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 19:20:07 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/19/2020 19:20:07 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/19/2020 19:20:07 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 19:20:11 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cpu'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/drive/My Drive/seneca/seneca.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='/content/drive/My Drive/seneca/output', model_type='gpt2', n_gpu=0, no_cuda=True, num_train_epochs=10.0, output_dir='/content/drive/My Drive/seneca/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/drive/My Drive/seneca/seneca.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/19/2020 19:20:11 - INFO - __main__ -   Loading features from cached file /content/drive/My Drive/seneca/gpt2_cached_lm_1024_seneca.txt\n",
            "02/19/2020 19:20:11 - INFO - __main__ -   ***** Running training *****\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Num examples = 169\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Num Epochs = 10\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Total optimization steps = 430\n",
            "02/19/2020 19:20:11 - INFO - __main__ -     Starting fine-tuning.\n",
            "Epoch:   0% 0/10 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:45<32:11, 45.99s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:29<30:55, 45.25s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:11<29:34, 44.36s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:54<28:31, 43.88s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:37<27:31, 43.45s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:19<26:38, 43.21s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [05:01<25:45, 42.92s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:44<24:55, 42.73s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:26<24:09, 42.63s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:08<23:21, 42.45s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:51<22:41, 42.56s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:33<21:54, 42.41s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:15<21:07, 42.24s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:58<20:29, 42.40s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:39<19:41, 42.19s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:21<18:56, 42.08s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:03<18:15, 42.14s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:46<17:33, 42.14s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:28<16:50, 42.10s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:10<16:11, 42.22s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:52<15:26, 42.10s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:34<14:42, 42.02s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:16<14:01, 42.09s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:58<13:19, 42.07s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:40<12:38, 42.15s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:23<11:57, 42.19s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:04<11:11, 42.00s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:47<10:31, 42.12s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:29<09:50, 42.18s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:11<09:06, 42.04s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:53<08:26, 42.21s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:35<07:43, 42.12s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:17<07:01, 42.17s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [24:00<06:20, 42.27s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:42<05:37, 42.18s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:24<04:54, 42.08s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:06<04:13, 42.19s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:48<03:30, 42.15s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:30<02:48, 42.06s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:13<02:06, 42.18s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:54<01:24, 42.12s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:36<00:42, 42.02s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:49<00:00, 33.13s/it]\u001b[A\n",
            "Epoch:  10% 1/10 [29:49<4:28:22, 1789.15s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:27, 42.09s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:23<28:39, 41.95s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:02, 42.06s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:47<27:15, 41.95s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:29<26:35, 41.99s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:12<26:00, 42.16s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:54<25:13, 42.03s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:36<24:29, 42.00s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:18<23:49, 42.05s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:00<23:06, 42.01s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:41<22:22, 41.96s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:24<21:43, 42.06s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:06<21:01, 42.06s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:48<20:18, 42.01s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:30<19:39, 42.13s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:12<18:54, 42.02s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:54<18:14, 42.09s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:36<17:32, 42.08s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:18<16:47, 41.98s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:00<16:08, 42.13s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:42<15:24, 42.02s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:24<14:40, 41.92s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:06<14:01, 42.07s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:48<13:17, 41.99s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:30<12:35, 41.96s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:12<11:54, 42.02s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:54<11:11, 41.97s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:36<10:29, 41.97s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:18<09:49, 42.10s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:00<09:06, 42.00s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:42<08:23, 41.99s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:24<07:42, 42.08s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:07<07:01, 42.12s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:49<06:19, 42.12s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:31<05:37, 42.21s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:13<04:54, 42.09s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:55<04:12, 42.07s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:37<03:30, 42.05s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:19<02:47, 41.96s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:01<02:06, 42.05s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:43<01:23, 42.00s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:25<00:41, 41.99s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:37<00:00, 33.06s/it]\u001b[A\n",
            "Epoch:  20% 2/10 [59:26<3:58:05, 1785.68s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:46, 42.55s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:51, 42.24s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:05<28:03, 42.09s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:25, 42.19s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:38, 42.07s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:54, 42.02s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:54<25:15, 42.10s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:36<24:34, 42.12s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:18<23:51, 42.11s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:00<23:07, 42.04s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:41<22:20, 41.89s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:24<21:41, 41.97s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:05<20:56, 41.90s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:47<20:11, 41.79s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:29<19:36, 42.03s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:11<18:51, 41.89s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:53<18:08, 41.86s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:35<17:29, 41.99s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:17<16:46, 41.95s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [13:59<16:04, 41.94s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:41<15:24, 42.01s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:23<14:41, 41.97s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:05<13:58, 41.93s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:47<13:17, 41.96s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:28<12:33, 41.86s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:10<11:52, 41.92s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:53<11:12, 42.02s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:34<10:29, 41.94s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:17<09:49, 42.08s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:59<09:07, 42.09s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:41<08:23, 41.96s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:23<07:42, 42.04s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:05<07:00, 42.02s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:46<06:16, 41.87s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:29<05:36, 42.12s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:11<04:54, 42.06s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:53<04:12, 42.04s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:35<03:30, 42.14s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:17<02:48, 42.05s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [27:59<02:06, 42.13s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:42<01:24, 42.25s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:24<00:42, 42.16s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:36<00:00, 33.23s/it]\u001b[A\n",
            "Epoch:  30% 3/10 [1:29:03<3:28:01, 1783.03s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:30, 42.17s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:48, 42.16s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:01, 42.04s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:22, 42.12s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:37, 42.04s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:12<25:54, 42.00s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:54<25:15, 42.09s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:36<24:30, 42.01s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:18<23:50, 42.07s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:00<23:09, 42.09s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:42<22:22, 41.96s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:24<21:39, 41.93s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:06<21:00, 42.03s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:48<20:15, 41.92s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:29<19:31, 41.85s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:11<18:52, 41.94s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:53<18:11, 41.98s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:35<17:28, 41.92s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:17<16:46, 41.95s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [13:59<16:02, 41.85s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:41<15:22, 41.92s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:23<14:40, 41.92s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:05<13:57, 41.85s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:47<13:17, 41.98s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:29<12:35, 41.95s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:10<11:51, 41.83s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:53<11:12, 42.01s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:34<10:28, 41.88s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:16<09:45, 41.84s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:58<09:04, 41.88s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:40<08:21, 41.82s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:22<07:40, 41.88s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:04<06:59, 41.98s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:46<06:17, 41.92s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:27<05:34, 41.86s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:09<04:53, 41.91s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:51<04:11, 41.86s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:33<03:29, 41.86s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:15<02:47, 41.94s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [27:57<02:05, 41.93s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:39<01:23, 41.98s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:21<00:42, 42.03s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:34<00:00, 33.08s/it]\u001b[A\n",
            "Epoch:  40% 4/10 [1:58:37<2:58:02, 1780.34s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:32, 42.21s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:52, 42.26s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:05, 42.13s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:23, 42.13s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:43, 42.20s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:12<25:59, 42.16s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:16, 42.13s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:37<24:36, 42.19s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:19<23:51, 42.11s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:01<23:08, 42.07s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:43<22:25, 42.04s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:25<21:41, 41.99s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:07<21:00, 42.01s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:49<20:18, 42.03s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:30<19:33, 41.91s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:13<18:54, 42.01s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:54<18:09, 41.92s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:36<17:25, 41.82s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:18<16:47, 41.98s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:00<16:03, 41.89s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:42<15:21, 41.90s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:24<14:41, 41.99s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:06<13:57, 41.89s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:48<13:15, 41.88s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:30<12:34, 41.94s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:11<11:51, 41.88s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:54<11:11, 41.95s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:36<10:30, 42.03s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:18<09:47, 41.99s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:59<09:05, 41.94s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:42<08:24, 42.04s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:24<07:43, 42.10s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:06<07:01, 42.19s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:49<06:19, 42.19s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:30<05:35, 41.99s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:12<04:54, 42.09s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:54<04:12, 42.03s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:36<03:29, 41.97s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:19<02:48, 42.12s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:00<02:05, 41.99s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:42<01:23, 41.97s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:24<00:42, 42.07s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:37<00:00, 33.14s/it]\u001b[A\n",
            "Epoch:  50% 5/10 [2:28:14<2:28:17, 1779.43s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:31, 42.18s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:46, 42.11s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:02, 42.05s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:47<27:15, 41.94s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:29<26:34, 41.96s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:55, 42.03s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:53<25:07, 41.86s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:35<24:28, 41.95s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:17<23:45, 41.92s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [06:59<23:03, 41.93s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:41<22:28, 42.13s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:23<21:41, 41.98s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:05<20:57, 41.93s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:47<20:19, 42.04s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:29<19:34, 41.96s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:11<18:50, 41.85s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:53<18:11, 41.98s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:34<17:26, 41.84s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:16<16:43, 41.81s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [13:58<16:02, 41.86s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:40<15:21, 41.87s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:22<14:38, 41.83s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:04<13:57, 41.90s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:46<13:15, 41.85s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:27<12:33, 41.85s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:09<11:51, 41.87s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:51<11:08, 41.78s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:33<10:27, 41.83s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:15<09:47, 41.93s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:57<09:03, 41.84s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:39<08:23, 41.97s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:21<07:41, 41.96s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:03<06:59, 41.91s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:45<06:18, 42.01s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:26<05:35, 41.89s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:08<04:53, 41.88s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:51<04:12, 42.02s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:32<03:29, 41.92s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:14<02:47, 41.87s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [27:56<02:05, 41.91s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:38<01:23, 41.88s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:19<00:41, 41.79s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:32<00:00, 32.95s/it]\u001b[A\n",
            "Epoch:  60% 6/10 [2:57:47<1:58:29, 1777.30s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:40, 42.40s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:48, 42.17s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:08, 42.22s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:20, 42.07s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:29<26:34, 41.95s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:54, 42.00s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:53<25:08, 41.90s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:35<24:26, 41.89s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:17<23:47, 41.99s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [06:59<23:03, 41.91s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:42<22:32, 42.27s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:24<21:49, 42.23s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:06<21:03, 42.11s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:48<20:18, 42.03s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:30<19:36, 42.01s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:12<18:53, 42.00s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:54<18:10, 41.96s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:36<17:29, 41.98s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:17<16:43, 41.82s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [13:59<16:02, 41.84s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:41<15:20, 41.85s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:22<14:37, 41.79s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:05<13:59, 42.00s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:47<13:16, 41.91s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:28<12:33, 41.87s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:11<11:53, 41.96s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:52<11:09, 41.86s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:34<10:26, 41.78s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:16<09:46, 41.89s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:58<09:03, 41.84s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:40<08:23, 41.93s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:22<07:42, 42.09s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:04<07:00, 42.01s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:46<06:17, 42.00s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:28<05:36, 42.05s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:10<04:53, 41.97s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:52<04:11, 41.99s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:34<03:30, 42.07s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:16<02:47, 41.96s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [27:58<02:05, 41.98s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:40<01:23, 41.96s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:22<00:41, 41.86s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:34<00:00, 32.95s/it]\u001b[A\n",
            "Epoch:  70% 7/10 [3:27:21<1:28:49, 1776.39s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:38, 42.34s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:23<28:46, 42.11s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:05<28:02, 42.06s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:24, 42.16s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:29<26:37, 42.03s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:52, 41.96s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:53<25:11, 41.98s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:35<24:26, 41.91s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:17<23:45, 41.94s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [06:59<23:05, 42.00s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:41<22:24, 42.01s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:23<21:43, 42.04s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:05<21:00, 42.02s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:47<20:16, 41.95s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:29<19:36, 42.03s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:11<18:53, 41.97s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:53<18:11, 41.97s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:36<17:34, 42.17s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:18<16:49, 42.06s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [13:59<16:06, 42.01s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:42<15:24, 42.04s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:23<14:41, 41.97s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:05<13:58, 41.94s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:48<13:20, 42.13s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:30<12:36, 42.04s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:12<11:54, 42.00s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:54<11:12, 42.00s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:35<10:29, 41.94s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:17<09:46, 41.90s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [20:59<09:05, 41.94s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:41<08:22, 41.83s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:23<07:40, 41.89s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:05<07:00, 42.00s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:47<06:16, 41.89s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:29<05:35, 41.99s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:11<04:53, 41.91s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:52<04:11, 41.85s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:35<03:30, 42.06s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:17<02:47, 41.97s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [27:59<02:05, 41.97s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:41<01:24, 42.06s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:23<00:41, 41.95s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:35<00:00, 33.03s/it]\u001b[A\n",
            "Epoch:  80% 8/10 [3:56:56<59:12, 1776.07s/it]  \n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:31, 42.18s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:45, 42.10s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:05<27:58, 41.96s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:47<27:19, 42.04s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:29<26:36, 42.02s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:52, 41.96s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:54<25:13, 42.05s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:35<24:30, 42.01s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:17<23:47, 41.98s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:00<23:10, 42.13s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:42<22:24, 42.00s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:23<21:40, 41.96s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:06<21:04, 42.15s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:48<20:19, 42.07s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:30<19:35, 41.97s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:12<18:55, 42.05s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:54<18:12, 42.02s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:36<17:29, 41.98s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:18<16:51, 42.13s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:00<16:09, 42.15s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:42<15:26, 42.12s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:24<14:43, 42.06s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:06<13:59, 41.95s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:48<13:19, 42.05s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:30<12:36, 42.04s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:12<11:53, 41.95s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:54<11:13, 42.09s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:36<10:31, 42.07s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:18<09:47, 41.98s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:01<09:07, 42.12s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:42<08:24, 42.01s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:24<07:41, 41.96s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:07<07:00, 42.09s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:49<06:18, 42.07s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:31<05:36, 42.06s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:13<04:54, 42.12s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:55<04:12, 42.05s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:37<03:29, 41.93s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:19<02:47, 42.00s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:01<02:06, 42.02s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:43<01:24, 42.06s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:25<00:42, 42.10s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:37<00:00, 33.17s/it]\u001b[A\n",
            "Epoch:  90% 9/10 [4:26:34<29:36, 1776.63s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:28, 42.12s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:47, 42.14s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:00, 42.01s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:47<27:16, 41.97s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:38, 42.06s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:11<25:53, 41.98s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:53<25:09, 41.93s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:36<24:32, 42.06s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:17<23:47, 41.98s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [06:59<23:03, 41.92s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:41<22:23, 41.97s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:23<21:40, 41.94s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:05<20:58, 41.96s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:47<20:18, 42.01s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:29<19:37, 42.04s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:13<19:07, 42.51s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:55<18:21, 42.36s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:37<17:35, 42.22s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:19<16:54, 42.25s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:01<16:08, 42.10s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:43<15:25, 42.06s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:26<14:46, 42.22s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:08<14:03, 42.15s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:49<13:19, 42.06s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:32<12:38, 42.12s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:14<11:55, 42.09s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:56<11:13, 42.10s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:38<10:32, 42.15s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:20<09:48, 42.01s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:02<09:07, 42.11s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:44<08:25, 42.14s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:26<07:42, 42.03s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:08<07:00, 42.02s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:50<06:18, 42.05s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:32<05:36, 42.01s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:14<04:54, 42.08s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:57<04:12, 42.16s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:39<03:30, 42.05s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:21<02:48, 42.22s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:03<02:06, 42.09s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:45<01:24, 42.10s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:28<00:42, 42.21s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:40<00:00, 33.26s/it]\u001b[A\n",
            "Epoch: 100% 10/10 [4:56:15<00:00, 1777.76s/it]\n",
            "02/20/2020 00:16:27 - INFO - __main__ -    global_step = 430, average loss = 2.1406112332676734\n",
            "02/20/2020 00:16:27 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/seneca/output\n",
            "02/20/2020 00:16:27 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/seneca/output/config.json\n",
            "02/20/2020 00:16:28 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/20/2020 00:16:29 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/20/2020 00:16:29 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/20/2020 00:16:29 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/20/2020 00:16:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/20/2020 00:16:33 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/My Drive/seneca/output']\n",
            "02/20/2020 00:16:33 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/20/2020 00:16:33 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/20/2020 00:16:33 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/20/2020 00:16:38 - INFO - __main__ -   Loading features from cached file /content/drive/My Drive/seneca/gpt2_cached_lm_1024_seneca.txt\n",
            "02/20/2020 00:16:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/20/2020 00:16:38 - INFO - __main__ -     Num examples = 169\n",
            "02/20/2020 00:16:38 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 43/43 [09:28<00:00, 10.44s/it]\n",
            "02/20/2020 00:26:06 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/20/2020 00:26:06 - INFO - __main__ -     perplexity = tensor(5.9563)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxO4jlXojC_K",
        "colab_type": "code",
        "outputId": "4148c95c-d9cc-41a4-cde1-cc931b1db544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python transformers/examples/run_generation.py --model_type=gpt2 --model_name_or_path='/content/drive/My Drive/seneca/output' --no_cuda --length 400"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/20/2020 08:02:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/20/2020 08:02:15 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/20/2020 08:02:15 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/20/2020 08:02:15 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/20/2020 08:02:19 - INFO - __main__ -   Namespace(device=device(type='cpu'), k=0, length=400, model_name_or_path='/content/drive/My Drive/seneca/output', model_type='gpt2', n_gpu=0, no_cuda=True, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> What is good?\n",
            "What is good?\n",
            "\n",
            "Whose virtue is it? who can deny that we receive it? Who can never give it away?\n",
            "Whithersoever thou seest it to be returned, unto whomsoever thou wilt send it, whence\n",
            "it may be found. So when thou wilt raise up the children of men to heaven and stand\n",
            "indeed before their parents and behold the offspring of all kings,\n",
            "when they shall gaze upon the celestial host, so many preserver and constellations\n",
            "shall arise in heaven and earth, and, seeing all things united, will\n",
            "compare and choose the one to the other, choosing each one after him as his own.\n",
            "\n",
            "  \"Who,\" asks he, \"who has not learned to anticipate the lot\n",
            "of kings?\"\n",
            "\n",
            "He who now wishes to comprehend the plan of his kingdom, must first learn to anticipate\n",
            "the glory of princes, and that of kings. There will be naught worse than to suffer these\n",
            "to be borne to their deaths, and that it should be done by force.\n",
            "  But who, wishing his own inheritance to be overstuffed with all\n",
            "which he inherits, assumes the reins over all of his estate, in order that by his\n",
            "influence no one may need to give anything to his own father?                         \n",
            "Those who govern all mankind seem to wish for no return for\n",
            "what they have received, and prove what they have not.\n",
            "\n",
            "How many princes have been in command of the seas?\n",
            "What have they done for thee, whose wrath has ceased to bear arms?\n",
            "What nation has given thee such supreme dominion? What has defiled thee\n",
            "and hast defiled thy self? How many have been able to follow in\n",
            "their footsteps the legions which they commanded? How many have remained faithful to\n",
            "their command even after death!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckogEp3vucv7",
        "colab_type": "text"
      },
      "source": [
        "# Wih perplexity 13 after 10 epochs\n",
        "\n",
        "## Love\n",
        "<pre>\n",
        "Love,    What blood, sooth, burns                1554\n",
        "     By the flame of good fortune,\n",
        "    Shall a grateful owner fall,\n",
        "    The vanquished foe's fate;\n",
        "    Thy brothers yoke, thy wives, thy daughters,                       1555\n",
        "    That wild youth of the Rhine who dared\n",
        "     As he might, give a victory to kings and princes?\n",
        "    The brave Caligula\n",
        "    Her hosts with their armored host                               1560\n",
        "    Gives courage to the Rhine's coast;\n",
        "</pre>\n",
        "\n",
        "## Whats the meaning of life?\n",
        "\n",
        "<pre>\n",
        "If not entirely silent, there is any interval in time when it becomes silent.\n",
        "And I shall return to the subject matter first, of dealing with the spirit,\n",
        "with which we are to deal in the Stoic philosophy. A word will bestow its\n",
        "benefit upon you: that is, not upon Stoics, for you must remember\n",
        "that philosophy is both really and unjustly led by common principles\n",
        "which are disgraceful, and which are wicked: that, consequently, Socrates becomes a\n",
        "prophet. A man who takes no pains to bear the truth, a man who\n",
        "who is forced to confess no secrets, falls into those channels of vice\n",
        "which afterwards form it, slips into your own being, and hides\n",
        "away; a man who is hard to believe is saved by prayers,\n",
        "who becomes famous, even when he has promised to come to me with\n",
        "discover it.\n",
        "\n",
        "\n",
        "XXXII. It would not!\n",
        "</pre>\n",
        "\n",
        "# perplexity 8.8301, 20 epochs\n",
        "\n",
        "## Love\n",
        "<pre>\n",
        "Love, the son of Prometheus;\n",
        "    Thou dost eat the forbidden fruit of the hidden\n",
        "    Inachus fixt his feet; since I put                      490\n",
        "    This kingly brass ornament on my head,\n",
        "    Has held my arms and loosened them my shoulders;                            505\n",
        "    I have brought the secrets of his secret fortune                       520\n",
        "    From Agamemnon; my gifts have conferred\n",
        "    Perpetual justice; O Lynceus, love conquers--\n",
        "    My affectionate love--love drives me mad;  !\n",
        "</pre>\n",
        "\n",
        "## Whats the meaning of life?\n",
        "\n",
        "<pre>\n",
        "Whose wicked self, so that he keeps in check and obtains security\n",
        "from him, profits us?\n",
        "Whose kindly father, after giving us food, supplies us with clothing,\n",
        "has fixed his precepts upon us? whither then is this so dreadful that\n",
        "we don't take notice of him? He is in such a position that if he says,\n",
        "\n",
        "\"I love my wife,\" there is nothing to keep us from doing so. \"I pray,\" replies he,\n",
        "\"that you may be satisfied with what you have received, I will not\n",
        "use that which I have received.\" True enough; yet if we allow that we have\n",
        "received it, we forfeit all gratitude.\n",
        "\n",
        "XVII. When I said that we were under no obligation to God, why do I say that we\n",
        "should not have received from him what we had hoped to receive? For a\n",
        "person does not need to receive anything!\n",
        "</pre>\n",
        "\n",
        "# perplexity 5.9563, 30 epochs\n",
        "\n",
        "## Love\n",
        "<pre>\n",
        "Love, to whom it belongs;\n",
        "    Say, What shall I gain by doing this?\n",
        "    A prize which does not please men?                      10\n",
        "    Can I cause an outrage at returning a deposit?\n",
        "    Why must I expend so much treasure?\n",
        "    Whatever happiness I gain,                                 15\n",
        "    Must come at once from these anxious hands.\n",
        "    Retreat boldly, supplicate those who boast of your\n",
        "    Ancien servitude; bestow a greater boon than this,\n",
        "    Make the wrong you did bear an opportunity of proving\n",
        "    To be right; nor may any disgrace ever reach you!\n",
        "</pre>\n",
        "\n",
        "## Whats the meaning of life?\n",
        "\n",
        "Heaven forbid that\n",
        "one should open himself to such demands, since there is no bestowal of benefits,\n",
        "yet each man has some gain that he can claim for himself.\n",
        "\"Granted,\" replies he, \"that you say, there is no profit in\n",
        "making the world a better or fairer place, you still deny the existence of\n",
        "social contract, since any one can bestow benefits upon any one,\n",
        "who can ask for one to be repaid.\" Nay, there is no profit in\n",
        "receiving what one really possesses, no one can put up\n",
        "with a woman's vanity which demands extraordinary fidelity to pay it. \"You\n",
        "still allow,\" says he, \"that the wealthy can retain so great\n",
        "value as to be able to give again and again without any further expenditure\n",
        "of their money. You think that the entire credit of mankind\n",
        "is due to the common good and not to the gods?\"!"
      ]
    }
  ]
}