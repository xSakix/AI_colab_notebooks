{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seneca.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvPlpWb/FE+2aGfgu2QS4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/seneca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMVTaWXijAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82665b08-fefa-44fe-d1cd-50977ae810f5"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers\n",
        "!pip install -r transformers/examples/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 20089, done.\u001b[K\n",
            "remote: Total 20089 (delta 0), reused 0 (delta 0), pack-reused 20089\n",
            "Receiving objects: 100% (20089/20089), 11.63 MiB | 6.38 MiB/s, done.\n",
            "Resolving deltas: 100% (14682/14682), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 6.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 22.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=664772d6ea73feb8028ae68ec7a4154cdff76acac2398b6fd2531147be8b9ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 3)) (0.22.1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (45.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.27.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r transformers/examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (2.8.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=a15185c3218769f20f2893beba17272cf3c197c9057db3939e67847174a0f84a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorboardX, seqeval\n",
            "Successfully installed seqeval-0.0.12 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac8MN2ZliuLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "edb67d9e-c493-4f3a-ded2-0d2b39588946"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtfDk87Vix7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "133be7bb-a24d-430d-9fe3-bfb7a096282c"
      },
      "source": [
        "!python transformers/examples/run_language_modeling.py --output_dir='/content/drive/My Drive/seneca/output' --model_type=gpt2 --model_name_or_path='/content/drive/My Drive/seneca/output' --do_train --train_data_file='/content/drive/My Drive/seneca/seneca.txt' --do_eval --eval_data_file='/content/drive/My Drive/seneca/seneca.txt' --no_cuda --num_train_epochs 10 --overwrite_output_dir"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/19/2020 13:56:35 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "02/19/2020 13:56:35 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 13:56:35 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/19/2020 13:56:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/19/2020 13:56:35 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 13:56:39 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cpu'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/drive/My Drive/seneca/seneca.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='/content/drive/My Drive/seneca/output', model_type='gpt2', n_gpu=0, no_cuda=True, num_train_epochs=10.0, output_dir='/content/drive/My Drive/seneca/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/drive/My Drive/seneca/seneca.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/19/2020 13:56:39 - INFO - __main__ -   Loading features from cached file /content/drive/My Drive/seneca/gpt2_cached_lm_1024_seneca.txt\n",
            "02/19/2020 13:56:39 - INFO - __main__ -   ***** Running training *****\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Num examples = 169\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Num Epochs = 10\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Total optimization steps = 430\n",
            "02/19/2020 13:56:39 - INFO - __main__ -     Starting fine-tuning.\n",
            "Epoch:   0% 0/10 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:48<34:07, 48.74s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:32<32:12, 47.13s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:14<30:24, 45.60s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:56<28:57, 44.55s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:39<27:53, 44.04s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:21<26:53, 43.60s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [05:06<26:22, 43.95s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:49<25:25, 43.60s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:31<24:29, 43.23s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:14<23:38, 42.99s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:56<22:51, 42.86s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:38<22:02, 42.67s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:21<21:19, 42.66s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [10:03<20:32, 42.51s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:45<19:45, 42.35s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:28<19:05, 42.42s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:10<18:19, 42.30s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:52<17:36, 42.25s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:35<16:57, 42.39s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:17<16:14, 42.37s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:59<15:31, 42.36s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:42<14:51, 42.45s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:24<14:07, 42.39s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [17:06<13:25, 42.37s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:49<12:45, 42.54s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:32<12:02, 42.49s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:15<11:21, 42.60s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:58<10:44, 42.96s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:41<10:00, 42.86s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:24<09:17, 42.85s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [22:06<08:31, 42.64s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:48<07:47, 42.46s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:31<07:06, 42.64s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [24:13<06:22, 42.45s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:56<05:39, 42.46s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:38<04:57, 42.48s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:20<04:14, 42.40s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [27:02<03:31, 42.34s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:45<02:49, 42.38s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:27<02:07, 42.34s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [29:10<01:24, 42.36s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:52<00:42, 42.36s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [30:05<00:00, 33.42s/it]\u001b[A\n",
            "Epoch:  10% 1/10 [30:05<4:30:45, 1805.04s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:42, 42.45s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:25<29:04, 42.55s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:07<28:16, 42.40s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:32, 42.38s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:32<26:52, 42.43s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:14<26:06, 42.35s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:56<25:23, 42.31s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:39<24:46, 42.47s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:21<24:01, 42.41s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:04<23:18, 42.39s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:46<22:37, 42.42s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:28<21:53, 42.36s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:11<21:11, 42.40s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:53<20:30, 42.42s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:36<19:47, 42.42s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:18<19:06, 42.47s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:00<18:22, 42.39s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:43<17:37, 42.32s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:25<16:59, 42.49s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:08<16:14, 42.38s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:50<15:31, 42.33s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:33<14:53, 42.52s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:15<14:09, 42.46s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:58<13:27, 42.52s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:40<12:46, 42.57s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:23<12:01, 42.44s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:05<11:18, 42.38s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:47<10:35, 42.40s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:29<09:52, 42.32s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:12<09:11, 42.41s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:54<08:28, 42.37s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:36<07:44, 42.25s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:19<07:03, 42.35s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [24:01<06:20, 42.30s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:43<05:38, 42.26s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:26<04:56, 42.35s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:09<04:16, 42.79s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:52<03:32, 42.55s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:34<02:50, 42.51s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:16<02:07, 42.38s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:58<01:24, 42.34s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:41<00:42, 42.34s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:53<00:00, 33.31s/it]\u001b[A\n",
            "Epoch:  20% 2/10 [59:58<4:00:12, 1801.52s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:34, 42.24s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:54, 42.31s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:08, 42.22s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:21, 42.08s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:44, 42.22s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:13<25:59, 42.15s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:16, 42.12s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:37<24:38, 42.24s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:19<23:57, 42.29s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:02<23:15, 42.27s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:44<22:34, 42.33s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:26<21:50, 42.28s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:09<21:08, 42.28s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:51<20:26, 42.29s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:33<19:45, 42.36s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:16<19:03, 42.34s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:58<18:23, 42.43s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:40<17:37, 42.28s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:23<16:55, 42.32s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:05<16:12, 42.30s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:47<15:28, 42.23s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:30<14:49, 42.35s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:12<14:05, 42.27s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:54<13:22, 42.26s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:37<12:42, 42.36s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:19<11:59, 42.31s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:01<11:16, 42.30s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:44<10:35, 42.37s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:26<09:52, 42.34s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:08<09:10, 42.32s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:51<08:28, 42.38s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:33<07:45, 42.34s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:15<07:03, 42.31s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:57<06:20, 42.27s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:39<05:37, 42.19s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:22<04:56, 42.35s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:04<04:13, 42.32s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:46<03:31, 42.22s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:29<02:49, 42.38s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:16<02:11, 43.85s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:59<01:26, 43.47s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:41<00:43, 43.21s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:54<00:00, 33.91s/it]\u001b[A\n",
            "Epoch:  30% 3/10 [1:29:52<3:29:55, 1799.33s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:44, 42.48s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:59, 42.43s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:14, 42.36s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:31, 42.35s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:52, 42.44s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:14<26:07, 42.37s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:56<25:23, 42.32s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:39<24:49, 42.54s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:21<24:04, 42.50s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:04<23:19, 42.41s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:46<22:40, 42.52s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:29<21:55, 42.43s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:11<21:10, 42.34s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:53<20:28, 42.38s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:35<19:45, 42.33s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:17<19:00, 42.25s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:00<18:19, 42.28s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:42<17:35, 42.22s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:24<16:55, 42.31s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:07<16:13, 42.33s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:49<15:28, 42.22s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:31<14:49, 42.35s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:13<14:05, 42.28s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:56<13:21, 42.21s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:38<12:42, 42.36s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:21<12:00, 42.38s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:03<11:16, 42.30s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:45<10:35, 42.38s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:28<09:53, 42.37s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:10<09:11, 42.43s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:53<08:30, 42.54s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:36<07:48, 42.56s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:18<07:06, 42.65s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [24:01<06:23, 42.60s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:43<05:39, 42.44s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:26<04:57, 42.51s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:08<04:14, 42.46s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:51<03:33, 42.63s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:34<02:50, 42.67s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:16<02:07, 42.59s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:59<01:25, 42.55s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:41<00:42, 42.61s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:54<00:00, 33.52s/it]\u001b[A\n",
            "Epoch:  40% 4/10 [1:59:46<2:59:46, 1797.81s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:49, 42.61s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<29:00, 42.46s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:15, 42.40s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:29, 42.30s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:52, 42.42s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:14<26:09, 42.42s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:56<25:21, 42.28s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:39<24:47, 42.51s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:21<24:01, 42.39s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:03<23:16, 42.33s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:46<22:37, 42.41s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:28<21:54, 42.40s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:10<21:13, 42.45s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:53<20:32, 42.50s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:35<19:47, 42.40s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:18<19:03, 42.37s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:00<18:20, 42.31s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:42<17:35, 42.22s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:24<16:52, 42.19s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:06<16:13, 42.32s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:49<15:29, 42.23s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:31<14:47, 42.28s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:13<14:05, 42.29s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:56<13:26, 42.46s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:39<12:45, 42.51s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:21<12:01, 42.43s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:04<11:19, 42.49s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:47<10:41, 42.79s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:30<09:57, 42.70s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:12<09:14, 42.66s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:55<08:31, 42.65s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:37<07:48, 42.55s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:19<07:04, 42.43s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [24:02<06:21, 42.42s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:44<05:39, 42.39s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:26<04:56, 42.35s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:09<04:14, 42.35s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:50<03:31, 42.21s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:33<02:49, 42.26s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:15<02:06, 42.28s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:57<01:24, 42.20s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:40<00:42, 42.40s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:52<00:00, 33.32s/it]\u001b[A\n",
            "Epoch:  50% 5/10 [2:29:39<2:29:41, 1796.26s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:44, 42.49s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:58, 42.41s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:07<28:15, 42.38s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:29, 42.30s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:45, 42.26s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:13<26:03, 42.25s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:19, 42.20s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:38<24:40, 42.29s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:20<23:57, 42.28s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:03<23:20, 42.45s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:45<22:39, 42.48s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:27<21:53, 42.36s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:10<21:14, 42.50s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:53<20:35, 42.60s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:35<19:49, 42.47s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:17<19:03, 42.36s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:00<18:22, 42.41s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:42<17:39, 42.36s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:24<16:53, 42.24s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:06<16:12, 42.28s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:49<15:29, 42.27s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:31<14:50, 42.41s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:14<14:07, 42.36s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:56<13:24, 42.32s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:38<12:42, 42.36s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:21<12:01, 42.45s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:03<11:16, 42.30s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:46<10:37, 42.48s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:28<09:54, 42.44s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:10<09:10, 42.32s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:53<08:29, 42.42s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:35<07:45, 42.31s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:17<07:03, 42.31s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:59<06:20, 42.29s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:42<05:38, 42.35s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:24<04:56, 42.33s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:07<04:14, 42.44s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:51<03:34, 42.82s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:33<02:50, 42.66s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:15<02:07, 42.52s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:57<01:24, 42.35s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:40<00:42, 42.39s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:52<00:00, 33.36s/it]\u001b[A\n",
            "Epoch:  60% 6/10 [2:59:31<1:59:40, 1795.09s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:41, 42.41s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:56, 42.34s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:07<28:16, 42.41s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:28, 42.27s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:43, 42.19s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:13<26:02, 42.24s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:19, 42.20s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:37<24:36, 42.18s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:20<23:58, 42.31s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:02<23:14, 42.25s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:44<22:33, 42.29s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:26<21:48, 42.22s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:08<21:04, 42.14s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:51<20:25, 42.25s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:33<19:40, 42.16s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:15<18:59, 42.20s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:58<18:19, 42.29s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:39<17:33, 42.15s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:21<16:50, 42.09s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:04<16:10, 42.21s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:46<15:27, 42.14s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:28<14:44, 42.12s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:10<14:04, 42.22s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:53<13:22, 42.23s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:35<12:40, 42.26s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:17<11:58, 42.27s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:59<11:15, 42.21s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:41<10:32, 42.18s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:24<09:50, 42.18s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:06<09:09, 42.30s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:49<08:29, 42.45s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:31<07:46, 42.37s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:13<07:02, 42.28s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:56<06:21, 42.40s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:38<05:38, 42.33s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:20<04:56, 42.29s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:03<04:14, 42.39s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:45<03:31, 42.37s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:27<02:49, 42.32s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:10<02:07, 42.37s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:52<01:24, 42.36s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:34<00:42, 42.31s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:47<00:00, 33.34s/it]\u001b[A\n",
            "Epoch:  70% 7/10 [3:29:19<1:29:38, 1792.76s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:43, 42.45s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:52, 42.26s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:13, 42.34s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:28, 42.26s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:44, 42.21s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:13<26:06, 42.34s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:19, 42.22s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:37<24:36, 42.19s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:20<23:58, 42.30s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:02<23:13, 42.23s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:44<22:33, 42.29s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:27<21:54, 42.41s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:09<21:10, 42.35s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:51<20:26, 42.29s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:34<19:44, 42.31s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:17<19:12, 42.68s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [12:00<18:27, 42.59s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:42<17:43, 42.53s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:24<16:56, 42.35s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:06<16:15, 42.42s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:49<15:31, 42.32s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:31<14:47, 42.27s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:13<14:07, 42.37s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:55<13:22, 42.26s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:37<12:39, 42.20s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:20<11:59, 42.35s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [19:02<11:18, 42.38s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:45<10:34, 42.32s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:27<09:52, 42.35s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:09<09:10, 42.35s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:52<08:28, 42.34s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:34<07:45, 42.36s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:17<07:03, 42.38s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:59<06:21, 42.36s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:41<05:38, 42.32s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:23<04:55, 42.14s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:05<04:13, 42.27s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:47<03:30, 42.14s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:29<02:48, 42.10s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:12<02:06, 42.31s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:54<01:24, 42.20s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:36<00:42, 42.16s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:48<00:00, 33.18s/it]\u001b[A\n",
            "Epoch:  80% 8/10 [3:59:07<59:43, 1791.58s/it]  \n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:57, 42.80s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:25<29:07, 42.63s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:07<28:19, 42.48s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:49<27:32, 42.37s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:31<26:46, 42.28s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:13<26:04, 42.28s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:55<25:17, 42.14s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:37<24:32, 42.08s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:19<23:55, 42.22s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:01<23:11, 42.16s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:44<22:29, 42.17s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:26<21:50, 42.27s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:08<21:08, 42.27s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:50<20:22, 42.14s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:32<19:40, 42.15s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:15<19:00, 42.23s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:57<18:16, 42.19s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:39<17:35, 42.22s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:21<16:50, 42.11s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:04<16:10, 42.20s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:46<15:29, 42.23s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:27<14:43, 42.06s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:10<14:02, 42.12s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:52<13:20, 42.11s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:34<12:37, 42.09s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:16<11:56, 42.17s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:58<11:14, 42.15s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:40<10:31, 42.12s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:23<09:51, 42.23s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:05<09:07, 42.13s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:47<08:25, 42.14s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:29<07:44, 42.26s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:12<07:02, 42.24s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:54<06:20, 42.26s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:36<05:38, 42.31s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:18<04:55, 42.18s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [26:00<04:12, 42.15s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:43<03:30, 42.20s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:24<02:48, 42.07s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:07<02:06, 42.10s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:49<01:24, 42.11s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:31<00:42, 42.14s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:43<00:00, 33.21s/it]\u001b[A\n",
            "Epoch:  90% 9/10 [4:28:51<29:49, 1789.25s/it]\n",
            "Iteration:   0% 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/43 [00:42<29:44, 42.49s/it]\u001b[A\n",
            "Iteration:   5% 2/43 [01:24<28:51, 42.22s/it]\u001b[A\n",
            "Iteration:   7% 3/43 [02:06<28:05, 42.15s/it]\u001b[A\n",
            "Iteration:   9% 4/43 [02:48<27:26, 42.21s/it]\u001b[A\n",
            "Iteration:  12% 5/43 [03:30<26:40, 42.12s/it]\u001b[A\n",
            "Iteration:  14% 6/43 [04:12<25:57, 42.09s/it]\u001b[A\n",
            "Iteration:  16% 7/43 [04:54<25:20, 42.22s/it]\u001b[A\n",
            "Iteration:  19% 8/43 [05:36<24:34, 42.12s/it]\u001b[A\n",
            "Iteration:  21% 9/43 [06:18<23:51, 42.09s/it]\u001b[A\n",
            "Iteration:  23% 10/43 [07:00<23:10, 42.13s/it]\u001b[A\n",
            "Iteration:  26% 11/43 [07:42<22:25, 42.05s/it]\u001b[A\n",
            "Iteration:  28% 12/43 [08:25<21:44, 42.08s/it]\u001b[A\n",
            "Iteration:  30% 13/43 [09:07<21:02, 42.10s/it]\u001b[A\n",
            "Iteration:  33% 14/43 [09:49<20:19, 42.03s/it]\u001b[A\n",
            "Iteration:  35% 15/43 [10:31<19:41, 42.19s/it]\u001b[A\n",
            "Iteration:  37% 16/43 [11:13<18:56, 42.09s/it]\u001b[A\n",
            "Iteration:  40% 17/43 [11:55<18:16, 42.16s/it]\u001b[A\n",
            "Iteration:  42% 18/43 [12:38<17:36, 42.27s/it]\u001b[A\n",
            "Iteration:  44% 19/43 [13:20<16:52, 42.17s/it]\u001b[A\n",
            "Iteration:  47% 20/43 [14:02<16:07, 42.09s/it]\u001b[A\n",
            "Iteration:  49% 21/43 [14:44<15:28, 42.22s/it]\u001b[A\n",
            "Iteration:  51% 22/43 [15:26<14:46, 42.21s/it]\u001b[A\n",
            "Iteration:  53% 23/43 [16:08<14:03, 42.19s/it]\u001b[A\n",
            "Iteration:  56% 24/43 [16:51<13:20, 42.15s/it]\u001b[A\n",
            "Iteration:  58% 25/43 [17:32<12:36, 42.05s/it]\u001b[A\n",
            "Iteration:  60% 26/43 [18:15<11:55, 42.09s/it]\u001b[A\n",
            "Iteration:  63% 27/43 [18:57<11:14, 42.17s/it]\u001b[A\n",
            "Iteration:  65% 28/43 [19:39<10:32, 42.15s/it]\u001b[A\n",
            "Iteration:  67% 29/43 [20:21<09:49, 42.12s/it]\u001b[A\n",
            "Iteration:  70% 30/43 [21:03<09:08, 42.19s/it]\u001b[A\n",
            "Iteration:  72% 31/43 [21:45<08:24, 42.03s/it]\u001b[A\n",
            "Iteration:  74% 32/43 [22:27<07:43, 42.13s/it]\u001b[A\n",
            "Iteration:  77% 33/43 [23:09<07:00, 42.07s/it]\u001b[A\n",
            "Iteration:  79% 34/43 [23:51<06:17, 41.98s/it]\u001b[A\n",
            "Iteration:  81% 35/43 [24:34<05:37, 42.15s/it]\u001b[A\n",
            "Iteration:  84% 36/43 [25:16<04:54, 42.07s/it]\u001b[A\n",
            "Iteration:  86% 37/43 [25:58<04:13, 42.18s/it]\u001b[A\n",
            "Iteration:  88% 38/43 [26:40<03:31, 42.22s/it]\u001b[A\n",
            "Iteration:  91% 39/43 [27:22<02:48, 42.17s/it]\u001b[A\n",
            "Iteration:  93% 40/43 [28:04<02:06, 42.09s/it]\u001b[A\n",
            "Iteration:  95% 41/43 [28:47<01:24, 42.19s/it]\u001b[A\n",
            "Iteration:  98% 42/43 [29:28<00:42, 42.07s/it]\u001b[A\n",
            "Iteration: 100% 43/43 [29:41<00:00, 33.11s/it]\u001b[A\n",
            "Epoch: 100% 10/10 [4:58:32<00:00, 1786.83s/it]\n",
            "02/19/2020 18:55:12 - INFO - __main__ -    global_step = 430, average loss = 2.5235162457754448\n",
            "02/19/2020 18:55:12 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/seneca/output\n",
            "02/19/2020 18:55:12 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 18:55:15 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 18:55:16 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 18:55:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/19/2020 18:55:16 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/19/2020 18:55:20 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/19/2020 18:55:20 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/My Drive/seneca/output']\n",
            "02/19/2020 18:55:20 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 18:55:20 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/19/2020 18:55:20 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 18:55:25 - INFO - __main__ -   Loading features from cached file /content/drive/My Drive/seneca/gpt2_cached_lm_1024_seneca.txt\n",
            "02/19/2020 18:55:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/19/2020 18:55:25 - INFO - __main__ -     Num examples = 169\n",
            "02/19/2020 18:55:25 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 43/43 [09:27<00:00, 10.36s/it]\n",
            "02/19/2020 19:04:52 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/19/2020 19:04:52 - INFO - __main__ -     perplexity = tensor(8.8301)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxO4jlXojC_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "903a2c96-e881-4bbc-de2e-0e60e1257553"
      },
      "source": [
        "!python transformers/examples/run_generation.py --model_type=gpt2 --model_name_or_path='/content/drive/My Drive/seneca/output' --no_cuda --length 200"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/seneca/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/seneca/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/seneca/output/added_tokens.json. We won't load it.\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/vocab.json\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/merges.txt\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/special_tokens_map.json\n",
            "02/19/2020 19:17:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/seneca/output/tokenizer_config.json\n",
            "02/19/2020 19:17:33 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/seneca/output/config.json\n",
            "02/19/2020 19:17:33 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/19/2020 19:17:33 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/seneca/output/pytorch_model.bin\n",
            "02/19/2020 19:17:38 - INFO - __main__ -   Namespace(device=device(type='cpu'), k=0, length=200, model_name_or_path='/content/drive/My Drive/seneca/output', model_type='gpt2', n_gpu=0, no_cuda=True, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> Whats the meaning of life?\n",
            "Whats the meaning of life?\n",
            "\n",
            "Whose wicked self, so that he keeps in check and obtains security\n",
            "from him, profits us?\n",
            "Whose kindly father, after giving us food, supplies us with clothing,\n",
            "has fixed his precepts upon us? whither then is this so dreadful that\n",
            "we don't take notice of him? He is in such a position that if he says,\n",
            "\n",
            "\"I love my wife,\" there is nothing to keep us from doing so. \"I pray,\" replies he,\n",
            "\"that you may be satisfied with what you have received, I will not\n",
            "use that which I have received.\" True enough; yet if we allow that we have\n",
            "received it, we forfeit all gratitude.\n",
            "\n",
            "XVII. When I said that we were under no obligation to God, why do I say that we\n",
            "should not have received from him what we had hoped to receive? For a\n",
            "person does not need to receive anything!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckogEp3vucv7",
        "colab_type": "text"
      },
      "source": [
        "# Wih perplexity 13 after 10 epochs\n",
        "\n",
        "## Love\n",
        "<pre>\n",
        "Love,    What blood, sooth, burns                1554\n",
        "     By the flame of good fortune,\n",
        "    Shall a grateful owner fall,\n",
        "    The vanquished foe's fate;\n",
        "    Thy brothers yoke, thy wives, thy daughters,                       1555\n",
        "    That wild youth of the Rhine who dared\n",
        "     As he might, give a victory to kings and princes?\n",
        "    The brave Caligula\n",
        "    Her hosts with their armored host                               1560\n",
        "    Gives courage to the Rhine's coast;\n",
        "</pre>\n",
        "\n",
        "## Whats the meaning of life?\n",
        "\n",
        "<pre>\n",
        "If not entirely silent, there is any interval in time when it becomes silent.\n",
        "And I shall return to the subject matter first, of dealing with the spirit,\n",
        "with which we are to deal in the Stoic philosophy. A word will bestow its\n",
        "benefit upon you: that is, not upon Stoics, for you must remember\n",
        "that philosophy is both really and unjustly led by common principles\n",
        "which are disgraceful, and which are wicked: that, consequently, Socrates becomes a\n",
        "prophet. A man who takes no pains to bear the truth, a man who\n",
        "who is forced to confess no secrets, falls into those channels of vice\n",
        "which afterwards form it, slips into your own being, and hides\n",
        "away; a man who is hard to believe is saved by prayers,\n",
        "who becomes famous, even when he has promised to come to me with\n",
        "discover it.\n",
        "\n",
        "\n",
        "XXXII. It would not!\n",
        "</pre>\n",
        "\n",
        "# perplexity 8.8301, 20 epochs\n",
        "\n",
        "## Love\n",
        "<pre>\n",
        "Love, the son of Prometheus;\n",
        "    Thou dost eat the forbidden fruit of the hidden\n",
        "    Inachus fixt his feet; since I put                      490\n",
        "    This kingly brass ornament on my head,\n",
        "    Has held my arms and loosened them my shoulders;                            505\n",
        "    I have brought the secrets of his secret fortune                       520\n",
        "    From Agamemnon; my gifts have conferred\n",
        "    Perpetual justice; O Lynceus, love conquers--\n",
        "    My affectionate love--love drives me mad;  !\n",
        "</pre>\n",
        "\n",
        "## Whats the meaning of life?\n",
        "\n",
        "<pre>\n",
        "Whose wicked self, so that he keeps in check and obtains security\n",
        "from him, profits us?\n",
        "Whose kindly father, after giving us food, supplies us with clothing,\n",
        "has fixed his precepts upon us? whither then is this so dreadful that\n",
        "we don't take notice of him? He is in such a position that if he says,\n",
        "\n",
        "\"I love my wife,\" there is nothing to keep us from doing so. \"I pray,\" replies he,\n",
        "\"that you may be satisfied with what you have received, I will not\n",
        "use that which I have received.\" True enough; yet if we allow that we have\n",
        "received it, we forfeit all gratitude.\n",
        "\n",
        "XVII. When I said that we were under no obligation to God, why do I say that we\n",
        "should not have received from him what we had hoped to receive? For a\n",
        "person does not need to receive anything!\n",
        "</pre>"
      ]
    }
  ]
}