{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reformer_pytorch_cuda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDUu9Q9kx3N1+mr3NgJBXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/reformer_pytorch_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6yaVql88P9Z",
        "colab_type": "code",
        "outputId": "df4f574e-1d33-4946-cf18-27dada251a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install reformer_pytorch\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Collecting reformer_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/76/e16c3f0904011223e8c4a853d3b08a300db74c4a90a4a983f1a7d934fd63/reformer_pytorch-0.12.7.tar.gz\n",
            "Collecting revtorch>=0.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/7f/6b2247e5ce4b8969dedfcaec064c59ce0417cddbe638bfa6169ff586eaea/revtorch-0.2.4.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from reformer_pytorch) (1.4.0)\n",
            "Building wheels for collected packages: reformer-pytorch, revtorch\n",
            "  Building wheel for reformer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for reformer-pytorch: filename=reformer_pytorch-0.12.7-cp36-none-any.whl size=8720 sha256=52665f37f9e968fc527845fabe1a43680e12aa72078735ba10c4a3c146628af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/b8/d4/a72dab74c922c6cb6544a50f5853b548071e1cb33eb76fda13\n",
            "  Building wheel for revtorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for revtorch: filename=revtorch-0.2.4-cp36-none-any.whl size=5750 sha256=ace4657a8c417356555591e23f3073000098a35d4026e1dd880f119724b167cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/83/1a/5eb5b1043769c607cbee5e6431a550fac20c90bed623e27e5f\n",
            "Successfully built reformer-pytorch revtorch\n",
            "Installing collected packages: revtorch, reformer-pytorch\n",
            "Successfully installed reformer-pytorch-0.12.7 revtorch-0.2.4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 64.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=4f41a120d3c3138db97b1eb5b2a9c2f13b6dfa351b40b8928b45b1f83c0fd857\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnBQKcY2fPC0",
        "colab_type": "code",
        "outputId": "2aa62e78-7d2a-4cbe-befa-cf903692c433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 12 08:08:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    32W / 250W |  13261MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knw2fpJ6Y93b",
        "colab_type": "code",
        "outputId": "e09006d8-c7e8-4108-bb34-696913e43c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UJJlb0WBqJw",
        "colab_type": "code",
        "outputId": "6bedd937-6d4f-484f-c5ee-01f6d24304b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# load model file and epoch\n",
        "import os\n",
        "import re\n",
        "files = [f for f in os.listdir('/content/drive/My Drive/model_saves') if f.startswith('epoch')]\n",
        "last_model_file = None\n",
        "epochs_run = 0\n",
        "if len(files) > 0:\n",
        "  files.sort(reverse=True)\n",
        "  last_model_file = os.path.join('/content/drive/My Drive/model_saves',files[0])\n",
        "  print(last_model_file)\n",
        "  epochs = re.findall(r'\\d+',files[0])\n",
        "  epochs_run = 0\n",
        "  if len(epochs) == 1:\n",
        "    epochs_run = int(epochs[0])\n",
        "  print('number of epochs run:',epochs_run)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/model_saves/epoch-17700.pt\n",
            "number of epochs run: 17700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4azFtyT98aRo",
        "colab_type": "code",
        "outputId": "8d162676-1fc7-4e63-9811-553eb27a6f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from reformer_pytorch import ReformerLM\n",
        "\n",
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# constants\n",
        "\n",
        "NUM_BATCHES = int(1e5)\n",
        "BATCH_SIZE = 8\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "VALIDATE_EVERY  = 100\n",
        "GENERATE_EVERY  = 500\n",
        "GENERATE_LENGTH = 512\n",
        "SEQ_LEN = 4096\n",
        "\n",
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def get_top_p(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "def sample_next_token(logits, top_p=0.9, temperature = 1.0):\n",
        "    logits = logits[0, -1, :] / temperature\n",
        "    filtered_logits = get_top_p(logits, top_p=top_p)\n",
        "\n",
        "    probs = F.softmax(filtered_logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(token))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "# instantiate model\n",
        "\n",
        "model = ReformerLM(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    max_seq_len = SEQ_LEN,\n",
        "    num_tokens = 256,\n",
        "    heads = 8,\n",
        "    bucket_size = 64,\n",
        "    n_hashes = 8,\n",
        "    ff_chunks = 10,\n",
        "    lsh_dropout = 0.1,\n",
        "    weight_tie = True,\n",
        "    causal = True,\n",
        "    use_full_attn = False # set this to true for comparison with full attention\n",
        ")\n",
        "\n",
        "# model = ReformerLM(\n",
        "#     dim = 512,\n",
        "#     depth = 6,\n",
        "#     max_seq_len = SEQ_LEN,\n",
        "#     num_tokens = 256,\n",
        "#     heads = 8,\n",
        "#     bucket_size = 64,\n",
        "#     n_hashes = 4,\n",
        "#     ff_chunks = 10,\n",
        "#     lsh_dropout = 0.1,\n",
        "#     weight_tie = True,\n",
        "#     causal = True,\n",
        "#     use_full_attn = False # set this to true for comparison with full attention\n",
        "# )\n",
        "\n",
        "if last_model_file is not None:\n",
        "  model.load_state_dict(torch.load(last_model_file ))\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "# prepare enwik8 data\n",
        "\n",
        "with gzip.open('/content/drive/My Drive/model_data/merged.gz') as file:\n",
        "    X = np.array([int(c) for c in file.read()])\n",
        "    si = int(len(X)-len(X)*0.2)\n",
        "    trX, vaX = np.split(X, [si])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        return full_seq[0:-1].cuda(), full_seq[1:].cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
        "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "# optimizer\n",
        "# optimizer.load_state_dict(torch.load('optimizer.pt'))\n",
        "# scheduler.load_state_dict(torch.load('scheduler.pt'))\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE,amsgrad=True)\n",
        "\n",
        "if os.path.exists('/content/drive/My Drive/model_saves/optim.pt'):\n",
        "  optim.load_state_dict(torch.load('/content/drive/My Drive/model_saves/optim.pt'))\n",
        "\n",
        "#scheduler\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=VALIDATE_EVERY, gamma=0.1)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "            optim,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataset) // GRADIENT_ACCUMULATE_EVERY * NUM_BATCHES\n",
        "        )\n",
        "\n",
        "if os.path.exists('/content/drive/My Drive/model_saves/scheduler.pt'):\n",
        "  scheduler.load_state_dict(torch.load('/content/drive/My Drive/model_saves/scheduler.pt'))\n",
        "\n",
        "# training\n",
        "\n",
        "def get_batch_loss(model, data):\n",
        "    x, y = data\n",
        "    pred = model(x)\n",
        "    return F.cross_entropy(pred.transpose(1, 2), y, reduction='mean')\n",
        "\n",
        "for i in tqdm.tqdm(range(epochs_run, NUM_BATCHES), mininterval=10., desc='training'):\n",
        "    model.train()\n",
        "\n",
        "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
        "        loss = get_batch_loss(model, next(train_loader))\n",
        "        loss.backward()\n",
        "\n",
        "    print(f'training loss: {loss.item()}')\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i % VALIDATE_EVERY == 0:\n",
        "        torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/model_saves', 'epoch-{}.pt'.format(i)))\n",
        "        torch.save(optim.state_dict(),'/content/drive/My Drive/model_saves/optim.pt')\n",
        "        torch.save(scheduler.state_dict(),'/content/drive/My Drive/model_saves/scheduler.pt')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss = get_batch_loss(model, next(val_loader))\n",
        "            print(f'validation loss: {loss.item()}')\n",
        "\n",
        "    if i % GENERATE_EVERY == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inp, _ = random.choice(val_dataset)\n",
        "            output_str = ''\n",
        "            prime = decode_tokens(inp)\n",
        "\n",
        "            # print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "            print(prime)\n",
        "            print('*'*100)\n",
        "\n",
        "            for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "                logits = model(inp[None, :])\n",
        "                next_token = sample_next_token(logits)\n",
        "                output_str += decode_token(next_token)\n",
        "                inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "            print(output_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 0/82300 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "39035\n",
            "9758\n",
            "training loss: 1.1895207166671753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 1/82300 [00:16<387:44:07, 16.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation loss: 0.9503108263015747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 2/82300 [00:32<378:18:03, 16.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.060807228088379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 3/82300 [00:48<371:45:24, 16.26s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1756713390350342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 4/82300 [01:03<367:05:06, 16.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0568010807037354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 5/82300 [01:19<363:50:36, 15.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0997706651687622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 6/82300 [01:34<361:30:13, 15.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2273523807525635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 7/82300 [01:50<359:55:57, 15.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.4169782400131226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 8/82300 [02:06<358:46:14, 15.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.7619534730911255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 9/82300 [02:21<358:02:10, 15.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2530237436294556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 10/82300 [02:37<357:28:16, 15.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.981792688369751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 11/82300 [02:52<357:11:26, 15.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2237261533737183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 12/82300 [03:08<356:50:50, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.235406756401062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 13/82300 [03:23<356:38:08, 15.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.9258752465248108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 14/82300 [03:39<356:29:27, 15.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2219524383544922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 15/82300 [03:55<356:26:09, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0885881185531616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 16/82300 [04:10<356:24:25, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0124324560165405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 17/82300 [04:26<356:24:03, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.3582189083099365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 18/82300 [04:41<356:22:01, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1168711185455322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 19/82300 [04:57<356:22:03, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.303199291229248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 20/82300 [05:13<356:14:07, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.8306032419204712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 21/82300 [05:28<356:14:02, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0929125547409058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 22/82300 [05:44<356:11:27, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0613465309143066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 23/82300 [05:59<356:18:08, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2249786853790283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 24/82300 [06:15<356:15:14, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2791032791137695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 25/82300 [06:31<356:58:04, 15.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.027601957321167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 26/82300 [06:46<356:49:30, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.3565394878387451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 27/82300 [07:02<356:44:05, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2776069641113281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 28/82300 [07:17<356:32:01, 15.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.9583160877227783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 29/82300 [07:33<356:38:51, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2230029106140137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 30/82300 [07:49<356:41:22, 15.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.9350292682647705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 31/82300 [08:04<356:29:49, 15.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1295626163482666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 32/82300 [08:20<356:24:58, 15.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1711156368255615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 33/82300 [08:35<356:18:04, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.4645940065383911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 34/82300 [08:51<356:21:32, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.174204707145691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 35/82300 [09:07<356:13:14, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0782009363174438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 36/82300 [09:22<356:13:21, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.9799803495407104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 37/82300 [09:38<356:10:22, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.180436611175537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 38/82300 [09:53<356:11:10, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.8830055594444275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 39/82300 [10:09<356:06:48, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0835990905761719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 40/82300 [10:24<356:02:50, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.0766915082931519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 41/82300 [10:40<356:01:57, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2683428525924683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 42/82300 [10:56<356:06:24, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.3210864067077637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 43/82300 [11:11<355:59:25, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.126894474029541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 44/82300 [11:27<355:59:21, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.3516286611557007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 45/82300 [11:42<355:57:31, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.2718085050582886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 46/82300 [11:58<356:06:08, 15.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.5456156730651855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 47/82300 [12:14<356:01:52, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1696194410324097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 48/82300 [12:29<356:03:27, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1387717723846436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training:   0%|          | 49/82300 [12:45<356:01:29, 15.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1308152675628662\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}