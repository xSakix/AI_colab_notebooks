{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2 slovak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOf70bEdv3gtJXFl5Wy53Jr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xSakix/AI_colab_notebooks/blob/master/gpt_2_slovak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npg83tYyKWcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdb2f326-87ac-4be3-dedc-322707f67678"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bbg6Q-TKYDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "dd6002be-38f8-48bb-91b8-6cd011ba9ff4"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar  2 18:46:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQglP74wKdx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65cce0c3-70be-4b69-a471-2f1e78478424"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2t3L_2qwOpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def _gelu_python(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        This is now written in C in torch.nn.functional\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "if torch.__version__ < \"1.4.0\":\n",
        "    gelu = _gelu_python\n",
        "else:\n",
        "    gelu = F.gelu\n",
        "\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "ACT2FN = {\n",
        "    \"relu\": F.relu,\n",
        "    \"swish\": swish,\n",
        "    \"gelu\": gelu,\n",
        "    \"tanh\": F.tanh,\n",
        "    \"gelu_new\": gelu_new,\n",
        "}\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "    if activation_string in ACT2FN:\n",
        "        return ACT2FN[activation_string]\n",
        "    else:\n",
        "        raise KeyError(\n",
        "            \"function {} not found in ACT2FN mapping {} or torch.nn.functional\".format(\n",
        "                activation_string, list(ACT2FN.keys())\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoMovS4Dxapj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import typing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# renamed because it's not conv1d form pytorch, but a linear layer with transposition\n",
        "class NotConv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n",
        "            Basically works like a Linear layer but the weights are transposed\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SequenceSummary(nn.Module):\n",
        "    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n",
        "        Args of the config class:\n",
        "            summary_type:\n",
        "                - 'last' => [default] take the last token hidden state (like XLNet)\n",
        "                - 'first' => take the first token hidden state (like Bert)\n",
        "                - 'mean' => take the mean of all tokens hidden states\n",
        "                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)\n",
        "                - 'attn' => Not implemented now, use multi-head attention\n",
        "            summary_use_proj: Add a projection after the vector extraction\n",
        "            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n",
        "            summary_activation: 'tanh' or another string => add an activation to the output, Other => no activation. Default\n",
        "            summary_first_dropout: Add a dropout before the projection and activation\n",
        "            summary_last_dropout: Add a dropout after the projection and activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.summary_type = getattr(config, \"summary_type\", \"last\")\n",
        "        if self.summary_type == \"attn\":\n",
        "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
        "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
        "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.summary = Identity()\n",
        "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
        "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
        "                num_classes = config.num_labels\n",
        "            else:\n",
        "                num_classes = config.hidden_size\n",
        "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        activation_string = getattr(config, \"summary_activation\", None)\n",
        "        self.activation = (\n",
        "            get_activation(activation_string) if activation_string else Identity()\n",
        "        )  # type: typing.Callable\n",
        "\n",
        "        self.first_dropout = Identity()\n",
        "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
        "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
        "\n",
        "        self.last_dropout = Identity()\n",
        "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
        "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "    def forward(self, hidden_states, cls_index=None):\n",
        "        \"\"\" hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.\n",
        "            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n",
        "                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n",
        "                if summary_type == 'cls_index' and cls_index is None:\n",
        "                    we take the last token of the sequence as classification token\n",
        "        \"\"\"\n",
        "        if self.summary_type == \"last\":\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.summary_type == \"first\":\n",
        "            output = hidden_states[:, 0]\n",
        "        elif self.summary_type == \"mean\":\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.summary_type == \"cls_index\":\n",
        "            if cls_index is None:\n",
        "                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2] - 1, dtype=torch.long)\n",
        "            else:\n",
        "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
        "                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n",
        "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
        "            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n",
        "        elif self.summary_type == \"attn\":\n",
        "            raise NotImplementedError\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def prune_conv1d_layer(layer, index, dim=1):\n",
        "    \"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n",
        "        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if dim == 0:\n",
        "        b = layer.bias.clone().detach()\n",
        "    else:\n",
        "        b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = NotConv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    new_layer.bias.requires_grad = False\n",
        "    new_layer.bias.copy_(b.contiguous())\n",
        "    new_layer.bias.requires_grad = True\n",
        "    return new_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8hU8FD0wB40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# gpt: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# transformer block: https://arxiv.org/pdf/1801.10198.pdf\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# memory compressed att? https://arxiv.org/pdf/1801.10198.pdf\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, config, scale=False):\n",
        "        super(Attention,self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
        "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
        "        # print(n_state,'|', config.n_head)\n",
        "        assert n_state % config.n_head == 0\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = config.n_head\n",
        "        self.split_size = n_state\n",
        "        self.scale = scale\n",
        "\n",
        "        self.c_attn = NotConv1D(n_state * 3, nx)\n",
        "        self.c_proj = NotConv1D(n_state, nx)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.n_head, self.split_size // self.n_head)\n",
        "        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n",
        "        for head in heads:\n",
        "            # Compute how many pruned heads are before the head and move the index accordingly\n",
        "            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
        "\n",
        "        # Prune conv1d layers\n",
        "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
        "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
        "\n",
        "        # Update hyper params\n",
        "        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n",
        "        self.n_head = self.n_head - len(heads)\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def _attn(self, q, k, v, attention_mask=None, head_mask=None):\n",
        "        w = torch.matmul(q, k)\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        b = self.bias[:, :, ns - nd : ns, :ns]\n",
        "        w = w * b - 1e4 * (1 - b)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask\n",
        "            w = w + attention_mask\n",
        "\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "        w = self.attn_dropout(w)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            w = w * head_mask\n",
        "\n",
        "        outputs = [torch.matmul(w, v)]\n",
        "        if self.output_attentions:\n",
        "            outputs.append(w)\n",
        "        return outputs\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
        "\n",
        "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value = x.split(self.split_size, dim=2)\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
        "\n",
        "        attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n",
        "        a = attn_outputs[0]\n",
        "\n",
        "        a = self.merge_heads(a)\n",
        "        a = self.c_proj(a)\n",
        "        a = self.resid_dropout(a)\n",
        "\n",
        "        outputs = [a, present] + attn_outputs[1:]\n",
        "        return outputs  # a, present, (attentions)\n",
        "\n",
        "# not pytorch MLP ...\n",
        "class NotMLP(nn.Module):\n",
        "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(NotMLP,self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.c_fc = NotConv1D(n_state, nx)\n",
        "        self.c_proj = NotConv1D(nx, n_state)\n",
        "        self.act = gelu_new\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return self.dropout(h2)\n",
        "\n",
        "# fig 1 in https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# simplified: masked-multi -> layer norm -> mlp-> layer norm \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(Block,self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.attn = Attention(nx, n_ctx, config, scale)\n",
        "        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = NotMLP(4 * nx, config)\n",
        "\n",
        "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n",
        "        output_attn = self.attn(\n",
        "            self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask\n",
        "        )\n",
        "        a = output_attn[0]  # output_attn: a, present, (attentions)\n",
        "\n",
        "        x = x + a\n",
        "        m = self.mlp(self.ln_2(x))\n",
        "        x = x + m\n",
        "\n",
        "        outputs = [x] + output_attn[1:]\n",
        "        return outputs  # x, present, (attentions)\n",
        "\n",
        "\n",
        "#The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):   \n",
        "        super(GPT2Model,self).__init__()\n",
        "        self.config = config     \n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_past = config.output_past\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.wte\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.wte = new_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.h[layer].attn.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "    ):\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "        if position_ids is not None:\n",
        "            position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "        if position_ids is None:\n",
        "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "        # Attention mask.\n",
        "        if attention_mask is not None:\n",
        "            batch_size = input_ids.shape[0]\n",
        "            attention_mask = attention_mask.view(batch_size, -1)\n",
        "            # We create a 3D attention mask from a 2D tensor mask.\n",
        "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "            # this attention mask is more simple than the triangular masking of causal attention\n",
        "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "            # masked positions, this operation will create a tensor which is 0.0 for\n",
        "            # positions we want to attend and -10000.0 for masked positions.\n",
        "            # Since we are adding it to the raw scores before the softmax, this is\n",
        "            # effectively the same as removing these entirely.\n",
        "            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                head_mask = (\n",
        "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "                )  # We can specify head_mask for each layer\n",
        "            head_mask = head_mask.to(\n",
        "                dtype=next(self.parameters()).dtype\n",
        "            )  # switch to fload if need + fp16 compatibility\n",
        "        else:\n",
        "            head_mask = [None] * self.config.n_layer\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        if token_type_ids is not None:\n",
        "            token_type_embeds = self.wte(token_type_ids)\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
        "        hidden_states = self.drop(hidden_states)\n",
        "\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "        presents = ()\n",
        "        all_attentions = []\n",
        "        all_hidden_states = ()\n",
        "        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n",
        "\n",
        "            outputs = block(\n",
        "                hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]\n",
        "            )\n",
        "\n",
        "            hidden_states, present = outputs[:2]\n",
        "            if self.output_past:\n",
        "                presents = presents + (present,)\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(outputs[2])\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(*output_shape)\n",
        "        # Add last hidden state\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_past:\n",
        "            outputs = outputs + (presents,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            # let the number of heads free (-1) so we can extract attention even after head pruning\n",
        "            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]\n",
        "            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # last hidden state, (presents), (all hidden_states), (attentions)\n",
        "\n",
        "\n",
        "class GPT2LMHeadModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2LMHeadModel,self).__init__()\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
        "        # only last token for inputs_ids if past is defined in kwargs\n",
        "        if \"past\" in kwargs and kwargs[\"past\"]:\n",
        "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
        "\n",
        "        inputs = {\"input_ids\": input_ids}\n",
        "        inputs.update(kwargs)\n",
        "        return inputs\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):        \n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past=past,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "\n",
        "        outputs = (lm_logits,) + transformer_outputs[1:]\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n",
        "\n",
        "\n",
        "class GPT2DoubleHeadsModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2DoubleHeadsModel,self).__init__()\n",
        "        config.num_labels = 1\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.multiple_choice_head = SequenceSummary(config)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        mc_token_ids=None,\n",
        "        lm_labels=None,\n",
        "        mc_labels=None,\n",
        "    ):\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past=past,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n",
        "\n",
        "        outputs = (lm_logits, mc_logits) + transformer_outputs[1:]\n",
        "        if mc_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "        if lm_labels is not None:\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = lm_labels[..., 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKdAadsE4pOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" OpenAI GPT-2 configuration \"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "#Number of parameters: 45171200\n",
        "class GPT2Config:\n",
        "    model_type = \"gpt2\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50257,\n",
        "        n_positions=1024,\n",
        "        n_ctx=1024,\n",
        "        # n_positions=512,\n",
        "        # n_ctx=512,\n",
        "        # n_embd=768,\n",
        "        # n_layer=12,\n",
        "        # n_head=12,\n",
        "        n_embd=512,\n",
        "        n_layer=6,\n",
        "        n_head=8,\n",
        "        resid_pdrop=0.1,\n",
        "        embd_pdrop=0.1,\n",
        "        attn_pdrop=0.1,\n",
        "        layer_norm_epsilon=1e-5,\n",
        "        initializer_range=0.02,\n",
        "        summary_type=\"cls_index\",\n",
        "        summary_use_proj=True,\n",
        "        summary_activation=None,\n",
        "        summary_proj_to_labels=True,\n",
        "        summary_first_dropout=0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n",
        "        self.summary_type = summary_type\n",
        "        self.summary_use_proj = summary_use_proj\n",
        "        self.summary_activation = summary_activation\n",
        "        self.summary_first_dropout = summary_first_dropout\n",
        "        self.summary_proj_to_labels = summary_proj_to_labels\n",
        "\n",
        "    @property\n",
        "    def max_position_embeddings(self):\n",
        "        return self.n_positions\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self):\n",
        "        return self.n_embd\n",
        "\n",
        "    @property\n",
        "    def num_attention_heads(self):\n",
        "        return self.n_head\n",
        "\n",
        "    @property\n",
        "    def num_hidden_layers(self):\n",
        "        return self.n_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmPrpVQOKRmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f48dcfc9-b767-43f3-fd5a-4cb2de0076d3"
      },
      "source": [
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "# from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# constants\n",
        "\n",
        "NUM_BATCHES = int(1e5)\n",
        "BATCH_SIZE = 8\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "VALIDATE_EVERY  = 100\n",
        "GENERATE_EVERY  = 500\n",
        "GENERATE_LENGTH = 512\n",
        "# SEQ_LEN = 4096\n",
        "SEQ_LEN = 1024\n",
        "\n",
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def get_top_p(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "def sample_next_token(logits, top_p=0.9, temperature = 1.0):\n",
        "    logits = logits[0, -1, :] / temperature\n",
        "    filtered_logits = get_top_p(logits, top_p=top_p)\n",
        "\n",
        "    probs = F.softmax(filtered_logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(token))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "# instantiate model\n",
        "config = GPT2Config()\n",
        "config.output_hidden_states = True\n",
        "config.output_attentions = True\n",
        "config.output_past = True\n",
        "\n",
        "model = GPT2Model(config)\n",
        "model.cuda()\n",
        "# print(model)\n",
        "print('Number of parameters:',get_n_params(model))\n",
        "\n",
        "with gzip.open('/content/drive/My Drive/model_data/merged.gz') as file:\n",
        "    X = np.array([int(c) for c in file.read()])\n",
        "    si = int(len(X)-len(X)*0.2)\n",
        "    trX, vaX = np.split(X, [si])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        return full_seq[0:-1].cuda(), full_seq[1:].cuda()\n",
        "        # return full_seq[0:-1], full_seq[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
        "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "# optimizer\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,amsgrad=True)\n",
        "\n",
        "# scheduler = get_linear_schedule_with_warmup(\n",
        "#             optim,\n",
        "#             num_warmup_steps=VALIDATE_EVERY,\n",
        "#             num_training_steps=len(train_dataset) // GRADIENT_ACCUMULATE_EVERY * NUM_BATCHES\n",
        "#         )\n",
        "\n",
        "# training\n",
        "\n",
        "def get_batch_loss(model, data):\n",
        "    x, y = data\n",
        "    pred = model(x)\n",
        "    return F.cross_entropy(pred[0].transpose(1, 2), y, reduction='mean')\n",
        "\n",
        "for i in tqdm.tqdm(range(0, NUM_BATCHES), mininterval=10., desc='training'):\n",
        "    model.train()\n",
        "\n",
        "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
        "        loss = get_batch_loss(model, next(train_loader))\n",
        "        loss.backward()\n",
        "\n",
        "    print(f'training loss: {loss.item()}')\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # scheduler.step()\n",
        "\n",
        "    if i % VALIDATE_EVERY == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss = get_batch_loss(model, next(val_loader))\n",
        "            print(f'validation loss: {loss.item()}')\n",
        "\n",
        "    if i % GENERATE_EVERY == 0:\n",
        "        torch.save(model.state_dict(), os.path.join('/content/drive/My Drive/gpt2', 'epoch-{}.pt'.format(i)))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inp, _ = random.choice(val_dataset)\n",
        "            output_str = ''\n",
        "            prime = decode_tokens(inp)\n",
        "\n",
        "            # print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "            print(prime)\n",
        "            print('*'*100)\n",
        "\n",
        "            for _ in tqdm.tqdm(range(GENERATE_LENGTH), desc='generating'):\n",
        "                logits = model(inp[None, :])[0]\n",
        "                next_token = sample_next_token(logits)\n",
        "                output_str += decode_token(next_token)\n",
        "                inp = torch.cat((inp[1:], next_token), dim=0)\n",
        "\n",
        "            print(output_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 45171200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 0/100000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "220958\n",
            "55239\n",
            "training loss: 6.7300004959106445\n",
            "validation loss: 5.425815582275391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "generating:   0%|          | 0/512 [00:00<?, ?it/s]\u001b[A\n",
            "generating:   1%|          | 6/512 [00:00<00:09, 54.38it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "navania mozu skoncit.\r\n",
            "Naznacili ste vsak, ze rychlost politickeho vyvoja nie je\r\n",
            "idealna. Nemali by ho zobrat politicke strany vratane ANO viac do ruk?\r\n",
            "Nielen cakat na to, co bude robit prezident.\r\n",
            "Predpokladam, ze nebudeme cakat na prezidenta s prvymi kontaktmi.\r\n",
            "Nepotrebujeme, aby nam to hlava statu dovolila, alebo nie. Uvidime vsak,\r\n",
            "ze aj ked sa niekto na niecom dohodne, nakolko sa tym pochvali. Mohlo by\r\n",
            "to totiz napriklad v pripade socialnej demokracie ovplyvnit to, koho\r\n",
            "prezident nakoniec poveri skladanim vlady. Nechcem sa vykrucat, ale\r\n",
            "situacia sa naozaj len velmi tazko odhaduje. Podla mna by si vsak nase\r\n",
            "hnutie malo trocha dopriat, ze sa nebude v tomto momente trapit tym, aka\r\n",
            "zlozita situacia moze nastat. Mozno by sme si mohli dozicit par\r\n",
            "hodin euforie.\r\n",
            "Situacia moze byt komplikovana, ale mate nejaky osobny nazor\r\n",
            "na to, s kym by ste si vedeli pripadne vladnutie predstavit?\r\n",
            "Skutocne je to este prilis cerstve. Rad by som sa po dlhom case vyspal\r\n",
            "aspon sest hodin. Potom vam zavolam. Ale teraz \n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "generating:   2%|â–         | 12/512 [00:00<00:09, 54.24it/s]\u001b[A\n",
            "generating:   4%|â–Ž         | 18/512 [00:00<00:09, 53.77it/s]\u001b[A\n",
            "generating:   5%|â–         | 24/512 [00:00<00:09, 53.84it/s]\u001b[A\n",
            "generating:   6%|â–Œ         | 30/512 [00:00<00:08, 54.09it/s]\u001b[A\n",
            "generating:   7%|â–‹         | 36/512 [00:00<00:08, 54.30it/s]\u001b[A\n",
            "generating:   8%|â–Š         | 42/512 [00:00<00:08, 53.90it/s]\u001b[A\n",
            "generating:   9%|â–‰         | 48/512 [00:00<00:08, 53.91it/s]\u001b[A\n",
            "generating:  11%|â–ˆ         | 54/512 [00:01<00:08, 54.03it/s]\u001b[A\n",
            "generating:  12%|â–ˆâ–        | 60/512 [00:01<00:08, 54.07it/s]\u001b[A\n",
            "generating:  13%|â–ˆâ–Ž        | 66/512 [00:01<00:08, 54.12it/s]\u001b[A\n",
            "generating:  14%|â–ˆâ–        | 72/512 [00:01<00:08, 53.86it/s]\u001b[A\n",
            "generating:  15%|â–ˆâ–Œ        | 78/512 [00:01<00:08, 53.79it/s]\u001b[A\n",
            "generating:  16%|â–ˆâ–‹        | 84/512 [00:01<00:07, 54.01it/s]\u001b[A\n",
            "generating:  18%|â–ˆâ–Š        | 90/512 [00:01<00:07, 54.06it/s]\u001b[A\n",
            "generating:  19%|â–ˆâ–‰        | 96/512 [00:01<00:07, 54.29it/s]\u001b[A\n",
            "generating:  20%|â–ˆâ–‰        | 102/512 [00:01<00:07, 54.66it/s]\u001b[A\n",
            "generating:  21%|â–ˆâ–ˆ        | 108/512 [00:01<00:07, 54.55it/s]\u001b[A\n",
            "generating:  22%|â–ˆâ–ˆâ–       | 114/512 [00:02<00:07, 54.78it/s]\u001b[A\n",
            "generating:  23%|â–ˆâ–ˆâ–Ž       | 120/512 [00:02<00:07, 55.00it/s]\u001b[A\n",
            "generating:  25%|â–ˆâ–ˆâ–       | 126/512 [00:02<00:07, 55.11it/s]\u001b[A\n",
            "generating:  26%|â–ˆâ–ˆâ–Œ       | 132/512 [00:02<00:06, 55.06it/s]\u001b[A\n",
            "generating:  27%|â–ˆâ–ˆâ–‹       | 138/512 [00:02<00:06, 55.01it/s]\u001b[A\n",
            "generating:  28%|â–ˆâ–ˆâ–Š       | 144/512 [00:02<00:06, 55.19it/s]\u001b[A\n",
            "generating:  29%|â–ˆâ–ˆâ–‰       | 150/512 [00:02<00:06, 55.21it/s]\u001b[A\n",
            "generating:  30%|â–ˆâ–ˆâ–ˆ       | 156/512 [00:02<00:06, 55.19it/s]\u001b[A\n",
            "generating:  32%|â–ˆâ–ˆâ–ˆâ–      | 162/512 [00:02<00:06, 55.10it/s]\u001b[A\n",
            "generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 168/512 [00:03<00:06, 55.09it/s]\u001b[A\n",
            "generating:  34%|â–ˆâ–ˆâ–ˆâ–      | 174/512 [00:03<00:06, 54.96it/s]\u001b[A\n",
            "generating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/512 [00:03<00:06, 54.89it/s]\u001b[A\n",
            "generating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 186/512 [00:03<00:05, 54.73it/s]\u001b[A\n",
            "generating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 192/512 [00:03<00:05, 54.71it/s]\u001b[A\n",
            "generating:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 198/512 [00:03<00:05, 54.67it/s]\u001b[A\n",
            "generating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 204/512 [00:03<00:05, 54.57it/s]\u001b[A\n",
            "generating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 210/512 [00:03<00:05, 54.41it/s]\u001b[A\n",
            "generating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 216/512 [00:03<00:05, 53.76it/s]\u001b[A\n",
            "generating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 222/512 [00:04<00:05, 53.73it/s]\u001b[A\n",
            "generating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 228/512 [00:04<00:05, 52.77it/s]\u001b[A\n",
            "generating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 234/512 [00:04<00:05, 53.02it/s]\u001b[A\n",
            "generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 240/512 [00:04<00:05, 53.06it/s]\u001b[A\n",
            "generating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 246/512 [00:04<00:05, 53.13it/s]\u001b[A\n",
            "generating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 252/512 [00:04<00:04, 53.80it/s]\u001b[A\n",
            "generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 258/512 [00:04<00:04, 54.20it/s]\u001b[A\n",
            "generating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 264/512 [00:04<00:04, 54.44it/s]\u001b[A\n",
            "generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 270/512 [00:04<00:04, 54.69it/s]\u001b[A\n",
            "generating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 276/512 [00:05<00:04, 54.79it/s]\u001b[A\n",
            "generating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 282/512 [00:05<00:04, 54.91it/s]\u001b[A\n",
            "generating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 288/512 [00:05<00:04, 54.94it/s]\u001b[A\n",
            "generating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 294/512 [00:05<00:03, 54.76it/s]\u001b[A\n",
            "generating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 300/512 [00:05<00:03, 54.79it/s]\u001b[A\n",
            "generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 306/512 [00:05<00:03, 54.85it/s]\u001b[A\n",
            "generating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 312/512 [00:05<00:03, 54.91it/s]\u001b[A\n",
            "generating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 318/512 [00:05<00:03, 54.86it/s]\u001b[A\n",
            "generating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 324/512 [00:05<00:03, 54.87it/s]\u001b[A\n",
            "generating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 330/512 [00:06<00:03, 54.80it/s]\u001b[A\n",
            "generating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 336/512 [00:06<00:03, 54.88it/s]\u001b[A\n",
            "generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 342/512 [00:06<00:03, 54.83it/s]\u001b[A\n",
            "generating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 348/512 [00:06<00:02, 54.89it/s]\u001b[A\n",
            "generating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 354/512 [00:06<00:02, 54.90it/s]\u001b[A\n",
            "generating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 360/512 [00:06<00:02, 54.82it/s]\u001b[A\n",
            "generating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 366/512 [00:06<00:02, 54.84it/s]\u001b[A\n",
            "generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 372/512 [00:06<00:02, 54.85it/s]\u001b[A\n",
            "generating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 378/512 [00:06<00:02, 54.90it/s]\u001b[A\n",
            "generating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 384/512 [00:07<00:02, 54.83it/s]\u001b[A\n",
            "generating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 390/512 [00:07<00:02, 54.90it/s]\u001b[A\n",
            "generating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 396/512 [00:07<00:02, 54.95it/s]\u001b[A\n",
            "generating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 402/512 [00:07<00:02, 54.89it/s]\u001b[A\n",
            "generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 408/512 [00:07<00:01, 54.77it/s]\u001b[A\n",
            "generating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 414/512 [00:07<00:01, 54.70it/s]\u001b[A\n",
            "generating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 420/512 [00:07<00:01, 54.66it/s]\u001b[A\n",
            "generating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 426/512 [00:07<00:01, 54.72it/s]\u001b[A\n",
            "generating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 432/512 [00:07<00:01, 54.44it/s]\u001b[A\n",
            "generating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 438/512 [00:08<00:01, 54.62it/s]\u001b[A\n",
            "generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 444/512 [00:08<00:01, 54.78it/s]\u001b[A\n",
            "generating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 450/512 [00:08<00:01, 54.88it/s]\u001b[A\n",
            "generating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 456/512 [00:08<00:01, 55.02it/s]\u001b[A\n",
            "generating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 462/512 [00:08<00:00, 54.24it/s]\u001b[A\n",
            "generating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 468/512 [00:08<00:00, 53.81it/s]\u001b[A\n",
            "generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 474/512 [00:08<00:00, 53.62it/s]\u001b[A\n",
            "generating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 480/512 [00:08<00:00, 53.86it/s]\u001b[A\n",
            "generating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 486/512 [00:08<00:00, 54.23it/s]\u001b[A\n",
            "generating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 492/512 [00:09<00:00, 54.45it/s]\u001b[A\n",
            "generating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 498/512 [00:09<00:00, 54.42it/s]\u001b[A\n",
            "generating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 504/512 [00:09<00:00, 54.10it/s]\u001b[A\n",
            "generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 510/512 [00:09<00:00, 54.39it/s]\u001b[A\n",
            "training:   0%|          | 1/100000 [00:12<347:26:42, 12.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ä¹Æ£Æ¤ÂŸÄ‡Æ—Å°Ä'Ç¼Â„IÄ¹Ã¼EÄŒÂ·ÆŒÆ¬Ä–ÄµÅ‚ÄÆ¥o9Ç£ÆÃŠÂžÄ”IÃ¨Ç¢Å†Å–Ã‚vTÇ¥Â¯Ä‹ÄŽÄ”Æ»AÅ¾AÃ§Ä»Å¶aÇ¼Â„Ã‹Åº\u0012Æ¦^Æ¬Â¢ÂºÄ‹Æ—Ã¹MÇ˜Ä†ÂÇ¦ÆÄ³ÇœÃ„Å‡kÂ¿ÂŽoaÇ“ÅÄº\u00143Ä¬Ç¤ÂŽÂ–Â«oÆ¥Ä¡Ç¥\u000bÅ‡Æ»ÆÇƒÆ°eÇ“Å®Å¢Å¦Ã±ÄÆ¥Ç¼RÃ†Å‚aÆÄµÄ–Â·<Ã¼ÅIÆ’lÂŸÅŠÂ²Å¹Â¯ÂÅ†ÄxÆ¤\u0007Ç»Ç²Ã)Ã·Â²Æ³Ç„Ä®Â’Ç\u0014Å—!Ä³Ä°Â´\u0012AÇ´ÃÅ¯Å…Ã¸kÄœÇIÃ‘Ä¬Ä­Â \u0003Ä²Å·ÄµoÇ®ÂŸÂ‚ÇŸÂ™Å†jÅº\u0016Â½Ã¹Ç™Ç Ä¦Â–Ä¾OÃ²oÄ¬Ä”]ÄµÇ¾(Å‰eÆ—Ä™Å–Å‚Â¯Æ€Å­pÆ¡Ä±Ã¿Ç‘*Ç¤,\u0012 ÄŸÂ¯ÅœÂŸiÅ¿Ç®Æ’Æ…Y\u0006Ç†ÆÆÅ‚ÅÂ®Ä”]Ç©Æ»Å’Â¨Ç¼RÃ™Å™Æ˜A&Ä™ÂŸ+\rLÂžÅ¸Æ¥ÆÆ¥Æ¾Å™Â—ÅÂ‡Â¿ÂˆÂŽÅºÇ„'Ç½Å¼Æ˜Ä’Â€Æ®Â¯Ä”Â‰ÂŸÄµÄ”TYÄ£Å™Å³Ä‚Ç­ZÆ¤\u001f\u001e Â”Ã‹Â–YÄ¬Ç…@ÆŸjRÂ·Ä”YÂŸÅ¨dÄ”Å†ÆBÆ³Ç¤Ä™Ä±Ä£ÅÂ¸Æ’ÇšÄ”Æ¥rÄ­Â´.\u0012Ã‹\u001dÆ¡Å‡Å‰Q\u001fÃ¹Äi\rÅ…<ÄhÂnÆ£Â²dÄ¯ÄœPÃ­ÅŸÂŸ$Â´Ã‚Ä´Ã•ÅƒÂ¥Ã·aÇ¿<@ÃŠÄ‹^+Ä”\u0013Â¤Æ¬jÂ—Ç¬Æ‡Å–rÄ”Ä”#Ç¼Ä¡Å–Ã³ÄµÂ¯Æ·Äš\u0005Ä¬Ã™8Æ—Æ’Ç½RÃ¯Ã¦Å Â²lÃ¶ÆƒÆ€Ç´Æ™Ç”Ç¼Â·Ç¿YÄ¹Æ²Ç±psjÅžÅªÅ‰Ã¨Ä°\u0012Â”ÂŸ\u0000ÅªÂŸÇªÆ‰Ä±Ç«\u0015Â›ÂˆvÃ†Å\u0005\u0018'akÇ‘pÂ²\\Ç™dÅŠÂ¡ÇÂ–Â¢Â…ÅÆ€\u0012ÅŽÆ‹Æ¤icÄÇ…ÄµÆ€ÆŒ=Æ…ÅÃ£\u0004&Â…ÇYÇ²\u001fÆ¦P\u0005Â„Å¾Ä‹ÅÇPÇ‰ÃŠÆµÂ·Ã˜ÄªÂ¤Æ†Æ˜\u0015Ä­Â‡Ç”\u0013<ÂŸÂ¯Ç•Æ’\n",
            "training loss: 5.554462432861328\n",
            "training loss: 4.505431175231934\n",
            "training loss: 3.8644094467163086\n",
            "training loss: 3.528592109680176\n",
            "training loss: 3.370464563369751\n",
            "training loss: 3.253868341445923\n",
            "training loss: 3.1708126068115234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 9/100000 [00:22<254:05:58,  9.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 3.082307815551758\n",
            "training loss: 3.0859127044677734\n",
            "training loss: 3.0725784301757812\n",
            "training loss: 2.9929463863372803\n",
            "training loss: 2.942091464996338\n",
            "training loss: 2.9331037998199463\n",
            "training loss: 2.9781744480133057\n",
            "training loss: 3.0124621391296387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 17/100000 [00:33<188:45:58,  6.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.8685598373413086\n",
            "training loss: 2.8872644901275635\n",
            "training loss: 2.865420341491699\n",
            "training loss: 2.89408016204834\n",
            "training loss: 2.8771233558654785\n",
            "training loss: 2.830559730529785\n",
            "training loss: 2.8485922813415527\n",
            "training loss: 2.8395330905914307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 25/100000 [00:43<143:01:57,  5.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.8474984169006348\n",
            "training loss: 2.811014175415039\n",
            "training loss: 2.8175606727600098\n",
            "training loss: 2.8125338554382324\n",
            "training loss: 2.8321638107299805\n",
            "training loss: 2.806725263595581\n",
            "training loss: 2.8113715648651123\n",
            "training loss: 2.8077011108398438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 33/100000 [00:54<111:01:11,  4.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.775887966156006\n",
            "training loss: 2.771512269973755\n",
            "training loss: 2.734405279159546\n",
            "training loss: 2.8077809810638428\n",
            "training loss: 2.758618116378784\n",
            "training loss: 2.764341354370117\n",
            "training loss: 2.8120713233947754\n",
            "training loss: 2.818218469619751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 41/100000 [01:04<88:37:05,  3.19s/it] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.747067451477051\n",
            "training loss: 2.736715078353882\n",
            "training loss: 2.7446796894073486\n",
            "training loss: 2.8236989974975586\n",
            "training loss: 2.7479872703552246\n",
            "training loss: 2.7620177268981934\n",
            "training loss: 2.761080026626587\n",
            "training loss: 2.7365775108337402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 49/100000 [01:15<72:56:17,  2.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.7026164531707764\n",
            "training loss: 2.7306859493255615\n",
            "training loss: 2.7265281677246094\n",
            "training loss: 2.7456796169281006\n",
            "training loss: 2.745838165283203\n",
            "training loss: 2.7184300422668457\n",
            "training loss: 2.722390651702881\n",
            "training loss: 2.7168211936950684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 57/100000 [01:25<61:58:01,  2.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.7450132369995117\n",
            "training loss: 2.7429182529449463\n",
            "training loss: 2.7908875942230225\n",
            "training loss: 2.7477385997772217\n",
            "training loss: 2.72412109375\n",
            "training loss: 2.7545390129089355\n",
            "training loss: 2.7445297241210938\n",
            "training loss: 2.7540674209594727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 65/100000 [01:36<54:16:11,  1.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6872129440307617\n",
            "training loss: 2.7293853759765625\n",
            "training loss: 2.706089973449707\n",
            "training loss: 2.708589792251587\n",
            "training loss: 2.7028865814208984\n",
            "training loss: 2.737964153289795\n",
            "training loss: 2.684918165206909\n",
            "training loss: 2.735795021057129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 73/100000 [01:46<48:53:33,  1.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.7634530067443848\n",
            "training loss: 2.672989845275879\n",
            "training loss: 2.7521755695343018\n",
            "training loss: 2.7435402870178223\n",
            "training loss: 2.684194564819336\n",
            "training loss: 2.7105016708374023\n",
            "training loss: 2.6738674640655518\n",
            "training loss: 2.6682276725769043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 81/100000 [01:57<45:08:01,  1.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.687108278274536\n",
            "training loss: 2.7178633213043213\n",
            "training loss: 2.701817035675049\n",
            "training loss: 2.7225403785705566\n",
            "training loss: 2.6644821166992188\n",
            "training loss: 2.674950122833252\n",
            "training loss: 2.690626859664917\n",
            "training loss: 2.7326526641845703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 89/100000 [02:07<42:29:54,  1.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.792579174041748\n",
            "training loss: 2.704545021057129\n",
            "training loss: 2.667829990386963\n",
            "training loss: 2.675516128540039\n",
            "training loss: 2.6859066486358643\n",
            "training loss: 2.681424856185913\n",
            "training loss: 2.6993727684020996\n",
            "training loss: 2.6939425468444824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 97/100000 [02:18<40:39:06,  1.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6861889362335205\n",
            "training loss: 2.692721366882324\n",
            "training loss: 2.7356576919555664\n",
            "training loss: 2.7100300788879395\n",
            "training loss: 2.663008689880371\n",
            "validation loss: 2.730726718902588\n",
            "training loss: 2.6700613498687744\n",
            "training loss: 2.7145979404449463\n",
            "training loss: 2.6614112854003906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 105/100000 [02:28<39:28:00,  1.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6709563732147217\n",
            "training loss: 2.760660409927368\n",
            "training loss: 2.7099649906158447\n",
            "training loss: 2.714167356491089\n",
            "training loss: 2.699575662612915\n",
            "training loss: 2.727445125579834\n",
            "training loss: 2.7055823802948\n",
            "training loss: 2.672332525253296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 113/100000 [02:39<38:31:13,  1.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6984598636627197\n",
            "training loss: 2.6429080963134766\n",
            "training loss: 2.6644034385681152\n",
            "training loss: 2.664602518081665\n",
            "training loss: 2.6724343299865723\n",
            "training loss: 2.664153575897217\n",
            "training loss: 2.667996883392334\n",
            "training loss: 2.676600217819214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 121/100000 [02:49<37:51:07,  1.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6847596168518066\n",
            "training loss: 2.6982550621032715\n",
            "training loss: 2.6711578369140625\n",
            "training loss: 2.6365914344787598\n",
            "training loss: 2.7112178802490234\n",
            "training loss: 2.676584243774414\n",
            "training loss: 2.6534790992736816\n",
            "training loss: 2.6790785789489746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training:   0%|          | 129/100000 [03:00<37:23:27,  1.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6624064445495605\n",
            "training loss: 2.683363676071167\n",
            "training loss: 2.6697025299072266\n",
            "training loss: 2.6883060932159424\n",
            "training loss: 2.683645486831665\n",
            "training loss: 2.6771717071533203\n",
            "training loss: 2.725641965866089\n",
            "training loss: 2.6669914722442627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 137/100000 [03:10<37:04:54,  1.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6861889362335205\n",
            "training loss: 2.6379287242889404\n",
            "training loss: 2.6689393520355225\n",
            "training loss: 2.6634552478790283\n",
            "training loss: 2.673346757888794\n",
            "training loss: 2.6943790912628174\n",
            "training loss: 2.6595664024353027\n",
            "training loss: 2.710904598236084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 145/100000 [03:21<36:51:54,  1.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.663346290588379\n",
            "training loss: 2.6544241905212402\n",
            "training loss: 2.6950249671936035\n",
            "training loss: 2.6359951496124268\n",
            "training loss: 2.66217041015625\n",
            "training loss: 2.6934094429016113\n",
            "training loss: 2.670741558074951\n",
            "training loss: 2.61901593208313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 153/100000 [03:31<36:41:10,  1.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.659205436706543\n",
            "training loss: 2.6786015033721924\n",
            "training loss: 2.620779514312744\n",
            "training loss: 2.68139910697937\n",
            "training loss: 2.7510740756988525\n",
            "training loss: 2.6946232318878174\n",
            "training loss: 2.663455009460449\n",
            "training loss: 2.6621410846710205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 161/100000 [03:42<36:34:11,  1.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.738870143890381\n",
            "training loss: 2.653292179107666\n",
            "training loss: 2.661766767501831\n",
            "training loss: 2.7121269702911377\n",
            "training loss: 2.655540704727173\n",
            "training loss: 2.650344133377075\n",
            "training loss: 2.727717161178589\n",
            "training loss: 2.698533058166504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 169/100000 [03:52<36:29:15,  1.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6965787410736084\n",
            "training loss: 2.6766982078552246\n",
            "training loss: 2.6903514862060547\n",
            "training loss: 2.6429123878479004\n",
            "training loss: 2.7391202449798584\n",
            "training loss: 2.6834373474121094\n",
            "training loss: 2.6656136512756348\n",
            "training loss: 2.652639389038086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 177/100000 [04:03<36:26:05,  1.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6450939178466797\n",
            "training loss: 2.6441543102264404\n",
            "training loss: 2.6256141662597656\n",
            "training loss: 2.6607797145843506\n",
            "training loss: 2.6637356281280518\n",
            "training loss: 2.6377243995666504\n",
            "training loss: 2.650367021560669\n",
            "training loss: 2.7062060832977295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 185/100000 [04:13<36:24:47,  1.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.6402173042297363\n",
            "training loss: 2.65810489654541\n",
            "training loss: 2.689021348953247\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}